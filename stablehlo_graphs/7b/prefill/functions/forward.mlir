module @IrToHlo.7512 attributes {mhlo.cross_program_prefetches = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<32000x4096xf32>, %arg1: tensor<4096xf32>, %arg2: tensor<f32>, %arg3: tensor<4096x11008xf32>, %arg4: tensor<11008x4096xf32>, %arg5: tensor<4096xf32>, %arg6: tensor<4096x4096xf32>, %arg7: tensor<2048xi64>, %arg8: tensor<4096x4096xf32>, %arg9: tensor<4096xf32>, %arg10: tensor<4096x11008xf32>, %arg11: tensor<11008x4096xf32>, %arg12: tensor<4096xf32>, %arg13: tensor<4096x4096xf32>, %arg14: tensor<4096x4096xf32>, %arg15: tensor<4096xf32>, %arg16: tensor<4096x11008xf32>, %arg17: tensor<11008x4096xf32>, %arg18: tensor<4096xf32>, %arg19: tensor<4096x4096xf32>, %arg20: tensor<4096x4096xf32>, %arg21: tensor<4096xf32>, %arg22: tensor<4096x11008xf32>, %arg23: tensor<11008x4096xf32>, %arg24: tensor<4096xf32>, %arg25: tensor<4096x4096xf32>, %arg26: tensor<4096x4096xf32>, %arg27: tensor<4096xf32>, %arg28: tensor<4096x11008xf32>, %arg29: tensor<11008x4096xf32>, %arg30: tensor<4096xf32>, %arg31: tensor<4096x4096xf32>, %arg32: tensor<4096x4096xf32>, %arg33: tensor<4096xf32>, %arg34: tensor<4096x11008xf32>, %arg35: tensor<11008x4096xf32>, %arg36: tensor<4096xf32>, %arg37: tensor<4096x4096xf32>, %arg38: tensor<4096x4096xf32>, %arg39: tensor<4096xf32>, %arg40: tensor<4096x11008xf32>, %arg41: tensor<11008x4096xf32>, %arg42: tensor<4096xf32>, %arg43: tensor<4096x4096xf32>, %arg44: tensor<4096x4096xf32>, %arg45: tensor<4096xf32>, %arg46: tensor<4096x11008xf32>, %arg47: tensor<11008x4096xf32>, %arg48: tensor<4096xf32>, %arg49: tensor<4096x4096xf32>, %arg50: tensor<4096x4096xf32>, %arg51: tensor<4096xf32>, %arg52: tensor<4096x11008xf32>, %arg53: tensor<11008x4096xf32>, %arg54: tensor<4096xf32>, %arg55: tensor<4096x4096xf32>, %arg56: tensor<4096x4096xf32>, %arg57: tensor<4096xf32>, %arg58: tensor<4096x11008xf32>, %arg59: tensor<11008x4096xf32>, %arg60: tensor<4096xf32>, %arg61: tensor<4096x4096xf32>, %arg62: tensor<4096x4096xf32>, %arg63: tensor<4096xf32>, %arg64: tensor<4096x11008xf32>, %arg65: tensor<11008x4096xf32>, %arg66: tensor<4096xf32>, %arg67: tensor<4096x4096xf32>, %arg68: tensor<4096x4096xf32>, %arg69: tensor<4096xf32>, %arg70: tensor<4096x11008xf32>, %arg71: tensor<11008x4096xf32>, %arg72: tensor<4096xf32>, %arg73: tensor<4096x4096xf32>, %arg74: tensor<4096x4096xf32>, %arg75: tensor<4096xf32>, %arg76: tensor<4096x11008xf32>, %arg77: tensor<11008x4096xf32>, %arg78: tensor<4096xf32>, %arg79: tensor<4096x4096xf32>, %arg80: tensor<4096x4096xf32>, %arg81: tensor<4096xf32>, %arg82: tensor<4096x11008xf32>, %arg83: tensor<11008x4096xf32>, %arg84: tensor<4096xf32>, %arg85: tensor<4096x4096xf32>, %arg86: tensor<4096x4096xf32>, %arg87: tensor<4096xf32>, %arg88: tensor<4096x11008xf32>, %arg89: tensor<11008x4096xf32>, %arg90: tensor<4096xf32>, %arg91: tensor<4096x4096xf32>, %arg92: tensor<4096x4096xf32>, %arg93: tensor<4096xf32>, %arg94: tensor<4096x11008xf32>, %arg95: tensor<11008x4096xf32>, %arg96: tensor<4096xf32>, %arg97: tensor<4096x4096xf32>, %arg98: tensor<4096x4096xf32>, %arg99: tensor<4096xf32>, %arg100: tensor<4096x11008xf32>, %arg101: tensor<11008x4096xf32>, %arg102: tensor<4096xf32>, %arg103: tensor<4096x4096xf32>, %arg104: tensor<4096x4096xf32>, %arg105: tensor<4096xf32>, %arg106: tensor<4096x11008xf32>, %arg107: tensor<11008x4096xf32>, %arg108: tensor<4096xf32>, %arg109: tensor<4096x4096xf32>, %arg110: tensor<4096x4096xf32>, %arg111: tensor<4096xf32>, %arg112: tensor<4096x11008xf32>, %arg113: tensor<11008x4096xf32>, %arg114: tensor<4096xf32>, %arg115: tensor<4096x4096xf32>, %arg116: tensor<4096x4096xf32>, %arg117: tensor<4096xf32>, %arg118: tensor<4096x11008xf32>, %arg119: tensor<11008x4096xf32>, %arg120: tensor<4096xf32>, %arg121: tensor<4096x4096xf32>, %arg122: tensor<4096x4096xf32>, %arg123: tensor<4096xf32>, %arg124: tensor<4096x11008xf32>, %arg125: tensor<11008x4096xf32>, %arg126: tensor<4096xf32>, %arg127: tensor<4096x4096xf32>, %arg128: tensor<4096x4096xf32>, %arg129: tensor<4096xf32>, %arg130: tensor<4096x11008xf32>, %arg131: tensor<11008x4096xf32>, %arg132: tensor<4096xf32>, %arg133: tensor<4096x4096xf32>, %arg134: tensor<4096x4096xf32>, %arg135: tensor<4096xf32>, %arg136: tensor<4096x11008xf32>, %arg137: tensor<11008x4096xf32>, %arg138: tensor<4096xf32>, %arg139: tensor<4096x4096xf32>, %arg140: tensor<4096x4096xf32>, %arg141: tensor<4096xf32>, %arg142: tensor<4096x11008xf32>, %arg143: tensor<11008x4096xf32>, %arg144: tensor<4096xf32>, %arg145: tensor<4096x4096xf32>, %arg146: tensor<4096x4096xf32>, %arg147: tensor<4096xf32>, %arg148: tensor<4096x11008xf32>, %arg149: tensor<11008x4096xf32>, %arg150: tensor<4096xf32>, %arg151: tensor<4096x4096xf32>, %arg152: tensor<4096x4096xf32>, %arg153: tensor<4096xf32>, %arg154: tensor<4096x11008xf32>, %arg155: tensor<11008x4096xf32>, %arg156: tensor<4096xf32>, %arg157: tensor<4096x4096xf32>, %arg158: tensor<4096x4096xf32>, %arg159: tensor<4096xf32>, %arg160: tensor<4096x11008xf32>, %arg161: tensor<11008x4096xf32>, %arg162: tensor<4096xf32>, %arg163: tensor<4096x4096xf32>, %arg164: tensor<4096x4096xf32>, %arg165: tensor<4096xf32>, %arg166: tensor<4096x11008xf32>, %arg167: tensor<11008x4096xf32>, %arg168: tensor<4096xf32>, %arg169: tensor<4096x4096xf32>, %arg170: tensor<4096x4096xf32>, %arg171: tensor<4096xf32>, %arg172: tensor<4096x11008xf32>, %arg173: tensor<11008x4096xf32>, %arg174: tensor<4096xf32>, %arg175: tensor<4096x4096xf32>, %arg176: tensor<4096x4096xf32>, %arg177: tensor<4096xf32>, %arg178: tensor<4096x11008xf32>, %arg179: tensor<11008x4096xf32>, %arg180: tensor<4096xf32>, %arg181: tensor<4096x4096xf32>, %arg182: tensor<4096x4096xf32>, %arg183: tensor<4096xf32>, %arg184: tensor<4096x11008xf32>, %arg185: tensor<11008x4096xf32>, %arg186: tensor<4096xf32>, %arg187: tensor<4096x4096xf32>, %arg188: tensor<4096x4096xf32>, %arg189: tensor<4096xf32>, %arg190: tensor<4096x11008xf32>, %arg191: tensor<11008x4096xf32>, %arg192: tensor<4096xf32>, %arg193: tensor<4096x4096xf32>, %arg194: tensor<4096x4096xf32>, %arg195: tensor<4096xf32>, %arg196: tensor<2048xi64>, %arg197: tensor<32000x4096xf32>, %arg198: tensor<2048xi64>, %arg199: tensor<i64>, %arg200: tensor<2304x32x128xf32>, %arg201: tensor<f32>, %arg202: tensor<f32>, %arg203: tensor<4608x64xcomplex<f32>>, %arg204: tensor<4096x4096xf32>, %arg205: tensor<2304x32x128xf32>, %arg206: tensor<4096x4096xf32>, %arg207: tensor<11008x4096xf32>, %arg208: tensor<2304x32x128xf32>, %arg209: tensor<4096x4096xf32>, %arg210: tensor<2304x32x128xf32>, %arg211: tensor<4096x4096xf32>, %arg212: tensor<11008x4096xf32>, %arg213: tensor<2304x32x128xf32>, %arg214: tensor<4096x4096xf32>, %arg215: tensor<2304x32x128xf32>, %arg216: tensor<4096x4096xf32>, %arg217: tensor<11008x4096xf32>, %arg218: tensor<2304x32x128xf32>, %arg219: tensor<4096x4096xf32>, %arg220: tensor<2304x32x128xf32>, %arg221: tensor<4096x4096xf32>, %arg222: tensor<11008x4096xf32>, %arg223: tensor<2304x32x128xf32>, %arg224: tensor<4096x4096xf32>, %arg225: tensor<2304x32x128xf32>, %arg226: tensor<4096x4096xf32>, %arg227: tensor<11008x4096xf32>, %arg228: tensor<2304x32x128xf32>, %arg229: tensor<4096x4096xf32>, %arg230: tensor<2304x32x128xf32>, %arg231: tensor<4096x4096xf32>, %arg232: tensor<11008x4096xf32>, %arg233: tensor<2304x32x128xf32>, %arg234: tensor<4096x4096xf32>, %arg235: tensor<2304x32x128xf32>, %arg236: tensor<4096x4096xf32>, %arg237: tensor<11008x4096xf32>, %arg238: tensor<2304x32x128xf32>, %arg239: tensor<4096x4096xf32>, %arg240: tensor<2304x32x128xf32>, %arg241: tensor<4096x4096xf32>, %arg242: tensor<11008x4096xf32>, %arg243: tensor<2304x32x128xf32>, %arg244: tensor<4096x4096xf32>, %arg245: tensor<2304x32x128xf32>, %arg246: tensor<4096x4096xf32>, %arg247: tensor<11008x4096xf32>, %arg248: tensor<2304x32x128xf32>, %arg249: tensor<4096x4096xf32>, %arg250: tensor<2304x32x128xf32>, %arg251: tensor<4096x4096xf32>, %arg252: tensor<11008x4096xf32>, %arg253: tensor<2304x32x128xf32>, %arg254: tensor<4096x4096xf32>, %arg255: tensor<2304x32x128xf32>, %arg256: tensor<4096x4096xf32>, %arg257: tensor<11008x4096xf32>, %arg258: tensor<2304x32x128xf32>, %arg259: tensor<4096x4096xf32>, %arg260: tensor<2304x32x128xf32>, %arg261: tensor<4096x4096xf32>, %arg262: tensor<11008x4096xf32>, %arg263: tensor<2304x32x128xf32>, %arg264: tensor<4096x4096xf32>, %arg265: tensor<2304x32x128xf32>, %arg266: tensor<4096x4096xf32>, %arg267: tensor<11008x4096xf32>, %arg268: tensor<2304x32x128xf32>, %arg269: tensor<4096x4096xf32>, %arg270: tensor<2304x32x128xf32>, %arg271: tensor<4096x4096xf32>, %arg272: tensor<11008x4096xf32>, %arg273: tensor<2304x32x128xf32>, %arg274: tensor<4096x4096xf32>, %arg275: tensor<2304x32x128xf32>, %arg276: tensor<4096x4096xf32>, %arg277: tensor<11008x4096xf32>, %arg278: tensor<2304x32x128xf32>, %arg279: tensor<4096x4096xf32>, %arg280: tensor<2304x32x128xf32>, %arg281: tensor<4096x4096xf32>, %arg282: tensor<11008x4096xf32>, %arg283: tensor<2304x32x128xf32>, %arg284: tensor<4096x4096xf32>, %arg285: tensor<2304x32x128xf32>, %arg286: tensor<4096x4096xf32>, %arg287: tensor<11008x4096xf32>, %arg288: tensor<2304x32x128xf32>, %arg289: tensor<4096x4096xf32>, %arg290: tensor<2304x32x128xf32>, %arg291: tensor<4096x4096xf32>, %arg292: tensor<11008x4096xf32>, %arg293: tensor<2304x32x128xf32>, %arg294: tensor<4096x4096xf32>, %arg295: tensor<2304x32x128xf32>, %arg296: tensor<4096x4096xf32>, %arg297: tensor<11008x4096xf32>, %arg298: tensor<2304x32x128xf32>, %arg299: tensor<4096x4096xf32>, %arg300: tensor<2304x32x128xf32>, %arg301: tensor<4096x4096xf32>, %arg302: tensor<11008x4096xf32>, %arg303: tensor<2304x32x128xf32>, %arg304: tensor<4096x4096xf32>, %arg305: tensor<2304x32x128xf32>, %arg306: tensor<4096x4096xf32>, %arg307: tensor<11008x4096xf32>, %arg308: tensor<2304x32x128xf32>, %arg309: tensor<4096x4096xf32>, %arg310: tensor<2304x32x128xf32>, %arg311: tensor<4096x4096xf32>, %arg312: tensor<11008x4096xf32>, %arg313: tensor<2304x32x128xf32>, %arg314: tensor<4096x4096xf32>, %arg315: tensor<2304x32x128xf32>, %arg316: tensor<4096x4096xf32>, %arg317: tensor<11008x4096xf32>, %arg318: tensor<2304x32x128xf32>, %arg319: tensor<4096x4096xf32>, %arg320: tensor<2304x32x128xf32>, %arg321: tensor<4096x4096xf32>, %arg322: tensor<11008x4096xf32>, %arg323: tensor<2304x32x128xf32>, %arg324: tensor<4096x4096xf32>, %arg325: tensor<2304x32x128xf32>, %arg326: tensor<4096x4096xf32>, %arg327: tensor<11008x4096xf32>, %arg328: tensor<2304x32x128xf32>, %arg329: tensor<4096x4096xf32>, %arg330: tensor<2304x32x128xf32>, %arg331: tensor<4096x4096xf32>, %arg332: tensor<11008x4096xf32>, %arg333: tensor<2304x32x128xf32>, %arg334: tensor<4096x4096xf32>, %arg335: tensor<2304x32x128xf32>, %arg336: tensor<4096x4096xf32>, %arg337: tensor<11008x4096xf32>, %arg338: tensor<2304x32x128xf32>, %arg339: tensor<4096x4096xf32>, %arg340: tensor<2304x32x128xf32>, %arg341: tensor<4096x4096xf32>, %arg342: tensor<11008x4096xf32>, %arg343: tensor<2304x32x128xf32>, %arg344: tensor<4096x4096xf32>, %arg345: tensor<2304x32x128xf32>, %arg346: tensor<4096x4096xf32>, %arg347: tensor<11008x4096xf32>, %arg348: tensor<2304x32x128xf32>, %arg349: tensor<4096x4096xf32>, %arg350: tensor<2304x32x128xf32>, %arg351: tensor<4096x4096xf32>, %arg352: tensor<11008x4096xf32>, %arg353: tensor<2304x32x128xf32>, %arg354: tensor<4096x4096xf32>, %arg355: tensor<2304x32x128xf32>, %arg356: tensor<4096x4096xf32>, %arg357: tensor<11008x4096xf32>, %arg358: tensor<2304x32x128xf32>, %arg359: tensor<4096x4096xf32>, %arg360: tensor<2304x32x128xf32>, %arg361: tensor<4096x4096xf32>, %arg362: tensor<11008x4096xf32>) -> (tensor<2048x32000xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>) {
    %0 = stablehlo.constant dense<2.44140625E-4> : tensor<2048xf32>
    %1 = stablehlo.constant dense<2.000000e+00> : tensor<2048x4096xf32>
    %2 = stablehlo.constant dense<0> : tensor<2048xi64>
    %3 = stablehlo.constant dense<0.000000e+00> : tensor<1x1x2048x2048xf32>
    %4 = stablehlo.constant dense<1> : tensor<2048x2048xi64>
    %5 = stablehlo.constant dense<"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tensor<2048xi64>
    %6 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %7 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %8 = stablehlo.convert %arg196 : (tensor<2048xi64>) -> tensor<2048xui32>
    %9 = "stablehlo.gather"(%arg197, %8) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 4096]> : tensor<2xi64>} : (tensor<32000x4096xf32>, tensor<2048xui32>) -> tensor<2048x4096xf32>
    %10 = stablehlo.power %9, %1 : tensor<2048x4096xf32>
    %11 = stablehlo.reduce(%10 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %12 = stablehlo.multiply %11, %0 : tensor<2048xf32>
    %13 = stablehlo.reshape %12 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %14 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %15 = stablehlo.add %13, %14 : tensor<2048x1xf32>
    %16 = stablehlo.rsqrt %15 : tensor<2048x1xf32>
    %17 = stablehlo.reshape %16 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %19 = stablehlo.multiply %9, %18 : tensor<2048x4096xf32>
    %20 = stablehlo.broadcast_in_dim %arg195, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %21 = stablehlo.multiply %19, %20 : tensor<2048x4096xf32>
    %22 = stablehlo.transpose %arg206, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %23 = stablehlo.dot %21, %22, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %24 = stablehlo.reshape %23 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %25 = stablehlo.slice %24 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %26 = stablehlo.reshape %25 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %27 = stablehlo.slice %24 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %28 = stablehlo.reshape %27 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %29 = stablehlo.complex %26, %28 : tensor<2048x32x64xcomplex<f32>>
    %30 = stablehlo.convert %arg198 : (tensor<2048xi64>) -> tensor<2048xui32>
    %31 = "stablehlo.gather"(%arg203, %30) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 64]> : tensor<2xi64>} : (tensor<4608x64xcomplex<f32>>, tensor<2048xui32>) -> tensor<2048x64xcomplex<f32>>
    %32 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %33 = stablehlo.multiply %29, %32 : tensor<2048x32x64xcomplex<f32>>
    %34 = stablehlo.real %33 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %35 = stablehlo.reshape %34 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %36 = stablehlo.imag %33 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %37 = stablehlo.reshape %36 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %38 = stablehlo.concatenate %35, %37, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %39 = stablehlo.reshape %38 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %40 = stablehlo.transpose %39, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %41 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %42 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %43 = stablehlo.add %arg198, %42 : tensor<2048xi64>
    %44 = stablehlo.select %41, %43, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %45 = stablehlo.reshape %44 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %46 = stablehlo.transpose %arg204, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %47 = stablehlo.dot %21, %46, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %48 = stablehlo.reshape %47 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %49 = stablehlo.slice %48 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %50 = stablehlo.reshape %49 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %51 = stablehlo.slice %48 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %52 = stablehlo.reshape %51 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %53 = stablehlo.complex %50, %52 : tensor<2048x32x64xcomplex<f32>>
    %54 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %55 = stablehlo.multiply %53, %54 : tensor<2048x32x64xcomplex<f32>>
    %56 = stablehlo.real %55 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %57 = stablehlo.reshape %56 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %58 = stablehlo.imag %55 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %59 = stablehlo.reshape %58 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %60 = stablehlo.concatenate %57, %59, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %61 = stablehlo.reshape %60 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %62 = "stablehlo.scatter"(%arg205, %45, %61) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %63 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %64 = "stablehlo.gather"(%62, %63) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %65 = stablehlo.transpose %64, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %66 = stablehlo.dot_general %40, %65, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %67 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %68 = stablehlo.divide %66, %67 : tensor<32x2048x2048xf32>
    %69 = stablehlo.reshape %68 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %70 = stablehlo.broadcast_in_dim %5, dims = [1] : (tensor<2048xi64>) -> tensor<2048x2048xi64>
    %71 = stablehlo.broadcast_in_dim %5, dims = [0] : (tensor<2048xi64>) -> tensor<2048x2048xi64>
    %72 = stablehlo.subtract %70, %71 : tensor<2048x2048xi64>
    %73 = stablehlo.compare  GE, %72, %4 : (tensor<2048x2048xi64>, tensor<2048x2048xi64>) -> tensor<2048x2048xi1>
    %74 = stablehlo.reshape %73 : (tensor<2048x2048xi1>) -> tensor<1x1x2048x2048xi1>
    %75 = stablehlo.reshape %arg201 : (tensor<f32>) -> tensor<1x1xf32>
    %76 = stablehlo.broadcast_in_dim %75, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x2048x2048xf32>
    %77 = stablehlo.select %74, %76, %3 : tensor<1x1x2048x2048xi1>, tensor<1x1x2048x2048xf32>
    %78 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %79 = stablehlo.broadcast_in_dim %78, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %80 = stablehlo.add %69, %79 : tensor<1x32x2048x2048xf32>
    %81 = stablehlo.reduce(%80 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %82 = stablehlo.broadcast_in_dim %81, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %83 = stablehlo.subtract %80, %82 : tensor<1x32x2048x2048xf32>
    %84 = stablehlo.exponential %83 : tensor<1x32x2048x2048xf32>
    %85 = stablehlo.reduce(%84 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %86 = stablehlo.broadcast_in_dim %85, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %87 = stablehlo.divide %84, %86 : tensor<1x32x2048x2048xf32>
    %88 = stablehlo.reshape %87 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %89 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %90 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %91 = stablehlo.add %arg198, %90 : tensor<2048xi64>
    %92 = stablehlo.select %89, %91, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %93 = stablehlo.reshape %92 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %94 = stablehlo.transpose %arg194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %95 = stablehlo.dot %21, %94, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %96 = stablehlo.reshape %95 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %97 = "stablehlo.scatter"(%arg200, %93, %96) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %98 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %99 = "stablehlo.gather"(%97, %98) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %100 = stablehlo.transpose %99, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %101 = stablehlo.dot_general %88, %100, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %102 = stablehlo.reshape %101 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %103 = stablehlo.transpose %102, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %104 = stablehlo.reshape %103 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %105 = stablehlo.transpose %arg193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %106 = stablehlo.dot %104, %105, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %107 = stablehlo.add %9, %106 : tensor<2048x4096xf32>
    %108 = stablehlo.power %107, %1 : tensor<2048x4096xf32>
    %109 = stablehlo.reduce(%108 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %110 = stablehlo.multiply %109, %0 : tensor<2048xf32>
    %111 = stablehlo.reshape %110 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %112 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %113 = stablehlo.add %111, %112 : tensor<2048x1xf32>
    %114 = stablehlo.rsqrt %113 : tensor<2048x1xf32>
    %115 = stablehlo.reshape %114 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %116 = stablehlo.broadcast_in_dim %115, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %117 = stablehlo.multiply %107, %116 : tensor<2048x4096xf32>
    %118 = stablehlo.broadcast_in_dim %arg192, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %119 = stablehlo.multiply %117, %118 : tensor<2048x4096xf32>
    %120 = stablehlo.transpose %arg207, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %121 = stablehlo.dot %119, %120, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %122 = stablehlo.logistic %121 : tensor<2048x11008xf32>
    %123 = stablehlo.multiply %121, %122 : tensor<2048x11008xf32>
    %124 = stablehlo.transpose %arg191, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %125 = stablehlo.dot %119, %124, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %126 = stablehlo.multiply %123, %125 : tensor<2048x11008xf32>
    %127 = stablehlo.transpose %arg190, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %128 = stablehlo.dot %126, %127, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %129 = stablehlo.add %107, %128 : tensor<2048x4096xf32>
    %130 = stablehlo.power %129, %1 : tensor<2048x4096xf32>
    %131 = stablehlo.reduce(%130 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %132 = stablehlo.multiply %131, %0 : tensor<2048xf32>
    %133 = stablehlo.reshape %132 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %134 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %135 = stablehlo.add %133, %134 : tensor<2048x1xf32>
    %136 = stablehlo.rsqrt %135 : tensor<2048x1xf32>
    %137 = stablehlo.reshape %136 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %139 = stablehlo.multiply %129, %138 : tensor<2048x4096xf32>
    %140 = stablehlo.broadcast_in_dim %arg189, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %141 = stablehlo.multiply %139, %140 : tensor<2048x4096xf32>
    %142 = stablehlo.transpose %arg211, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %143 = stablehlo.dot %141, %142, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %144 = stablehlo.reshape %143 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %145 = stablehlo.slice %144 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %146 = stablehlo.reshape %145 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %147 = stablehlo.slice %144 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %148 = stablehlo.reshape %147 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %149 = stablehlo.complex %146, %148 : tensor<2048x32x64xcomplex<f32>>
    %150 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %151 = stablehlo.multiply %149, %150 : tensor<2048x32x64xcomplex<f32>>
    %152 = stablehlo.real %151 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %153 = stablehlo.reshape %152 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %154 = stablehlo.imag %151 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %155 = stablehlo.reshape %154 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %156 = stablehlo.concatenate %153, %155, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %157 = stablehlo.reshape %156 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %158 = stablehlo.transpose %157, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %159 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %160 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %161 = stablehlo.add %arg198, %160 : tensor<2048xi64>
    %162 = stablehlo.select %159, %161, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %163 = stablehlo.reshape %162 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %164 = stablehlo.transpose %arg209, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %165 = stablehlo.dot %141, %164, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %166 = stablehlo.reshape %165 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %167 = stablehlo.slice %166 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %168 = stablehlo.reshape %167 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %169 = stablehlo.slice %166 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %170 = stablehlo.reshape %169 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %171 = stablehlo.complex %168, %170 : tensor<2048x32x64xcomplex<f32>>
    %172 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %173 = stablehlo.multiply %171, %172 : tensor<2048x32x64xcomplex<f32>>
    %174 = stablehlo.real %173 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %175 = stablehlo.reshape %174 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %176 = stablehlo.imag %173 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %177 = stablehlo.reshape %176 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %178 = stablehlo.concatenate %175, %177, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %179 = stablehlo.reshape %178 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %180 = "stablehlo.scatter"(%arg210, %163, %179) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %181 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %182 = "stablehlo.gather"(%180, %181) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %183 = stablehlo.transpose %182, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %184 = stablehlo.dot_general %158, %183, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %185 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %186 = stablehlo.divide %184, %185 : tensor<32x2048x2048xf32>
    %187 = stablehlo.reshape %186 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %188 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %189 = stablehlo.broadcast_in_dim %188, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %190 = stablehlo.add %187, %189 : tensor<1x32x2048x2048xf32>
    %191 = stablehlo.reduce(%190 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %192 = stablehlo.broadcast_in_dim %191, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %193 = stablehlo.subtract %190, %192 : tensor<1x32x2048x2048xf32>
    %194 = stablehlo.exponential %193 : tensor<1x32x2048x2048xf32>
    %195 = stablehlo.reduce(%194 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %196 = stablehlo.broadcast_in_dim %195, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %197 = stablehlo.divide %194, %196 : tensor<1x32x2048x2048xf32>
    %198 = stablehlo.reshape %197 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %199 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %200 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %201 = stablehlo.add %arg198, %200 : tensor<2048xi64>
    %202 = stablehlo.select %199, %201, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %203 = stablehlo.reshape %202 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %204 = stablehlo.transpose %arg188, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %205 = stablehlo.dot %141, %204, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %206 = stablehlo.reshape %205 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %207 = "stablehlo.scatter"(%arg208, %203, %206) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %208 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %209 = "stablehlo.gather"(%207, %208) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %210 = stablehlo.transpose %209, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %211 = stablehlo.dot_general %198, %210, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %212 = stablehlo.reshape %211 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %213 = stablehlo.transpose %212, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %214 = stablehlo.reshape %213 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %215 = stablehlo.transpose %arg187, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %216 = stablehlo.dot %214, %215, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %217 = stablehlo.add %129, %216 : tensor<2048x4096xf32>
    %218 = stablehlo.power %217, %1 : tensor<2048x4096xf32>
    %219 = stablehlo.reduce(%218 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %220 = stablehlo.multiply %219, %0 : tensor<2048xf32>
    %221 = stablehlo.reshape %220 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %222 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %223 = stablehlo.add %221, %222 : tensor<2048x1xf32>
    %224 = stablehlo.rsqrt %223 : tensor<2048x1xf32>
    %225 = stablehlo.reshape %224 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %226 = stablehlo.broadcast_in_dim %225, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %227 = stablehlo.multiply %217, %226 : tensor<2048x4096xf32>
    %228 = stablehlo.broadcast_in_dim %arg186, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %229 = stablehlo.multiply %227, %228 : tensor<2048x4096xf32>
    %230 = stablehlo.transpose %arg212, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %231 = stablehlo.dot %229, %230, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %232 = stablehlo.logistic %231 : tensor<2048x11008xf32>
    %233 = stablehlo.multiply %231, %232 : tensor<2048x11008xf32>
    %234 = stablehlo.transpose %arg185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %235 = stablehlo.dot %229, %234, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %236 = stablehlo.multiply %233, %235 : tensor<2048x11008xf32>
    %237 = stablehlo.transpose %arg184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %238 = stablehlo.dot %236, %237, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %239 = stablehlo.add %217, %238 : tensor<2048x4096xf32>
    %240 = stablehlo.power %239, %1 : tensor<2048x4096xf32>
    %241 = stablehlo.reduce(%240 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %242 = stablehlo.multiply %241, %0 : tensor<2048xf32>
    %243 = stablehlo.reshape %242 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %244 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %245 = stablehlo.add %243, %244 : tensor<2048x1xf32>
    %246 = stablehlo.rsqrt %245 : tensor<2048x1xf32>
    %247 = stablehlo.reshape %246 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %248 = stablehlo.broadcast_in_dim %247, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %249 = stablehlo.multiply %239, %248 : tensor<2048x4096xf32>
    %250 = stablehlo.broadcast_in_dim %arg183, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %251 = stablehlo.multiply %249, %250 : tensor<2048x4096xf32>
    %252 = stablehlo.transpose %arg216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %253 = stablehlo.dot %251, %252, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %254 = stablehlo.reshape %253 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %255 = stablehlo.slice %254 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %256 = stablehlo.reshape %255 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %257 = stablehlo.slice %254 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %258 = stablehlo.reshape %257 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %259 = stablehlo.complex %256, %258 : tensor<2048x32x64xcomplex<f32>>
    %260 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %261 = stablehlo.multiply %259, %260 : tensor<2048x32x64xcomplex<f32>>
    %262 = stablehlo.real %261 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %263 = stablehlo.reshape %262 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %264 = stablehlo.imag %261 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %265 = stablehlo.reshape %264 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %266 = stablehlo.concatenate %263, %265, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %267 = stablehlo.reshape %266 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %268 = stablehlo.transpose %267, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %269 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %270 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %271 = stablehlo.add %arg198, %270 : tensor<2048xi64>
    %272 = stablehlo.select %269, %271, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %273 = stablehlo.reshape %272 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %274 = stablehlo.transpose %arg214, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %275 = stablehlo.dot %251, %274, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %276 = stablehlo.reshape %275 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %277 = stablehlo.slice %276 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %278 = stablehlo.reshape %277 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %279 = stablehlo.slice %276 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %280 = stablehlo.reshape %279 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %281 = stablehlo.complex %278, %280 : tensor<2048x32x64xcomplex<f32>>
    %282 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %283 = stablehlo.multiply %281, %282 : tensor<2048x32x64xcomplex<f32>>
    %284 = stablehlo.real %283 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %285 = stablehlo.reshape %284 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %286 = stablehlo.imag %283 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %287 = stablehlo.reshape %286 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %288 = stablehlo.concatenate %285, %287, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %289 = stablehlo.reshape %288 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %290 = "stablehlo.scatter"(%arg215, %273, %289) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %291 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %292 = "stablehlo.gather"(%290, %291) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %293 = stablehlo.transpose %292, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %294 = stablehlo.dot_general %268, %293, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %295 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %296 = stablehlo.divide %294, %295 : tensor<32x2048x2048xf32>
    %297 = stablehlo.reshape %296 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %298 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %299 = stablehlo.broadcast_in_dim %298, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %300 = stablehlo.add %297, %299 : tensor<1x32x2048x2048xf32>
    %301 = stablehlo.reduce(%300 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %302 = stablehlo.broadcast_in_dim %301, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %303 = stablehlo.subtract %300, %302 : tensor<1x32x2048x2048xf32>
    %304 = stablehlo.exponential %303 : tensor<1x32x2048x2048xf32>
    %305 = stablehlo.reduce(%304 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %306 = stablehlo.broadcast_in_dim %305, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %307 = stablehlo.divide %304, %306 : tensor<1x32x2048x2048xf32>
    %308 = stablehlo.reshape %307 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %309 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %310 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %311 = stablehlo.add %arg198, %310 : tensor<2048xi64>
    %312 = stablehlo.select %309, %311, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %313 = stablehlo.reshape %312 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %314 = stablehlo.transpose %arg182, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %315 = stablehlo.dot %251, %314, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %316 = stablehlo.reshape %315 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %317 = "stablehlo.scatter"(%arg213, %313, %316) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %318 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %319 = "stablehlo.gather"(%317, %318) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %320 = stablehlo.transpose %319, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %321 = stablehlo.dot_general %308, %320, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %322 = stablehlo.reshape %321 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %323 = stablehlo.transpose %322, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %324 = stablehlo.reshape %323 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %325 = stablehlo.transpose %arg181, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %326 = stablehlo.dot %324, %325, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %327 = stablehlo.add %239, %326 : tensor<2048x4096xf32>
    %328 = stablehlo.power %327, %1 : tensor<2048x4096xf32>
    %329 = stablehlo.reduce(%328 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %330 = stablehlo.multiply %329, %0 : tensor<2048xf32>
    %331 = stablehlo.reshape %330 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %332 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %333 = stablehlo.add %331, %332 : tensor<2048x1xf32>
    %334 = stablehlo.rsqrt %333 : tensor<2048x1xf32>
    %335 = stablehlo.reshape %334 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %336 = stablehlo.broadcast_in_dim %335, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %337 = stablehlo.multiply %327, %336 : tensor<2048x4096xf32>
    %338 = stablehlo.broadcast_in_dim %arg180, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %339 = stablehlo.multiply %337, %338 : tensor<2048x4096xf32>
    %340 = stablehlo.transpose %arg217, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %341 = stablehlo.dot %339, %340, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %342 = stablehlo.logistic %341 : tensor<2048x11008xf32>
    %343 = stablehlo.multiply %341, %342 : tensor<2048x11008xf32>
    %344 = stablehlo.transpose %arg179, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %345 = stablehlo.dot %339, %344, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %346 = stablehlo.multiply %343, %345 : tensor<2048x11008xf32>
    %347 = stablehlo.transpose %arg178, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %348 = stablehlo.dot %346, %347, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %349 = stablehlo.add %327, %348 : tensor<2048x4096xf32>
    %350 = stablehlo.power %349, %1 : tensor<2048x4096xf32>
    %351 = stablehlo.reduce(%350 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %352 = stablehlo.multiply %351, %0 : tensor<2048xf32>
    %353 = stablehlo.reshape %352 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %354 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %355 = stablehlo.add %353, %354 : tensor<2048x1xf32>
    %356 = stablehlo.rsqrt %355 : tensor<2048x1xf32>
    %357 = stablehlo.reshape %356 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %358 = stablehlo.broadcast_in_dim %357, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %359 = stablehlo.multiply %349, %358 : tensor<2048x4096xf32>
    %360 = stablehlo.broadcast_in_dim %arg177, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %361 = stablehlo.multiply %359, %360 : tensor<2048x4096xf32>
    %362 = stablehlo.transpose %arg221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %363 = stablehlo.dot %361, %362, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %364 = stablehlo.reshape %363 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %365 = stablehlo.slice %364 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %366 = stablehlo.reshape %365 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %367 = stablehlo.slice %364 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %368 = stablehlo.reshape %367 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %369 = stablehlo.complex %366, %368 : tensor<2048x32x64xcomplex<f32>>
    %370 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %371 = stablehlo.multiply %369, %370 : tensor<2048x32x64xcomplex<f32>>
    %372 = stablehlo.real %371 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %373 = stablehlo.reshape %372 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %374 = stablehlo.imag %371 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %375 = stablehlo.reshape %374 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %376 = stablehlo.concatenate %373, %375, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %377 = stablehlo.reshape %376 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %378 = stablehlo.transpose %377, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %379 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %380 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %381 = stablehlo.add %arg198, %380 : tensor<2048xi64>
    %382 = stablehlo.select %379, %381, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %383 = stablehlo.reshape %382 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %384 = stablehlo.transpose %arg219, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %385 = stablehlo.dot %361, %384, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %386 = stablehlo.reshape %385 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %387 = stablehlo.slice %386 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %388 = stablehlo.reshape %387 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %389 = stablehlo.slice %386 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %390 = stablehlo.reshape %389 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %391 = stablehlo.complex %388, %390 : tensor<2048x32x64xcomplex<f32>>
    %392 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %393 = stablehlo.multiply %391, %392 : tensor<2048x32x64xcomplex<f32>>
    %394 = stablehlo.real %393 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %395 = stablehlo.reshape %394 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %396 = stablehlo.imag %393 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %397 = stablehlo.reshape %396 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %398 = stablehlo.concatenate %395, %397, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %399 = stablehlo.reshape %398 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %400 = "stablehlo.scatter"(%arg220, %383, %399) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %401 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %402 = "stablehlo.gather"(%400, %401) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %403 = stablehlo.transpose %402, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %404 = stablehlo.dot_general %378, %403, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %405 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %406 = stablehlo.divide %404, %405 : tensor<32x2048x2048xf32>
    %407 = stablehlo.reshape %406 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %408 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %409 = stablehlo.broadcast_in_dim %408, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %410 = stablehlo.add %407, %409 : tensor<1x32x2048x2048xf32>
    %411 = stablehlo.reduce(%410 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %412 = stablehlo.broadcast_in_dim %411, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %413 = stablehlo.subtract %410, %412 : tensor<1x32x2048x2048xf32>
    %414 = stablehlo.exponential %413 : tensor<1x32x2048x2048xf32>
    %415 = stablehlo.reduce(%414 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %416 = stablehlo.broadcast_in_dim %415, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %417 = stablehlo.divide %414, %416 : tensor<1x32x2048x2048xf32>
    %418 = stablehlo.reshape %417 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %419 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %420 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %421 = stablehlo.add %arg198, %420 : tensor<2048xi64>
    %422 = stablehlo.select %419, %421, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %423 = stablehlo.reshape %422 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %424 = stablehlo.transpose %arg176, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %425 = stablehlo.dot %361, %424, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %426 = stablehlo.reshape %425 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %427 = "stablehlo.scatter"(%arg218, %423, %426) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %428 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %429 = "stablehlo.gather"(%427, %428) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %430 = stablehlo.transpose %429, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %431 = stablehlo.dot_general %418, %430, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %432 = stablehlo.reshape %431 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %433 = stablehlo.transpose %432, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %434 = stablehlo.reshape %433 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %435 = stablehlo.transpose %arg175, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %436 = stablehlo.dot %434, %435, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %437 = stablehlo.add %349, %436 : tensor<2048x4096xf32>
    %438 = stablehlo.power %437, %1 : tensor<2048x4096xf32>
    %439 = stablehlo.reduce(%438 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %440 = stablehlo.multiply %439, %0 : tensor<2048xf32>
    %441 = stablehlo.reshape %440 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %442 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %443 = stablehlo.add %441, %442 : tensor<2048x1xf32>
    %444 = stablehlo.rsqrt %443 : tensor<2048x1xf32>
    %445 = stablehlo.reshape %444 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %446 = stablehlo.broadcast_in_dim %445, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %447 = stablehlo.multiply %437, %446 : tensor<2048x4096xf32>
    %448 = stablehlo.broadcast_in_dim %arg174, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %449 = stablehlo.multiply %447, %448 : tensor<2048x4096xf32>
    %450 = stablehlo.transpose %arg222, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %451 = stablehlo.dot %449, %450, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %452 = stablehlo.logistic %451 : tensor<2048x11008xf32>
    %453 = stablehlo.multiply %451, %452 : tensor<2048x11008xf32>
    %454 = stablehlo.transpose %arg173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %455 = stablehlo.dot %449, %454, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %456 = stablehlo.multiply %453, %455 : tensor<2048x11008xf32>
    %457 = stablehlo.transpose %arg172, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %458 = stablehlo.dot %456, %457, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %459 = stablehlo.add %437, %458 : tensor<2048x4096xf32>
    %460 = stablehlo.power %459, %1 : tensor<2048x4096xf32>
    %461 = stablehlo.reduce(%460 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %462 = stablehlo.multiply %461, %0 : tensor<2048xf32>
    %463 = stablehlo.reshape %462 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %464 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %465 = stablehlo.add %463, %464 : tensor<2048x1xf32>
    %466 = stablehlo.rsqrt %465 : tensor<2048x1xf32>
    %467 = stablehlo.reshape %466 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %468 = stablehlo.broadcast_in_dim %467, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %469 = stablehlo.multiply %459, %468 : tensor<2048x4096xf32>
    %470 = stablehlo.broadcast_in_dim %arg171, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %471 = stablehlo.multiply %469, %470 : tensor<2048x4096xf32>
    %472 = stablehlo.transpose %arg226, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %473 = stablehlo.dot %471, %472, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %474 = stablehlo.reshape %473 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %475 = stablehlo.slice %474 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %476 = stablehlo.reshape %475 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %477 = stablehlo.slice %474 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %478 = stablehlo.reshape %477 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %479 = stablehlo.complex %476, %478 : tensor<2048x32x64xcomplex<f32>>
    %480 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %481 = stablehlo.multiply %479, %480 : tensor<2048x32x64xcomplex<f32>>
    %482 = stablehlo.real %481 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %483 = stablehlo.reshape %482 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %484 = stablehlo.imag %481 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %485 = stablehlo.reshape %484 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %486 = stablehlo.concatenate %483, %485, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %487 = stablehlo.reshape %486 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %488 = stablehlo.transpose %487, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %489 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %490 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %491 = stablehlo.add %arg198, %490 : tensor<2048xi64>
    %492 = stablehlo.select %489, %491, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %493 = stablehlo.reshape %492 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %494 = stablehlo.transpose %arg224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %495 = stablehlo.dot %471, %494, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %496 = stablehlo.reshape %495 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %497 = stablehlo.slice %496 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %498 = stablehlo.reshape %497 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %499 = stablehlo.slice %496 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %500 = stablehlo.reshape %499 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %501 = stablehlo.complex %498, %500 : tensor<2048x32x64xcomplex<f32>>
    %502 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %503 = stablehlo.multiply %501, %502 : tensor<2048x32x64xcomplex<f32>>
    %504 = stablehlo.real %503 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %505 = stablehlo.reshape %504 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %506 = stablehlo.imag %503 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %507 = stablehlo.reshape %506 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %508 = stablehlo.concatenate %505, %507, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %509 = stablehlo.reshape %508 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %510 = "stablehlo.scatter"(%arg225, %493, %509) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %511 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %512 = "stablehlo.gather"(%510, %511) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %513 = stablehlo.transpose %512, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %514 = stablehlo.dot_general %488, %513, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %515 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %516 = stablehlo.divide %514, %515 : tensor<32x2048x2048xf32>
    %517 = stablehlo.reshape %516 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %518 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %519 = stablehlo.broadcast_in_dim %518, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %520 = stablehlo.add %517, %519 : tensor<1x32x2048x2048xf32>
    %521 = stablehlo.reduce(%520 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %522 = stablehlo.broadcast_in_dim %521, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %523 = stablehlo.subtract %520, %522 : tensor<1x32x2048x2048xf32>
    %524 = stablehlo.exponential %523 : tensor<1x32x2048x2048xf32>
    %525 = stablehlo.reduce(%524 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %526 = stablehlo.broadcast_in_dim %525, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %527 = stablehlo.divide %524, %526 : tensor<1x32x2048x2048xf32>
    %528 = stablehlo.reshape %527 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %529 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %530 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %531 = stablehlo.add %arg198, %530 : tensor<2048xi64>
    %532 = stablehlo.select %529, %531, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %533 = stablehlo.reshape %532 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %534 = stablehlo.transpose %arg170, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %535 = stablehlo.dot %471, %534, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %536 = stablehlo.reshape %535 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %537 = "stablehlo.scatter"(%arg223, %533, %536) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %538 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %539 = "stablehlo.gather"(%537, %538) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %540 = stablehlo.transpose %539, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %541 = stablehlo.dot_general %528, %540, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %542 = stablehlo.reshape %541 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %543 = stablehlo.transpose %542, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %544 = stablehlo.reshape %543 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %545 = stablehlo.transpose %arg169, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %546 = stablehlo.dot %544, %545, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %547 = stablehlo.add %459, %546 : tensor<2048x4096xf32>
    %548 = stablehlo.power %547, %1 : tensor<2048x4096xf32>
    %549 = stablehlo.reduce(%548 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %550 = stablehlo.multiply %549, %0 : tensor<2048xf32>
    %551 = stablehlo.reshape %550 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %552 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %553 = stablehlo.add %551, %552 : tensor<2048x1xf32>
    %554 = stablehlo.rsqrt %553 : tensor<2048x1xf32>
    %555 = stablehlo.reshape %554 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %556 = stablehlo.broadcast_in_dim %555, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %557 = stablehlo.multiply %547, %556 : tensor<2048x4096xf32>
    %558 = stablehlo.broadcast_in_dim %arg168, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %559 = stablehlo.multiply %557, %558 : tensor<2048x4096xf32>
    %560 = stablehlo.transpose %arg227, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %561 = stablehlo.dot %559, %560, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %562 = stablehlo.logistic %561 : tensor<2048x11008xf32>
    %563 = stablehlo.multiply %561, %562 : tensor<2048x11008xf32>
    %564 = stablehlo.transpose %arg167, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %565 = stablehlo.dot %559, %564, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %566 = stablehlo.multiply %563, %565 : tensor<2048x11008xf32>
    %567 = stablehlo.transpose %arg166, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %568 = stablehlo.dot %566, %567, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %569 = stablehlo.add %547, %568 : tensor<2048x4096xf32>
    %570 = stablehlo.power %569, %1 : tensor<2048x4096xf32>
    %571 = stablehlo.reduce(%570 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %572 = stablehlo.multiply %571, %0 : tensor<2048xf32>
    %573 = stablehlo.reshape %572 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %574 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %575 = stablehlo.add %573, %574 : tensor<2048x1xf32>
    %576 = stablehlo.rsqrt %575 : tensor<2048x1xf32>
    %577 = stablehlo.reshape %576 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %578 = stablehlo.broadcast_in_dim %577, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %579 = stablehlo.multiply %569, %578 : tensor<2048x4096xf32>
    %580 = stablehlo.broadcast_in_dim %arg165, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %581 = stablehlo.multiply %579, %580 : tensor<2048x4096xf32>
    %582 = stablehlo.transpose %arg231, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %583 = stablehlo.dot %581, %582, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %584 = stablehlo.reshape %583 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %585 = stablehlo.slice %584 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %586 = stablehlo.reshape %585 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %587 = stablehlo.slice %584 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %588 = stablehlo.reshape %587 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %589 = stablehlo.complex %586, %588 : tensor<2048x32x64xcomplex<f32>>
    %590 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %591 = stablehlo.multiply %589, %590 : tensor<2048x32x64xcomplex<f32>>
    %592 = stablehlo.real %591 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %593 = stablehlo.reshape %592 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %594 = stablehlo.imag %591 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %595 = stablehlo.reshape %594 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %596 = stablehlo.concatenate %593, %595, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %597 = stablehlo.reshape %596 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %598 = stablehlo.transpose %597, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %599 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %600 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %601 = stablehlo.add %arg198, %600 : tensor<2048xi64>
    %602 = stablehlo.select %599, %601, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %603 = stablehlo.reshape %602 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %604 = stablehlo.transpose %arg229, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %605 = stablehlo.dot %581, %604, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %606 = stablehlo.reshape %605 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %607 = stablehlo.slice %606 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %608 = stablehlo.reshape %607 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %609 = stablehlo.slice %606 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %610 = stablehlo.reshape %609 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %611 = stablehlo.complex %608, %610 : tensor<2048x32x64xcomplex<f32>>
    %612 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %613 = stablehlo.multiply %611, %612 : tensor<2048x32x64xcomplex<f32>>
    %614 = stablehlo.real %613 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %615 = stablehlo.reshape %614 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %616 = stablehlo.imag %613 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %617 = stablehlo.reshape %616 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %618 = stablehlo.concatenate %615, %617, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %619 = stablehlo.reshape %618 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %620 = "stablehlo.scatter"(%arg230, %603, %619) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %621 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %622 = "stablehlo.gather"(%620, %621) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %623 = stablehlo.transpose %622, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %624 = stablehlo.dot_general %598, %623, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %625 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %626 = stablehlo.divide %624, %625 : tensor<32x2048x2048xf32>
    %627 = stablehlo.reshape %626 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %628 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %629 = stablehlo.broadcast_in_dim %628, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %630 = stablehlo.add %627, %629 : tensor<1x32x2048x2048xf32>
    %631 = stablehlo.reduce(%630 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %632 = stablehlo.broadcast_in_dim %631, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %633 = stablehlo.subtract %630, %632 : tensor<1x32x2048x2048xf32>
    %634 = stablehlo.exponential %633 : tensor<1x32x2048x2048xf32>
    %635 = stablehlo.reduce(%634 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %636 = stablehlo.broadcast_in_dim %635, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %637 = stablehlo.divide %634, %636 : tensor<1x32x2048x2048xf32>
    %638 = stablehlo.reshape %637 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %639 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %640 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %641 = stablehlo.add %arg198, %640 : tensor<2048xi64>
    %642 = stablehlo.select %639, %641, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %643 = stablehlo.reshape %642 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %644 = stablehlo.transpose %arg164, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %645 = stablehlo.dot %581, %644, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %646 = stablehlo.reshape %645 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %647 = "stablehlo.scatter"(%arg228, %643, %646) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %648 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %649 = "stablehlo.gather"(%647, %648) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %650 = stablehlo.transpose %649, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %651 = stablehlo.dot_general %638, %650, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %652 = stablehlo.reshape %651 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %653 = stablehlo.transpose %652, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %654 = stablehlo.reshape %653 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %655 = stablehlo.transpose %arg163, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %656 = stablehlo.dot %654, %655, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %657 = stablehlo.add %569, %656 : tensor<2048x4096xf32>
    %658 = stablehlo.power %657, %1 : tensor<2048x4096xf32>
    %659 = stablehlo.reduce(%658 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %660 = stablehlo.multiply %659, %0 : tensor<2048xf32>
    %661 = stablehlo.reshape %660 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %662 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %663 = stablehlo.add %661, %662 : tensor<2048x1xf32>
    %664 = stablehlo.rsqrt %663 : tensor<2048x1xf32>
    %665 = stablehlo.reshape %664 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %666 = stablehlo.broadcast_in_dim %665, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %667 = stablehlo.multiply %657, %666 : tensor<2048x4096xf32>
    %668 = stablehlo.broadcast_in_dim %arg162, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %669 = stablehlo.multiply %667, %668 : tensor<2048x4096xf32>
    %670 = stablehlo.transpose %arg232, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %671 = stablehlo.dot %669, %670, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %672 = stablehlo.logistic %671 : tensor<2048x11008xf32>
    %673 = stablehlo.multiply %671, %672 : tensor<2048x11008xf32>
    %674 = stablehlo.transpose %arg161, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %675 = stablehlo.dot %669, %674, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %676 = stablehlo.multiply %673, %675 : tensor<2048x11008xf32>
    %677 = stablehlo.transpose %arg160, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %678 = stablehlo.dot %676, %677, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %679 = stablehlo.add %657, %678 : tensor<2048x4096xf32>
    %680 = stablehlo.power %679, %1 : tensor<2048x4096xf32>
    %681 = stablehlo.reduce(%680 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %682 = stablehlo.multiply %681, %0 : tensor<2048xf32>
    %683 = stablehlo.reshape %682 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %684 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %685 = stablehlo.add %683, %684 : tensor<2048x1xf32>
    %686 = stablehlo.rsqrt %685 : tensor<2048x1xf32>
    %687 = stablehlo.reshape %686 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %688 = stablehlo.broadcast_in_dim %687, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %689 = stablehlo.multiply %679, %688 : tensor<2048x4096xf32>
    %690 = stablehlo.broadcast_in_dim %arg159, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %691 = stablehlo.multiply %689, %690 : tensor<2048x4096xf32>
    %692 = stablehlo.transpose %arg236, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %693 = stablehlo.dot %691, %692, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %694 = stablehlo.reshape %693 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %695 = stablehlo.slice %694 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %696 = stablehlo.reshape %695 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %697 = stablehlo.slice %694 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %698 = stablehlo.reshape %697 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %699 = stablehlo.complex %696, %698 : tensor<2048x32x64xcomplex<f32>>
    %700 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %701 = stablehlo.multiply %699, %700 : tensor<2048x32x64xcomplex<f32>>
    %702 = stablehlo.real %701 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %703 = stablehlo.reshape %702 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %704 = stablehlo.imag %701 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %705 = stablehlo.reshape %704 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %706 = stablehlo.concatenate %703, %705, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %707 = stablehlo.reshape %706 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %708 = stablehlo.transpose %707, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %709 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %710 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %711 = stablehlo.add %arg198, %710 : tensor<2048xi64>
    %712 = stablehlo.select %709, %711, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %713 = stablehlo.reshape %712 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %714 = stablehlo.transpose %arg234, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %715 = stablehlo.dot %691, %714, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %716 = stablehlo.reshape %715 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %717 = stablehlo.slice %716 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %718 = stablehlo.reshape %717 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %719 = stablehlo.slice %716 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %720 = stablehlo.reshape %719 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %721 = stablehlo.complex %718, %720 : tensor<2048x32x64xcomplex<f32>>
    %722 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %723 = stablehlo.multiply %721, %722 : tensor<2048x32x64xcomplex<f32>>
    %724 = stablehlo.real %723 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %725 = stablehlo.reshape %724 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %726 = stablehlo.imag %723 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %727 = stablehlo.reshape %726 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %728 = stablehlo.concatenate %725, %727, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %729 = stablehlo.reshape %728 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %730 = "stablehlo.scatter"(%arg235, %713, %729) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %731 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %732 = "stablehlo.gather"(%730, %731) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %733 = stablehlo.transpose %732, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %734 = stablehlo.dot_general %708, %733, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %735 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %736 = stablehlo.divide %734, %735 : tensor<32x2048x2048xf32>
    %737 = stablehlo.reshape %736 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %738 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %739 = stablehlo.broadcast_in_dim %738, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %740 = stablehlo.add %737, %739 : tensor<1x32x2048x2048xf32>
    %741 = stablehlo.reduce(%740 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %742 = stablehlo.broadcast_in_dim %741, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %743 = stablehlo.subtract %740, %742 : tensor<1x32x2048x2048xf32>
    %744 = stablehlo.exponential %743 : tensor<1x32x2048x2048xf32>
    %745 = stablehlo.reduce(%744 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %746 = stablehlo.broadcast_in_dim %745, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %747 = stablehlo.divide %744, %746 : tensor<1x32x2048x2048xf32>
    %748 = stablehlo.reshape %747 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %749 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %750 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %751 = stablehlo.add %arg198, %750 : tensor<2048xi64>
    %752 = stablehlo.select %749, %751, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %753 = stablehlo.reshape %752 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %754 = stablehlo.transpose %arg158, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %755 = stablehlo.dot %691, %754, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %756 = stablehlo.reshape %755 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %757 = "stablehlo.scatter"(%arg233, %753, %756) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %758 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %759 = "stablehlo.gather"(%757, %758) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %760 = stablehlo.transpose %759, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %761 = stablehlo.dot_general %748, %760, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %762 = stablehlo.reshape %761 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %763 = stablehlo.transpose %762, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %764 = stablehlo.reshape %763 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %765 = stablehlo.transpose %arg157, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %766 = stablehlo.dot %764, %765, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %767 = stablehlo.add %679, %766 : tensor<2048x4096xf32>
    %768 = stablehlo.power %767, %1 : tensor<2048x4096xf32>
    %769 = stablehlo.reduce(%768 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %770 = stablehlo.multiply %769, %0 : tensor<2048xf32>
    %771 = stablehlo.reshape %770 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %772 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %773 = stablehlo.add %771, %772 : tensor<2048x1xf32>
    %774 = stablehlo.rsqrt %773 : tensor<2048x1xf32>
    %775 = stablehlo.reshape %774 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %776 = stablehlo.broadcast_in_dim %775, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %777 = stablehlo.multiply %767, %776 : tensor<2048x4096xf32>
    %778 = stablehlo.broadcast_in_dim %arg156, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %779 = stablehlo.multiply %777, %778 : tensor<2048x4096xf32>
    %780 = stablehlo.transpose %arg237, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %781 = stablehlo.dot %779, %780, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %782 = stablehlo.logistic %781 : tensor<2048x11008xf32>
    %783 = stablehlo.multiply %781, %782 : tensor<2048x11008xf32>
    %784 = stablehlo.transpose %arg155, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %785 = stablehlo.dot %779, %784, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %786 = stablehlo.multiply %783, %785 : tensor<2048x11008xf32>
    %787 = stablehlo.transpose %arg154, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %788 = stablehlo.dot %786, %787, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %789 = stablehlo.add %767, %788 : tensor<2048x4096xf32>
    %790 = stablehlo.power %789, %1 : tensor<2048x4096xf32>
    %791 = stablehlo.reduce(%790 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %792 = stablehlo.multiply %791, %0 : tensor<2048xf32>
    %793 = stablehlo.reshape %792 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %794 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %795 = stablehlo.add %793, %794 : tensor<2048x1xf32>
    %796 = stablehlo.rsqrt %795 : tensor<2048x1xf32>
    %797 = stablehlo.reshape %796 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %798 = stablehlo.broadcast_in_dim %797, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %799 = stablehlo.multiply %789, %798 : tensor<2048x4096xf32>
    %800 = stablehlo.broadcast_in_dim %arg153, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %801 = stablehlo.multiply %799, %800 : tensor<2048x4096xf32>
    %802 = stablehlo.transpose %arg241, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %803 = stablehlo.dot %801, %802, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %804 = stablehlo.reshape %803 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %805 = stablehlo.slice %804 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %806 = stablehlo.reshape %805 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %807 = stablehlo.slice %804 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %808 = stablehlo.reshape %807 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %809 = stablehlo.complex %806, %808 : tensor<2048x32x64xcomplex<f32>>
    %810 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %811 = stablehlo.multiply %809, %810 : tensor<2048x32x64xcomplex<f32>>
    %812 = stablehlo.real %811 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %813 = stablehlo.reshape %812 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %814 = stablehlo.imag %811 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %815 = stablehlo.reshape %814 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %816 = stablehlo.concatenate %813, %815, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %817 = stablehlo.reshape %816 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %818 = stablehlo.transpose %817, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %819 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %820 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %821 = stablehlo.add %arg198, %820 : tensor<2048xi64>
    %822 = stablehlo.select %819, %821, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %823 = stablehlo.reshape %822 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %824 = stablehlo.transpose %arg239, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %825 = stablehlo.dot %801, %824, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %826 = stablehlo.reshape %825 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %827 = stablehlo.slice %826 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %828 = stablehlo.reshape %827 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %829 = stablehlo.slice %826 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %830 = stablehlo.reshape %829 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %831 = stablehlo.complex %828, %830 : tensor<2048x32x64xcomplex<f32>>
    %832 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %833 = stablehlo.multiply %831, %832 : tensor<2048x32x64xcomplex<f32>>
    %834 = stablehlo.real %833 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %835 = stablehlo.reshape %834 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %836 = stablehlo.imag %833 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %837 = stablehlo.reshape %836 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %838 = stablehlo.concatenate %835, %837, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %839 = stablehlo.reshape %838 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %840 = "stablehlo.scatter"(%arg240, %823, %839) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %841 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %842 = "stablehlo.gather"(%840, %841) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %843 = stablehlo.transpose %842, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %844 = stablehlo.dot_general %818, %843, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %845 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %846 = stablehlo.divide %844, %845 : tensor<32x2048x2048xf32>
    %847 = stablehlo.reshape %846 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %848 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %849 = stablehlo.broadcast_in_dim %848, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %850 = stablehlo.add %847, %849 : tensor<1x32x2048x2048xf32>
    %851 = stablehlo.reduce(%850 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %852 = stablehlo.broadcast_in_dim %851, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %853 = stablehlo.subtract %850, %852 : tensor<1x32x2048x2048xf32>
    %854 = stablehlo.exponential %853 : tensor<1x32x2048x2048xf32>
    %855 = stablehlo.reduce(%854 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %856 = stablehlo.broadcast_in_dim %855, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %857 = stablehlo.divide %854, %856 : tensor<1x32x2048x2048xf32>
    %858 = stablehlo.reshape %857 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %859 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %860 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %861 = stablehlo.add %arg198, %860 : tensor<2048xi64>
    %862 = stablehlo.select %859, %861, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %863 = stablehlo.reshape %862 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %864 = stablehlo.transpose %arg152, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %865 = stablehlo.dot %801, %864, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %866 = stablehlo.reshape %865 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %867 = "stablehlo.scatter"(%arg238, %863, %866) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %868 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %869 = "stablehlo.gather"(%867, %868) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %870 = stablehlo.transpose %869, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %871 = stablehlo.dot_general %858, %870, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %872 = stablehlo.reshape %871 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %873 = stablehlo.transpose %872, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %874 = stablehlo.reshape %873 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %875 = stablehlo.transpose %arg151, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %876 = stablehlo.dot %874, %875, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %877 = stablehlo.add %789, %876 : tensor<2048x4096xf32>
    %878 = stablehlo.power %877, %1 : tensor<2048x4096xf32>
    %879 = stablehlo.reduce(%878 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %880 = stablehlo.multiply %879, %0 : tensor<2048xf32>
    %881 = stablehlo.reshape %880 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %882 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %883 = stablehlo.add %881, %882 : tensor<2048x1xf32>
    %884 = stablehlo.rsqrt %883 : tensor<2048x1xf32>
    %885 = stablehlo.reshape %884 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %886 = stablehlo.broadcast_in_dim %885, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %887 = stablehlo.multiply %877, %886 : tensor<2048x4096xf32>
    %888 = stablehlo.broadcast_in_dim %arg150, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %889 = stablehlo.multiply %887, %888 : tensor<2048x4096xf32>
    %890 = stablehlo.transpose %arg242, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %891 = stablehlo.dot %889, %890, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %892 = stablehlo.logistic %891 : tensor<2048x11008xf32>
    %893 = stablehlo.multiply %891, %892 : tensor<2048x11008xf32>
    %894 = stablehlo.transpose %arg149, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %895 = stablehlo.dot %889, %894, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %896 = stablehlo.multiply %893, %895 : tensor<2048x11008xf32>
    %897 = stablehlo.transpose %arg148, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %898 = stablehlo.dot %896, %897, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %899 = stablehlo.add %877, %898 : tensor<2048x4096xf32>
    %900 = stablehlo.power %899, %1 : tensor<2048x4096xf32>
    %901 = stablehlo.reduce(%900 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %902 = stablehlo.multiply %901, %0 : tensor<2048xf32>
    %903 = stablehlo.reshape %902 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %904 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %905 = stablehlo.add %903, %904 : tensor<2048x1xf32>
    %906 = stablehlo.rsqrt %905 : tensor<2048x1xf32>
    %907 = stablehlo.reshape %906 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %908 = stablehlo.broadcast_in_dim %907, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %909 = stablehlo.multiply %899, %908 : tensor<2048x4096xf32>
    %910 = stablehlo.broadcast_in_dim %arg147, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %911 = stablehlo.multiply %909, %910 : tensor<2048x4096xf32>
    %912 = stablehlo.transpose %arg246, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %913 = stablehlo.dot %911, %912, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %914 = stablehlo.reshape %913 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %915 = stablehlo.slice %914 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %916 = stablehlo.reshape %915 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %917 = stablehlo.slice %914 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %918 = stablehlo.reshape %917 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %919 = stablehlo.complex %916, %918 : tensor<2048x32x64xcomplex<f32>>
    %920 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %921 = stablehlo.multiply %919, %920 : tensor<2048x32x64xcomplex<f32>>
    %922 = stablehlo.real %921 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %923 = stablehlo.reshape %922 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %924 = stablehlo.imag %921 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %925 = stablehlo.reshape %924 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %926 = stablehlo.concatenate %923, %925, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %927 = stablehlo.reshape %926 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %928 = stablehlo.transpose %927, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %929 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %930 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %931 = stablehlo.add %arg198, %930 : tensor<2048xi64>
    %932 = stablehlo.select %929, %931, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %933 = stablehlo.reshape %932 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %934 = stablehlo.transpose %arg244, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %935 = stablehlo.dot %911, %934, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %936 = stablehlo.reshape %935 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %937 = stablehlo.slice %936 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %938 = stablehlo.reshape %937 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %939 = stablehlo.slice %936 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %940 = stablehlo.reshape %939 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %941 = stablehlo.complex %938, %940 : tensor<2048x32x64xcomplex<f32>>
    %942 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %943 = stablehlo.multiply %941, %942 : tensor<2048x32x64xcomplex<f32>>
    %944 = stablehlo.real %943 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %945 = stablehlo.reshape %944 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %946 = stablehlo.imag %943 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %947 = stablehlo.reshape %946 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %948 = stablehlo.concatenate %945, %947, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %949 = stablehlo.reshape %948 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %950 = "stablehlo.scatter"(%arg245, %933, %949) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %951 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %952 = "stablehlo.gather"(%950, %951) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %953 = stablehlo.transpose %952, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %954 = stablehlo.dot_general %928, %953, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %955 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %956 = stablehlo.divide %954, %955 : tensor<32x2048x2048xf32>
    %957 = stablehlo.reshape %956 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %958 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %959 = stablehlo.broadcast_in_dim %958, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %960 = stablehlo.add %957, %959 : tensor<1x32x2048x2048xf32>
    %961 = stablehlo.reduce(%960 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %962 = stablehlo.broadcast_in_dim %961, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %963 = stablehlo.subtract %960, %962 : tensor<1x32x2048x2048xf32>
    %964 = stablehlo.exponential %963 : tensor<1x32x2048x2048xf32>
    %965 = stablehlo.reduce(%964 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %966 = stablehlo.broadcast_in_dim %965, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %967 = stablehlo.divide %964, %966 : tensor<1x32x2048x2048xf32>
    %968 = stablehlo.reshape %967 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %969 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %970 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %971 = stablehlo.add %arg198, %970 : tensor<2048xi64>
    %972 = stablehlo.select %969, %971, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %973 = stablehlo.reshape %972 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %974 = stablehlo.transpose %arg146, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %975 = stablehlo.dot %911, %974, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %976 = stablehlo.reshape %975 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %977 = "stablehlo.scatter"(%arg243, %973, %976) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %978 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %979 = "stablehlo.gather"(%977, %978) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %980 = stablehlo.transpose %979, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %981 = stablehlo.dot_general %968, %980, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %982 = stablehlo.reshape %981 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %983 = stablehlo.transpose %982, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %984 = stablehlo.reshape %983 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %985 = stablehlo.transpose %arg145, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %986 = stablehlo.dot %984, %985, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %987 = stablehlo.add %899, %986 : tensor<2048x4096xf32>
    %988 = stablehlo.power %987, %1 : tensor<2048x4096xf32>
    %989 = stablehlo.reduce(%988 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %990 = stablehlo.multiply %989, %0 : tensor<2048xf32>
    %991 = stablehlo.reshape %990 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %992 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %993 = stablehlo.add %991, %992 : tensor<2048x1xf32>
    %994 = stablehlo.rsqrt %993 : tensor<2048x1xf32>
    %995 = stablehlo.reshape %994 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %996 = stablehlo.broadcast_in_dim %995, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %997 = stablehlo.multiply %987, %996 : tensor<2048x4096xf32>
    %998 = stablehlo.broadcast_in_dim %arg144, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %999 = stablehlo.multiply %997, %998 : tensor<2048x4096xf32>
    %1000 = stablehlo.transpose %arg247, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1001 = stablehlo.dot %999, %1000, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1002 = stablehlo.logistic %1001 : tensor<2048x11008xf32>
    %1003 = stablehlo.multiply %1001, %1002 : tensor<2048x11008xf32>
    %1004 = stablehlo.transpose %arg143, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1005 = stablehlo.dot %999, %1004, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1006 = stablehlo.multiply %1003, %1005 : tensor<2048x11008xf32>
    %1007 = stablehlo.transpose %arg142, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %1008 = stablehlo.dot %1006, %1007, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %1009 = stablehlo.add %987, %1008 : tensor<2048x4096xf32>
    %1010 = stablehlo.power %1009, %1 : tensor<2048x4096xf32>
    %1011 = stablehlo.reduce(%1010 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1012 = stablehlo.multiply %1011, %0 : tensor<2048xf32>
    %1013 = stablehlo.reshape %1012 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1014 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1015 = stablehlo.add %1013, %1014 : tensor<2048x1xf32>
    %1016 = stablehlo.rsqrt %1015 : tensor<2048x1xf32>
    %1017 = stablehlo.reshape %1016 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1018 = stablehlo.broadcast_in_dim %1017, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1019 = stablehlo.multiply %1009, %1018 : tensor<2048x4096xf32>
    %1020 = stablehlo.broadcast_in_dim %arg141, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1021 = stablehlo.multiply %1019, %1020 : tensor<2048x4096xf32>
    %1022 = stablehlo.transpose %arg251, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1023 = stablehlo.dot %1021, %1022, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1024 = stablehlo.reshape %1023 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1025 = stablehlo.slice %1024 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1026 = stablehlo.reshape %1025 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1027 = stablehlo.slice %1024 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1028 = stablehlo.reshape %1027 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1029 = stablehlo.complex %1026, %1028 : tensor<2048x32x64xcomplex<f32>>
    %1030 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1031 = stablehlo.multiply %1029, %1030 : tensor<2048x32x64xcomplex<f32>>
    %1032 = stablehlo.real %1031 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1033 = stablehlo.reshape %1032 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1034 = stablehlo.imag %1031 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1035 = stablehlo.reshape %1034 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1036 = stablehlo.concatenate %1033, %1035, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1037 = stablehlo.reshape %1036 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1038 = stablehlo.transpose %1037, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1039 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1040 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1041 = stablehlo.add %arg198, %1040 : tensor<2048xi64>
    %1042 = stablehlo.select %1039, %1041, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1043 = stablehlo.reshape %1042 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1044 = stablehlo.transpose %arg249, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1045 = stablehlo.dot %1021, %1044, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1046 = stablehlo.reshape %1045 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1047 = stablehlo.slice %1046 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1048 = stablehlo.reshape %1047 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1049 = stablehlo.slice %1046 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1050 = stablehlo.reshape %1049 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1051 = stablehlo.complex %1048, %1050 : tensor<2048x32x64xcomplex<f32>>
    %1052 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1053 = stablehlo.multiply %1051, %1052 : tensor<2048x32x64xcomplex<f32>>
    %1054 = stablehlo.real %1053 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1055 = stablehlo.reshape %1054 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1056 = stablehlo.imag %1053 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1057 = stablehlo.reshape %1056 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1058 = stablehlo.concatenate %1055, %1057, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1059 = stablehlo.reshape %1058 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1060 = "stablehlo.scatter"(%arg250, %1043, %1059) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1061 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1062 = "stablehlo.gather"(%1060, %1061) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1063 = stablehlo.transpose %1062, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %1064 = stablehlo.dot_general %1038, %1063, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %1065 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %1066 = stablehlo.divide %1064, %1065 : tensor<32x2048x2048xf32>
    %1067 = stablehlo.reshape %1066 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1068 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %1069 = stablehlo.broadcast_in_dim %1068, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1070 = stablehlo.add %1067, %1069 : tensor<1x32x2048x2048xf32>
    %1071 = stablehlo.reduce(%1070 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1072 = stablehlo.broadcast_in_dim %1071, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1073 = stablehlo.subtract %1070, %1072 : tensor<1x32x2048x2048xf32>
    %1074 = stablehlo.exponential %1073 : tensor<1x32x2048x2048xf32>
    %1075 = stablehlo.reduce(%1074 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1076 = stablehlo.broadcast_in_dim %1075, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1077 = stablehlo.divide %1074, %1076 : tensor<1x32x2048x2048xf32>
    %1078 = stablehlo.reshape %1077 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %1079 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1080 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1081 = stablehlo.add %arg198, %1080 : tensor<2048xi64>
    %1082 = stablehlo.select %1079, %1081, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1083 = stablehlo.reshape %1082 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1084 = stablehlo.transpose %arg140, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1085 = stablehlo.dot %1021, %1084, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1086 = stablehlo.reshape %1085 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %1087 = "stablehlo.scatter"(%arg248, %1083, %1086) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1088 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1089 = "stablehlo.gather"(%1087, %1088) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1090 = stablehlo.transpose %1089, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1091 = stablehlo.dot_general %1078, %1090, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %1092 = stablehlo.reshape %1091 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %1093 = stablehlo.transpose %1092, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %1094 = stablehlo.reshape %1093 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %1095 = stablehlo.transpose %arg139, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1096 = stablehlo.dot %1094, %1095, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1097 = stablehlo.add %1009, %1096 : tensor<2048x4096xf32>
    %1098 = stablehlo.power %1097, %1 : tensor<2048x4096xf32>
    %1099 = stablehlo.reduce(%1098 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1100 = stablehlo.multiply %1099, %0 : tensor<2048xf32>
    %1101 = stablehlo.reshape %1100 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1102 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1103 = stablehlo.add %1101, %1102 : tensor<2048x1xf32>
    %1104 = stablehlo.rsqrt %1103 : tensor<2048x1xf32>
    %1105 = stablehlo.reshape %1104 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1106 = stablehlo.broadcast_in_dim %1105, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1107 = stablehlo.multiply %1097, %1106 : tensor<2048x4096xf32>
    %1108 = stablehlo.broadcast_in_dim %arg138, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1109 = stablehlo.multiply %1107, %1108 : tensor<2048x4096xf32>
    %1110 = stablehlo.transpose %arg252, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1111 = stablehlo.dot %1109, %1110, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1112 = stablehlo.logistic %1111 : tensor<2048x11008xf32>
    %1113 = stablehlo.multiply %1111, %1112 : tensor<2048x11008xf32>
    %1114 = stablehlo.transpose %arg137, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1115 = stablehlo.dot %1109, %1114, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1116 = stablehlo.multiply %1113, %1115 : tensor<2048x11008xf32>
    %1117 = stablehlo.transpose %arg136, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %1118 = stablehlo.dot %1116, %1117, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %1119 = stablehlo.add %1097, %1118 : tensor<2048x4096xf32>
    %1120 = stablehlo.power %1119, %1 : tensor<2048x4096xf32>
    %1121 = stablehlo.reduce(%1120 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1122 = stablehlo.multiply %1121, %0 : tensor<2048xf32>
    %1123 = stablehlo.reshape %1122 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1124 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1125 = stablehlo.add %1123, %1124 : tensor<2048x1xf32>
    %1126 = stablehlo.rsqrt %1125 : tensor<2048x1xf32>
    %1127 = stablehlo.reshape %1126 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1128 = stablehlo.broadcast_in_dim %1127, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1129 = stablehlo.multiply %1119, %1128 : tensor<2048x4096xf32>
    %1130 = stablehlo.broadcast_in_dim %arg135, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1131 = stablehlo.multiply %1129, %1130 : tensor<2048x4096xf32>
    %1132 = stablehlo.transpose %arg256, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1133 = stablehlo.dot %1131, %1132, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1134 = stablehlo.reshape %1133 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1135 = stablehlo.slice %1134 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1136 = stablehlo.reshape %1135 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1137 = stablehlo.slice %1134 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1138 = stablehlo.reshape %1137 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1139 = stablehlo.complex %1136, %1138 : tensor<2048x32x64xcomplex<f32>>
    %1140 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1141 = stablehlo.multiply %1139, %1140 : tensor<2048x32x64xcomplex<f32>>
    %1142 = stablehlo.real %1141 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1143 = stablehlo.reshape %1142 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1144 = stablehlo.imag %1141 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1145 = stablehlo.reshape %1144 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1146 = stablehlo.concatenate %1143, %1145, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1147 = stablehlo.reshape %1146 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1148 = stablehlo.transpose %1147, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1149 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1150 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1151 = stablehlo.add %arg198, %1150 : tensor<2048xi64>
    %1152 = stablehlo.select %1149, %1151, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1153 = stablehlo.reshape %1152 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1154 = stablehlo.transpose %arg254, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1155 = stablehlo.dot %1131, %1154, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1156 = stablehlo.reshape %1155 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1157 = stablehlo.slice %1156 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1158 = stablehlo.reshape %1157 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1159 = stablehlo.slice %1156 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1160 = stablehlo.reshape %1159 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1161 = stablehlo.complex %1158, %1160 : tensor<2048x32x64xcomplex<f32>>
    %1162 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1163 = stablehlo.multiply %1161, %1162 : tensor<2048x32x64xcomplex<f32>>
    %1164 = stablehlo.real %1163 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1165 = stablehlo.reshape %1164 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1166 = stablehlo.imag %1163 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1167 = stablehlo.reshape %1166 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1168 = stablehlo.concatenate %1165, %1167, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1169 = stablehlo.reshape %1168 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1170 = "stablehlo.scatter"(%arg255, %1153, %1169) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1171 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1172 = "stablehlo.gather"(%1170, %1171) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1173 = stablehlo.transpose %1172, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %1174 = stablehlo.dot_general %1148, %1173, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %1175 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %1176 = stablehlo.divide %1174, %1175 : tensor<32x2048x2048xf32>
    %1177 = stablehlo.reshape %1176 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1178 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %1179 = stablehlo.broadcast_in_dim %1178, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1180 = stablehlo.add %1177, %1179 : tensor<1x32x2048x2048xf32>
    %1181 = stablehlo.reduce(%1180 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1182 = stablehlo.broadcast_in_dim %1181, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1183 = stablehlo.subtract %1180, %1182 : tensor<1x32x2048x2048xf32>
    %1184 = stablehlo.exponential %1183 : tensor<1x32x2048x2048xf32>
    %1185 = stablehlo.reduce(%1184 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1186 = stablehlo.broadcast_in_dim %1185, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1187 = stablehlo.divide %1184, %1186 : tensor<1x32x2048x2048xf32>
    %1188 = stablehlo.reshape %1187 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %1189 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1190 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1191 = stablehlo.add %arg198, %1190 : tensor<2048xi64>
    %1192 = stablehlo.select %1189, %1191, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1193 = stablehlo.reshape %1192 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1194 = stablehlo.transpose %arg134, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1195 = stablehlo.dot %1131, %1194, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1196 = stablehlo.reshape %1195 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %1197 = "stablehlo.scatter"(%arg253, %1193, %1196) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1198 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1199 = "stablehlo.gather"(%1197, %1198) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1200 = stablehlo.transpose %1199, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1201 = stablehlo.dot_general %1188, %1200, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %1202 = stablehlo.reshape %1201 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %1203 = stablehlo.transpose %1202, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %1204 = stablehlo.reshape %1203 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %1205 = stablehlo.transpose %arg133, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1206 = stablehlo.dot %1204, %1205, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1207 = stablehlo.add %1119, %1206 : tensor<2048x4096xf32>
    %1208 = stablehlo.power %1207, %1 : tensor<2048x4096xf32>
    %1209 = stablehlo.reduce(%1208 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1210 = stablehlo.multiply %1209, %0 : tensor<2048xf32>
    %1211 = stablehlo.reshape %1210 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1212 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1213 = stablehlo.add %1211, %1212 : tensor<2048x1xf32>
    %1214 = stablehlo.rsqrt %1213 : tensor<2048x1xf32>
    %1215 = stablehlo.reshape %1214 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1216 = stablehlo.broadcast_in_dim %1215, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1217 = stablehlo.multiply %1207, %1216 : tensor<2048x4096xf32>
    %1218 = stablehlo.broadcast_in_dim %arg132, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1219 = stablehlo.multiply %1217, %1218 : tensor<2048x4096xf32>
    %1220 = stablehlo.transpose %arg257, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1221 = stablehlo.dot %1219, %1220, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1222 = stablehlo.logistic %1221 : tensor<2048x11008xf32>
    %1223 = stablehlo.multiply %1221, %1222 : tensor<2048x11008xf32>
    %1224 = stablehlo.transpose %arg131, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1225 = stablehlo.dot %1219, %1224, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1226 = stablehlo.multiply %1223, %1225 : tensor<2048x11008xf32>
    %1227 = stablehlo.transpose %arg130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %1228 = stablehlo.dot %1226, %1227, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %1229 = stablehlo.add %1207, %1228 : tensor<2048x4096xf32>
    %1230 = stablehlo.power %1229, %1 : tensor<2048x4096xf32>
    %1231 = stablehlo.reduce(%1230 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1232 = stablehlo.multiply %1231, %0 : tensor<2048xf32>
    %1233 = stablehlo.reshape %1232 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1234 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1235 = stablehlo.add %1233, %1234 : tensor<2048x1xf32>
    %1236 = stablehlo.rsqrt %1235 : tensor<2048x1xf32>
    %1237 = stablehlo.reshape %1236 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1238 = stablehlo.broadcast_in_dim %1237, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1239 = stablehlo.multiply %1229, %1238 : tensor<2048x4096xf32>
    %1240 = stablehlo.broadcast_in_dim %arg129, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1241 = stablehlo.multiply %1239, %1240 : tensor<2048x4096xf32>
    %1242 = stablehlo.transpose %arg261, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1243 = stablehlo.dot %1241, %1242, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1244 = stablehlo.reshape %1243 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1245 = stablehlo.slice %1244 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1246 = stablehlo.reshape %1245 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1247 = stablehlo.slice %1244 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1248 = stablehlo.reshape %1247 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1249 = stablehlo.complex %1246, %1248 : tensor<2048x32x64xcomplex<f32>>
    %1250 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1251 = stablehlo.multiply %1249, %1250 : tensor<2048x32x64xcomplex<f32>>
    %1252 = stablehlo.real %1251 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1253 = stablehlo.reshape %1252 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1254 = stablehlo.imag %1251 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1255 = stablehlo.reshape %1254 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1256 = stablehlo.concatenate %1253, %1255, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1257 = stablehlo.reshape %1256 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1258 = stablehlo.transpose %1257, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1259 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1260 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1261 = stablehlo.add %arg198, %1260 : tensor<2048xi64>
    %1262 = stablehlo.select %1259, %1261, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1263 = stablehlo.reshape %1262 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1264 = stablehlo.transpose %arg259, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1265 = stablehlo.dot %1241, %1264, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1266 = stablehlo.reshape %1265 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1267 = stablehlo.slice %1266 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1268 = stablehlo.reshape %1267 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1269 = stablehlo.slice %1266 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1270 = stablehlo.reshape %1269 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1271 = stablehlo.complex %1268, %1270 : tensor<2048x32x64xcomplex<f32>>
    %1272 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1273 = stablehlo.multiply %1271, %1272 : tensor<2048x32x64xcomplex<f32>>
    %1274 = stablehlo.real %1273 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1275 = stablehlo.reshape %1274 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1276 = stablehlo.imag %1273 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1277 = stablehlo.reshape %1276 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1278 = stablehlo.concatenate %1275, %1277, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1279 = stablehlo.reshape %1278 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1280 = "stablehlo.scatter"(%arg260, %1263, %1279) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1281 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1282 = "stablehlo.gather"(%1280, %1281) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1283 = stablehlo.transpose %1282, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %1284 = stablehlo.dot_general %1258, %1283, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %1285 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %1286 = stablehlo.divide %1284, %1285 : tensor<32x2048x2048xf32>
    %1287 = stablehlo.reshape %1286 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1288 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %1289 = stablehlo.broadcast_in_dim %1288, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1290 = stablehlo.add %1287, %1289 : tensor<1x32x2048x2048xf32>
    %1291 = stablehlo.reduce(%1290 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1292 = stablehlo.broadcast_in_dim %1291, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1293 = stablehlo.subtract %1290, %1292 : tensor<1x32x2048x2048xf32>
    %1294 = stablehlo.exponential %1293 : tensor<1x32x2048x2048xf32>
    %1295 = stablehlo.reduce(%1294 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1296 = stablehlo.broadcast_in_dim %1295, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1297 = stablehlo.divide %1294, %1296 : tensor<1x32x2048x2048xf32>
    %1298 = stablehlo.reshape %1297 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %1299 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1300 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1301 = stablehlo.add %arg198, %1300 : tensor<2048xi64>
    %1302 = stablehlo.select %1299, %1301, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1303 = stablehlo.reshape %1302 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1304 = stablehlo.transpose %arg128, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1305 = stablehlo.dot %1241, %1304, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1306 = stablehlo.reshape %1305 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %1307 = "stablehlo.scatter"(%arg258, %1303, %1306) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1308 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1309 = "stablehlo.gather"(%1307, %1308) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1310 = stablehlo.transpose %1309, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1311 = stablehlo.dot_general %1298, %1310, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %1312 = stablehlo.reshape %1311 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %1313 = stablehlo.transpose %1312, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %1314 = stablehlo.reshape %1313 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %1315 = stablehlo.transpose %arg127, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1316 = stablehlo.dot %1314, %1315, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1317 = stablehlo.add %1229, %1316 : tensor<2048x4096xf32>
    %1318 = stablehlo.power %1317, %1 : tensor<2048x4096xf32>
    %1319 = stablehlo.reduce(%1318 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1320 = stablehlo.multiply %1319, %0 : tensor<2048xf32>
    %1321 = stablehlo.reshape %1320 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1322 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1323 = stablehlo.add %1321, %1322 : tensor<2048x1xf32>
    %1324 = stablehlo.rsqrt %1323 : tensor<2048x1xf32>
    %1325 = stablehlo.reshape %1324 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1326 = stablehlo.broadcast_in_dim %1325, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1327 = stablehlo.multiply %1317, %1326 : tensor<2048x4096xf32>
    %1328 = stablehlo.broadcast_in_dim %arg126, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1329 = stablehlo.multiply %1327, %1328 : tensor<2048x4096xf32>
    %1330 = stablehlo.transpose %arg262, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1331 = stablehlo.dot %1329, %1330, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1332 = stablehlo.logistic %1331 : tensor<2048x11008xf32>
    %1333 = stablehlo.multiply %1331, %1332 : tensor<2048x11008xf32>
    %1334 = stablehlo.transpose %arg125, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1335 = stablehlo.dot %1329, %1334, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1336 = stablehlo.multiply %1333, %1335 : tensor<2048x11008xf32>
    %1337 = stablehlo.transpose %arg124, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %1338 = stablehlo.dot %1336, %1337, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %1339 = stablehlo.add %1317, %1338 : tensor<2048x4096xf32>
    %1340 = stablehlo.power %1339, %1 : tensor<2048x4096xf32>
    %1341 = stablehlo.reduce(%1340 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1342 = stablehlo.multiply %1341, %0 : tensor<2048xf32>
    %1343 = stablehlo.reshape %1342 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1344 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1345 = stablehlo.add %1343, %1344 : tensor<2048x1xf32>
    %1346 = stablehlo.rsqrt %1345 : tensor<2048x1xf32>
    %1347 = stablehlo.reshape %1346 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1348 = stablehlo.broadcast_in_dim %1347, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1349 = stablehlo.multiply %1339, %1348 : tensor<2048x4096xf32>
    %1350 = stablehlo.broadcast_in_dim %arg123, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1351 = stablehlo.multiply %1349, %1350 : tensor<2048x4096xf32>
    %1352 = stablehlo.transpose %arg266, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1353 = stablehlo.dot %1351, %1352, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1354 = stablehlo.reshape %1353 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1355 = stablehlo.slice %1354 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1356 = stablehlo.reshape %1355 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1357 = stablehlo.slice %1354 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1358 = stablehlo.reshape %1357 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1359 = stablehlo.complex %1356, %1358 : tensor<2048x32x64xcomplex<f32>>
    %1360 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1361 = stablehlo.multiply %1359, %1360 : tensor<2048x32x64xcomplex<f32>>
    %1362 = stablehlo.real %1361 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1363 = stablehlo.reshape %1362 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1364 = stablehlo.imag %1361 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1365 = stablehlo.reshape %1364 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1366 = stablehlo.concatenate %1363, %1365, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1367 = stablehlo.reshape %1366 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1368 = stablehlo.transpose %1367, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1369 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1370 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1371 = stablehlo.add %arg198, %1370 : tensor<2048xi64>
    %1372 = stablehlo.select %1369, %1371, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1373 = stablehlo.reshape %1372 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1374 = stablehlo.transpose %arg264, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1375 = stablehlo.dot %1351, %1374, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1376 = stablehlo.reshape %1375 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1377 = stablehlo.slice %1376 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1378 = stablehlo.reshape %1377 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1379 = stablehlo.slice %1376 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1380 = stablehlo.reshape %1379 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1381 = stablehlo.complex %1378, %1380 : tensor<2048x32x64xcomplex<f32>>
    %1382 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1383 = stablehlo.multiply %1381, %1382 : tensor<2048x32x64xcomplex<f32>>
    %1384 = stablehlo.real %1383 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1385 = stablehlo.reshape %1384 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1386 = stablehlo.imag %1383 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1387 = stablehlo.reshape %1386 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1388 = stablehlo.concatenate %1385, %1387, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1389 = stablehlo.reshape %1388 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1390 = "stablehlo.scatter"(%arg265, %1373, %1389) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1391 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1392 = "stablehlo.gather"(%1390, %1391) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1393 = stablehlo.transpose %1392, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %1394 = stablehlo.dot_general %1368, %1393, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %1395 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %1396 = stablehlo.divide %1394, %1395 : tensor<32x2048x2048xf32>
    %1397 = stablehlo.reshape %1396 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1398 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %1399 = stablehlo.broadcast_in_dim %1398, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1400 = stablehlo.add %1397, %1399 : tensor<1x32x2048x2048xf32>
    %1401 = stablehlo.reduce(%1400 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1402 = stablehlo.broadcast_in_dim %1401, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1403 = stablehlo.subtract %1400, %1402 : tensor<1x32x2048x2048xf32>
    %1404 = stablehlo.exponential %1403 : tensor<1x32x2048x2048xf32>
    %1405 = stablehlo.reduce(%1404 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1406 = stablehlo.broadcast_in_dim %1405, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1407 = stablehlo.divide %1404, %1406 : tensor<1x32x2048x2048xf32>
    %1408 = stablehlo.reshape %1407 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %1409 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1410 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1411 = stablehlo.add %arg198, %1410 : tensor<2048xi64>
    %1412 = stablehlo.select %1409, %1411, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1413 = stablehlo.reshape %1412 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1414 = stablehlo.transpose %arg122, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1415 = stablehlo.dot %1351, %1414, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1416 = stablehlo.reshape %1415 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %1417 = "stablehlo.scatter"(%arg263, %1413, %1416) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1418 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1419 = "stablehlo.gather"(%1417, %1418) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1420 = stablehlo.transpose %1419, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1421 = stablehlo.dot_general %1408, %1420, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %1422 = stablehlo.reshape %1421 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %1423 = stablehlo.transpose %1422, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %1424 = stablehlo.reshape %1423 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %1425 = stablehlo.transpose %arg121, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1426 = stablehlo.dot %1424, %1425, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1427 = stablehlo.add %1339, %1426 : tensor<2048x4096xf32>
    %1428 = stablehlo.power %1427, %1 : tensor<2048x4096xf32>
    %1429 = stablehlo.reduce(%1428 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1430 = stablehlo.multiply %1429, %0 : tensor<2048xf32>
    %1431 = stablehlo.reshape %1430 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1432 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1433 = stablehlo.add %1431, %1432 : tensor<2048x1xf32>
    %1434 = stablehlo.rsqrt %1433 : tensor<2048x1xf32>
    %1435 = stablehlo.reshape %1434 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1436 = stablehlo.broadcast_in_dim %1435, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1437 = stablehlo.multiply %1427, %1436 : tensor<2048x4096xf32>
    %1438 = stablehlo.broadcast_in_dim %arg120, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1439 = stablehlo.multiply %1437, %1438 : tensor<2048x4096xf32>
    %1440 = stablehlo.transpose %arg267, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1441 = stablehlo.dot %1439, %1440, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1442 = stablehlo.logistic %1441 : tensor<2048x11008xf32>
    %1443 = stablehlo.multiply %1441, %1442 : tensor<2048x11008xf32>
    %1444 = stablehlo.transpose %arg119, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1445 = stablehlo.dot %1439, %1444, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1446 = stablehlo.multiply %1443, %1445 : tensor<2048x11008xf32>
    %1447 = stablehlo.transpose %arg118, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %1448 = stablehlo.dot %1446, %1447, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %1449 = stablehlo.add %1427, %1448 : tensor<2048x4096xf32>
    %1450 = stablehlo.power %1449, %1 : tensor<2048x4096xf32>
    %1451 = stablehlo.reduce(%1450 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1452 = stablehlo.multiply %1451, %0 : tensor<2048xf32>
    %1453 = stablehlo.reshape %1452 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1454 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1455 = stablehlo.add %1453, %1454 : tensor<2048x1xf32>
    %1456 = stablehlo.rsqrt %1455 : tensor<2048x1xf32>
    %1457 = stablehlo.reshape %1456 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1458 = stablehlo.broadcast_in_dim %1457, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1459 = stablehlo.multiply %1449, %1458 : tensor<2048x4096xf32>
    %1460 = stablehlo.broadcast_in_dim %arg117, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1461 = stablehlo.multiply %1459, %1460 : tensor<2048x4096xf32>
    %1462 = stablehlo.transpose %arg271, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1463 = stablehlo.dot %1461, %1462, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1464 = stablehlo.reshape %1463 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1465 = stablehlo.slice %1464 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1466 = stablehlo.reshape %1465 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1467 = stablehlo.slice %1464 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1468 = stablehlo.reshape %1467 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1469 = stablehlo.complex %1466, %1468 : tensor<2048x32x64xcomplex<f32>>
    %1470 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1471 = stablehlo.multiply %1469, %1470 : tensor<2048x32x64xcomplex<f32>>
    %1472 = stablehlo.real %1471 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1473 = stablehlo.reshape %1472 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1474 = stablehlo.imag %1471 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1475 = stablehlo.reshape %1474 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1476 = stablehlo.concatenate %1473, %1475, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1477 = stablehlo.reshape %1476 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1478 = stablehlo.transpose %1477, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1479 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1480 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1481 = stablehlo.add %arg198, %1480 : tensor<2048xi64>
    %1482 = stablehlo.select %1479, %1481, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1483 = stablehlo.reshape %1482 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1484 = stablehlo.transpose %arg269, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1485 = stablehlo.dot %1461, %1484, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1486 = stablehlo.reshape %1485 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1487 = stablehlo.slice %1486 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1488 = stablehlo.reshape %1487 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1489 = stablehlo.slice %1486 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1490 = stablehlo.reshape %1489 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1491 = stablehlo.complex %1488, %1490 : tensor<2048x32x64xcomplex<f32>>
    %1492 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1493 = stablehlo.multiply %1491, %1492 : tensor<2048x32x64xcomplex<f32>>
    %1494 = stablehlo.real %1493 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1495 = stablehlo.reshape %1494 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1496 = stablehlo.imag %1493 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1497 = stablehlo.reshape %1496 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1498 = stablehlo.concatenate %1495, %1497, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1499 = stablehlo.reshape %1498 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1500 = "stablehlo.scatter"(%arg270, %1483, %1499) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1501 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1502 = "stablehlo.gather"(%1500, %1501) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1503 = stablehlo.transpose %1502, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %1504 = stablehlo.dot_general %1478, %1503, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %1505 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %1506 = stablehlo.divide %1504, %1505 : tensor<32x2048x2048xf32>
    %1507 = stablehlo.reshape %1506 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1508 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %1509 = stablehlo.broadcast_in_dim %1508, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1510 = stablehlo.add %1507, %1509 : tensor<1x32x2048x2048xf32>
    %1511 = stablehlo.reduce(%1510 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1512 = stablehlo.broadcast_in_dim %1511, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1513 = stablehlo.subtract %1510, %1512 : tensor<1x32x2048x2048xf32>
    %1514 = stablehlo.exponential %1513 : tensor<1x32x2048x2048xf32>
    %1515 = stablehlo.reduce(%1514 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1516 = stablehlo.broadcast_in_dim %1515, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1517 = stablehlo.divide %1514, %1516 : tensor<1x32x2048x2048xf32>
    %1518 = stablehlo.reshape %1517 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %1519 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1520 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1521 = stablehlo.add %arg198, %1520 : tensor<2048xi64>
    %1522 = stablehlo.select %1519, %1521, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1523 = stablehlo.reshape %1522 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1524 = stablehlo.transpose %arg116, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1525 = stablehlo.dot %1461, %1524, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1526 = stablehlo.reshape %1525 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %1527 = "stablehlo.scatter"(%arg268, %1523, %1526) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1528 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1529 = "stablehlo.gather"(%1527, %1528) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1530 = stablehlo.transpose %1529, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1531 = stablehlo.dot_general %1518, %1530, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %1532 = stablehlo.reshape %1531 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %1533 = stablehlo.transpose %1532, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %1534 = stablehlo.reshape %1533 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %1535 = stablehlo.transpose %arg115, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1536 = stablehlo.dot %1534, %1535, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1537 = stablehlo.add %1449, %1536 : tensor<2048x4096xf32>
    %1538 = stablehlo.power %1537, %1 : tensor<2048x4096xf32>
    %1539 = stablehlo.reduce(%1538 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1540 = stablehlo.multiply %1539, %0 : tensor<2048xf32>
    %1541 = stablehlo.reshape %1540 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1542 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1543 = stablehlo.add %1541, %1542 : tensor<2048x1xf32>
    %1544 = stablehlo.rsqrt %1543 : tensor<2048x1xf32>
    %1545 = stablehlo.reshape %1544 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1546 = stablehlo.broadcast_in_dim %1545, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1547 = stablehlo.multiply %1537, %1546 : tensor<2048x4096xf32>
    %1548 = stablehlo.broadcast_in_dim %arg114, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1549 = stablehlo.multiply %1547, %1548 : tensor<2048x4096xf32>
    %1550 = stablehlo.transpose %arg272, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1551 = stablehlo.dot %1549, %1550, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1552 = stablehlo.logistic %1551 : tensor<2048x11008xf32>
    %1553 = stablehlo.multiply %1551, %1552 : tensor<2048x11008xf32>
    %1554 = stablehlo.transpose %arg113, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1555 = stablehlo.dot %1549, %1554, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1556 = stablehlo.multiply %1553, %1555 : tensor<2048x11008xf32>
    %1557 = stablehlo.transpose %arg112, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %1558 = stablehlo.dot %1556, %1557, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %1559 = stablehlo.add %1537, %1558 : tensor<2048x4096xf32>
    %1560 = stablehlo.power %1559, %1 : tensor<2048x4096xf32>
    %1561 = stablehlo.reduce(%1560 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1562 = stablehlo.multiply %1561, %0 : tensor<2048xf32>
    %1563 = stablehlo.reshape %1562 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1564 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1565 = stablehlo.add %1563, %1564 : tensor<2048x1xf32>
    %1566 = stablehlo.rsqrt %1565 : tensor<2048x1xf32>
    %1567 = stablehlo.reshape %1566 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1568 = stablehlo.broadcast_in_dim %1567, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1569 = stablehlo.multiply %1559, %1568 : tensor<2048x4096xf32>
    %1570 = stablehlo.broadcast_in_dim %arg111, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1571 = stablehlo.multiply %1569, %1570 : tensor<2048x4096xf32>
    %1572 = stablehlo.transpose %arg276, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1573 = stablehlo.dot %1571, %1572, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1574 = stablehlo.reshape %1573 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1575 = stablehlo.slice %1574 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1576 = stablehlo.reshape %1575 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1577 = stablehlo.slice %1574 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1578 = stablehlo.reshape %1577 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1579 = stablehlo.complex %1576, %1578 : tensor<2048x32x64xcomplex<f32>>
    %1580 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1581 = stablehlo.multiply %1579, %1580 : tensor<2048x32x64xcomplex<f32>>
    %1582 = stablehlo.real %1581 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1583 = stablehlo.reshape %1582 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1584 = stablehlo.imag %1581 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1585 = stablehlo.reshape %1584 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1586 = stablehlo.concatenate %1583, %1585, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1587 = stablehlo.reshape %1586 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1588 = stablehlo.transpose %1587, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1589 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1590 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1591 = stablehlo.add %arg198, %1590 : tensor<2048xi64>
    %1592 = stablehlo.select %1589, %1591, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1593 = stablehlo.reshape %1592 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1594 = stablehlo.transpose %arg274, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1595 = stablehlo.dot %1571, %1594, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1596 = stablehlo.reshape %1595 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1597 = stablehlo.slice %1596 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1598 = stablehlo.reshape %1597 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1599 = stablehlo.slice %1596 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1600 = stablehlo.reshape %1599 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1601 = stablehlo.complex %1598, %1600 : tensor<2048x32x64xcomplex<f32>>
    %1602 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1603 = stablehlo.multiply %1601, %1602 : tensor<2048x32x64xcomplex<f32>>
    %1604 = stablehlo.real %1603 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1605 = stablehlo.reshape %1604 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1606 = stablehlo.imag %1603 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1607 = stablehlo.reshape %1606 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1608 = stablehlo.concatenate %1605, %1607, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1609 = stablehlo.reshape %1608 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1610 = "stablehlo.scatter"(%arg275, %1593, %1609) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1611 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1612 = "stablehlo.gather"(%1610, %1611) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1613 = stablehlo.transpose %1612, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %1614 = stablehlo.dot_general %1588, %1613, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %1615 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %1616 = stablehlo.divide %1614, %1615 : tensor<32x2048x2048xf32>
    %1617 = stablehlo.reshape %1616 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1618 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %1619 = stablehlo.broadcast_in_dim %1618, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1620 = stablehlo.add %1617, %1619 : tensor<1x32x2048x2048xf32>
    %1621 = stablehlo.reduce(%1620 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1622 = stablehlo.broadcast_in_dim %1621, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1623 = stablehlo.subtract %1620, %1622 : tensor<1x32x2048x2048xf32>
    %1624 = stablehlo.exponential %1623 : tensor<1x32x2048x2048xf32>
    %1625 = stablehlo.reduce(%1624 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1626 = stablehlo.broadcast_in_dim %1625, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1627 = stablehlo.divide %1624, %1626 : tensor<1x32x2048x2048xf32>
    %1628 = stablehlo.reshape %1627 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %1629 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1630 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1631 = stablehlo.add %arg198, %1630 : tensor<2048xi64>
    %1632 = stablehlo.select %1629, %1631, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1633 = stablehlo.reshape %1632 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1634 = stablehlo.transpose %arg110, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1635 = stablehlo.dot %1571, %1634, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1636 = stablehlo.reshape %1635 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %1637 = "stablehlo.scatter"(%arg273, %1633, %1636) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1638 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1639 = "stablehlo.gather"(%1637, %1638) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1640 = stablehlo.transpose %1639, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1641 = stablehlo.dot_general %1628, %1640, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %1642 = stablehlo.reshape %1641 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %1643 = stablehlo.transpose %1642, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %1644 = stablehlo.reshape %1643 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %1645 = stablehlo.transpose %arg109, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1646 = stablehlo.dot %1644, %1645, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1647 = stablehlo.add %1559, %1646 : tensor<2048x4096xf32>
    %1648 = stablehlo.power %1647, %1 : tensor<2048x4096xf32>
    %1649 = stablehlo.reduce(%1648 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1650 = stablehlo.multiply %1649, %0 : tensor<2048xf32>
    %1651 = stablehlo.reshape %1650 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1652 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1653 = stablehlo.add %1651, %1652 : tensor<2048x1xf32>
    %1654 = stablehlo.rsqrt %1653 : tensor<2048x1xf32>
    %1655 = stablehlo.reshape %1654 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1656 = stablehlo.broadcast_in_dim %1655, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1657 = stablehlo.multiply %1647, %1656 : tensor<2048x4096xf32>
    %1658 = stablehlo.broadcast_in_dim %arg108, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1659 = stablehlo.multiply %1657, %1658 : tensor<2048x4096xf32>
    %1660 = stablehlo.transpose %arg277, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1661 = stablehlo.dot %1659, %1660, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1662 = stablehlo.logistic %1661 : tensor<2048x11008xf32>
    %1663 = stablehlo.multiply %1661, %1662 : tensor<2048x11008xf32>
    %1664 = stablehlo.transpose %arg107, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1665 = stablehlo.dot %1659, %1664, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1666 = stablehlo.multiply %1663, %1665 : tensor<2048x11008xf32>
    %1667 = stablehlo.transpose %arg106, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %1668 = stablehlo.dot %1666, %1667, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %1669 = stablehlo.add %1647, %1668 : tensor<2048x4096xf32>
    %1670 = stablehlo.power %1669, %1 : tensor<2048x4096xf32>
    %1671 = stablehlo.reduce(%1670 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1672 = stablehlo.multiply %1671, %0 : tensor<2048xf32>
    %1673 = stablehlo.reshape %1672 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1674 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1675 = stablehlo.add %1673, %1674 : tensor<2048x1xf32>
    %1676 = stablehlo.rsqrt %1675 : tensor<2048x1xf32>
    %1677 = stablehlo.reshape %1676 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1678 = stablehlo.broadcast_in_dim %1677, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1679 = stablehlo.multiply %1669, %1678 : tensor<2048x4096xf32>
    %1680 = stablehlo.broadcast_in_dim %arg105, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1681 = stablehlo.multiply %1679, %1680 : tensor<2048x4096xf32>
    %1682 = stablehlo.transpose %arg281, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1683 = stablehlo.dot %1681, %1682, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1684 = stablehlo.reshape %1683 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1685 = stablehlo.slice %1684 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1686 = stablehlo.reshape %1685 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1687 = stablehlo.slice %1684 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1688 = stablehlo.reshape %1687 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1689 = stablehlo.complex %1686, %1688 : tensor<2048x32x64xcomplex<f32>>
    %1690 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1691 = stablehlo.multiply %1689, %1690 : tensor<2048x32x64xcomplex<f32>>
    %1692 = stablehlo.real %1691 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1693 = stablehlo.reshape %1692 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1694 = stablehlo.imag %1691 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1695 = stablehlo.reshape %1694 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1696 = stablehlo.concatenate %1693, %1695, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1697 = stablehlo.reshape %1696 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1698 = stablehlo.transpose %1697, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1699 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1700 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1701 = stablehlo.add %arg198, %1700 : tensor<2048xi64>
    %1702 = stablehlo.select %1699, %1701, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1703 = stablehlo.reshape %1702 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1704 = stablehlo.transpose %arg279, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1705 = stablehlo.dot %1681, %1704, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1706 = stablehlo.reshape %1705 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1707 = stablehlo.slice %1706 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1708 = stablehlo.reshape %1707 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1709 = stablehlo.slice %1706 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1710 = stablehlo.reshape %1709 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1711 = stablehlo.complex %1708, %1710 : tensor<2048x32x64xcomplex<f32>>
    %1712 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1713 = stablehlo.multiply %1711, %1712 : tensor<2048x32x64xcomplex<f32>>
    %1714 = stablehlo.real %1713 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1715 = stablehlo.reshape %1714 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1716 = stablehlo.imag %1713 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1717 = stablehlo.reshape %1716 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1718 = stablehlo.concatenate %1715, %1717, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1719 = stablehlo.reshape %1718 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1720 = "stablehlo.scatter"(%arg280, %1703, %1719) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1721 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1722 = "stablehlo.gather"(%1720, %1721) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1723 = stablehlo.transpose %1722, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %1724 = stablehlo.dot_general %1698, %1723, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %1725 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %1726 = stablehlo.divide %1724, %1725 : tensor<32x2048x2048xf32>
    %1727 = stablehlo.reshape %1726 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1728 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %1729 = stablehlo.broadcast_in_dim %1728, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1730 = stablehlo.add %1727, %1729 : tensor<1x32x2048x2048xf32>
    %1731 = stablehlo.reduce(%1730 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1732 = stablehlo.broadcast_in_dim %1731, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1733 = stablehlo.subtract %1730, %1732 : tensor<1x32x2048x2048xf32>
    %1734 = stablehlo.exponential %1733 : tensor<1x32x2048x2048xf32>
    %1735 = stablehlo.reduce(%1734 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1736 = stablehlo.broadcast_in_dim %1735, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1737 = stablehlo.divide %1734, %1736 : tensor<1x32x2048x2048xf32>
    %1738 = stablehlo.reshape %1737 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %1739 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1740 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1741 = stablehlo.add %arg198, %1740 : tensor<2048xi64>
    %1742 = stablehlo.select %1739, %1741, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1743 = stablehlo.reshape %1742 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1744 = stablehlo.transpose %arg104, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1745 = stablehlo.dot %1681, %1744, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1746 = stablehlo.reshape %1745 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %1747 = "stablehlo.scatter"(%arg278, %1743, %1746) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1748 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1749 = "stablehlo.gather"(%1747, %1748) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1750 = stablehlo.transpose %1749, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1751 = stablehlo.dot_general %1738, %1750, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %1752 = stablehlo.reshape %1751 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %1753 = stablehlo.transpose %1752, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %1754 = stablehlo.reshape %1753 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %1755 = stablehlo.transpose %arg103, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1756 = stablehlo.dot %1754, %1755, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1757 = stablehlo.add %1669, %1756 : tensor<2048x4096xf32>
    %1758 = stablehlo.power %1757, %1 : tensor<2048x4096xf32>
    %1759 = stablehlo.reduce(%1758 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1760 = stablehlo.multiply %1759, %0 : tensor<2048xf32>
    %1761 = stablehlo.reshape %1760 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1762 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1763 = stablehlo.add %1761, %1762 : tensor<2048x1xf32>
    %1764 = stablehlo.rsqrt %1763 : tensor<2048x1xf32>
    %1765 = stablehlo.reshape %1764 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1766 = stablehlo.broadcast_in_dim %1765, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1767 = stablehlo.multiply %1757, %1766 : tensor<2048x4096xf32>
    %1768 = stablehlo.broadcast_in_dim %arg102, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1769 = stablehlo.multiply %1767, %1768 : tensor<2048x4096xf32>
    %1770 = stablehlo.transpose %arg282, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1771 = stablehlo.dot %1769, %1770, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1772 = stablehlo.logistic %1771 : tensor<2048x11008xf32>
    %1773 = stablehlo.multiply %1771, %1772 : tensor<2048x11008xf32>
    %1774 = stablehlo.transpose %arg101, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1775 = stablehlo.dot %1769, %1774, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1776 = stablehlo.multiply %1773, %1775 : tensor<2048x11008xf32>
    %1777 = stablehlo.transpose %arg100, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %1778 = stablehlo.dot %1776, %1777, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %1779 = stablehlo.add %1757, %1778 : tensor<2048x4096xf32>
    %1780 = stablehlo.power %1779, %1 : tensor<2048x4096xf32>
    %1781 = stablehlo.reduce(%1780 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1782 = stablehlo.multiply %1781, %0 : tensor<2048xf32>
    %1783 = stablehlo.reshape %1782 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1784 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1785 = stablehlo.add %1783, %1784 : tensor<2048x1xf32>
    %1786 = stablehlo.rsqrt %1785 : tensor<2048x1xf32>
    %1787 = stablehlo.reshape %1786 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1788 = stablehlo.broadcast_in_dim %1787, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1789 = stablehlo.multiply %1779, %1788 : tensor<2048x4096xf32>
    %1790 = stablehlo.broadcast_in_dim %arg99, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1791 = stablehlo.multiply %1789, %1790 : tensor<2048x4096xf32>
    %1792 = stablehlo.transpose %arg286, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1793 = stablehlo.dot %1791, %1792, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1794 = stablehlo.reshape %1793 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1795 = stablehlo.slice %1794 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1796 = stablehlo.reshape %1795 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1797 = stablehlo.slice %1794 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1798 = stablehlo.reshape %1797 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1799 = stablehlo.complex %1796, %1798 : tensor<2048x32x64xcomplex<f32>>
    %1800 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1801 = stablehlo.multiply %1799, %1800 : tensor<2048x32x64xcomplex<f32>>
    %1802 = stablehlo.real %1801 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1803 = stablehlo.reshape %1802 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1804 = stablehlo.imag %1801 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1805 = stablehlo.reshape %1804 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1806 = stablehlo.concatenate %1803, %1805, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1807 = stablehlo.reshape %1806 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1808 = stablehlo.transpose %1807, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1809 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1810 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1811 = stablehlo.add %arg198, %1810 : tensor<2048xi64>
    %1812 = stablehlo.select %1809, %1811, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1813 = stablehlo.reshape %1812 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1814 = stablehlo.transpose %arg284, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1815 = stablehlo.dot %1791, %1814, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1816 = stablehlo.reshape %1815 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1817 = stablehlo.slice %1816 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1818 = stablehlo.reshape %1817 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1819 = stablehlo.slice %1816 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1820 = stablehlo.reshape %1819 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1821 = stablehlo.complex %1818, %1820 : tensor<2048x32x64xcomplex<f32>>
    %1822 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1823 = stablehlo.multiply %1821, %1822 : tensor<2048x32x64xcomplex<f32>>
    %1824 = stablehlo.real %1823 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1825 = stablehlo.reshape %1824 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1826 = stablehlo.imag %1823 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1827 = stablehlo.reshape %1826 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1828 = stablehlo.concatenate %1825, %1827, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1829 = stablehlo.reshape %1828 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1830 = "stablehlo.scatter"(%arg285, %1813, %1829) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1831 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1832 = "stablehlo.gather"(%1830, %1831) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1833 = stablehlo.transpose %1832, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %1834 = stablehlo.dot_general %1808, %1833, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %1835 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %1836 = stablehlo.divide %1834, %1835 : tensor<32x2048x2048xf32>
    %1837 = stablehlo.reshape %1836 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1838 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %1839 = stablehlo.broadcast_in_dim %1838, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1840 = stablehlo.add %1837, %1839 : tensor<1x32x2048x2048xf32>
    %1841 = stablehlo.reduce(%1840 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1842 = stablehlo.broadcast_in_dim %1841, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1843 = stablehlo.subtract %1840, %1842 : tensor<1x32x2048x2048xf32>
    %1844 = stablehlo.exponential %1843 : tensor<1x32x2048x2048xf32>
    %1845 = stablehlo.reduce(%1844 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1846 = stablehlo.broadcast_in_dim %1845, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1847 = stablehlo.divide %1844, %1846 : tensor<1x32x2048x2048xf32>
    %1848 = stablehlo.reshape %1847 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %1849 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1850 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1851 = stablehlo.add %arg198, %1850 : tensor<2048xi64>
    %1852 = stablehlo.select %1849, %1851, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1853 = stablehlo.reshape %1852 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1854 = stablehlo.transpose %arg98, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1855 = stablehlo.dot %1791, %1854, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1856 = stablehlo.reshape %1855 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %1857 = "stablehlo.scatter"(%arg283, %1853, %1856) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1858 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1859 = "stablehlo.gather"(%1857, %1858) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1860 = stablehlo.transpose %1859, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1861 = stablehlo.dot_general %1848, %1860, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %1862 = stablehlo.reshape %1861 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %1863 = stablehlo.transpose %1862, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %1864 = stablehlo.reshape %1863 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %1865 = stablehlo.transpose %arg97, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1866 = stablehlo.dot %1864, %1865, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1867 = stablehlo.add %1779, %1866 : tensor<2048x4096xf32>
    %1868 = stablehlo.power %1867, %1 : tensor<2048x4096xf32>
    %1869 = stablehlo.reduce(%1868 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1870 = stablehlo.multiply %1869, %0 : tensor<2048xf32>
    %1871 = stablehlo.reshape %1870 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1872 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1873 = stablehlo.add %1871, %1872 : tensor<2048x1xf32>
    %1874 = stablehlo.rsqrt %1873 : tensor<2048x1xf32>
    %1875 = stablehlo.reshape %1874 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1876 = stablehlo.broadcast_in_dim %1875, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1877 = stablehlo.multiply %1867, %1876 : tensor<2048x4096xf32>
    %1878 = stablehlo.broadcast_in_dim %arg96, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1879 = stablehlo.multiply %1877, %1878 : tensor<2048x4096xf32>
    %1880 = stablehlo.transpose %arg287, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1881 = stablehlo.dot %1879, %1880, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1882 = stablehlo.logistic %1881 : tensor<2048x11008xf32>
    %1883 = stablehlo.multiply %1881, %1882 : tensor<2048x11008xf32>
    %1884 = stablehlo.transpose %arg95, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1885 = stablehlo.dot %1879, %1884, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1886 = stablehlo.multiply %1883, %1885 : tensor<2048x11008xf32>
    %1887 = stablehlo.transpose %arg94, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %1888 = stablehlo.dot %1886, %1887, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %1889 = stablehlo.add %1867, %1888 : tensor<2048x4096xf32>
    %1890 = stablehlo.power %1889, %1 : tensor<2048x4096xf32>
    %1891 = stablehlo.reduce(%1890 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1892 = stablehlo.multiply %1891, %0 : tensor<2048xf32>
    %1893 = stablehlo.reshape %1892 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1894 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1895 = stablehlo.add %1893, %1894 : tensor<2048x1xf32>
    %1896 = stablehlo.rsqrt %1895 : tensor<2048x1xf32>
    %1897 = stablehlo.reshape %1896 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1898 = stablehlo.broadcast_in_dim %1897, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1899 = stablehlo.multiply %1889, %1898 : tensor<2048x4096xf32>
    %1900 = stablehlo.broadcast_in_dim %arg93, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1901 = stablehlo.multiply %1899, %1900 : tensor<2048x4096xf32>
    %1902 = stablehlo.transpose %arg291, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1903 = stablehlo.dot %1901, %1902, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1904 = stablehlo.reshape %1903 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1905 = stablehlo.slice %1904 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1906 = stablehlo.reshape %1905 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1907 = stablehlo.slice %1904 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1908 = stablehlo.reshape %1907 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1909 = stablehlo.complex %1906, %1908 : tensor<2048x32x64xcomplex<f32>>
    %1910 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1911 = stablehlo.multiply %1909, %1910 : tensor<2048x32x64xcomplex<f32>>
    %1912 = stablehlo.real %1911 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1913 = stablehlo.reshape %1912 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1914 = stablehlo.imag %1911 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1915 = stablehlo.reshape %1914 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1916 = stablehlo.concatenate %1913, %1915, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1917 = stablehlo.reshape %1916 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1918 = stablehlo.transpose %1917, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1919 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1920 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1921 = stablehlo.add %arg198, %1920 : tensor<2048xi64>
    %1922 = stablehlo.select %1919, %1921, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1923 = stablehlo.reshape %1922 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1924 = stablehlo.transpose %arg289, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1925 = stablehlo.dot %1901, %1924, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1926 = stablehlo.reshape %1925 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %1927 = stablehlo.slice %1926 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1928 = stablehlo.reshape %1927 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1929 = stablehlo.slice %1926 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %1930 = stablehlo.reshape %1929 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %1931 = stablehlo.complex %1928, %1930 : tensor<2048x32x64xcomplex<f32>>
    %1932 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %1933 = stablehlo.multiply %1931, %1932 : tensor<2048x32x64xcomplex<f32>>
    %1934 = stablehlo.real %1933 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1935 = stablehlo.reshape %1934 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1936 = stablehlo.imag %1933 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %1937 = stablehlo.reshape %1936 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %1938 = stablehlo.concatenate %1935, %1937, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %1939 = stablehlo.reshape %1938 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %1940 = "stablehlo.scatter"(%arg290, %1923, %1939) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1941 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1942 = "stablehlo.gather"(%1940, %1941) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1943 = stablehlo.transpose %1942, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %1944 = stablehlo.dot_general %1918, %1943, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %1945 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %1946 = stablehlo.divide %1944, %1945 : tensor<32x2048x2048xf32>
    %1947 = stablehlo.reshape %1946 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1948 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %1949 = stablehlo.broadcast_in_dim %1948, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1950 = stablehlo.add %1947, %1949 : tensor<1x32x2048x2048xf32>
    %1951 = stablehlo.reduce(%1950 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1952 = stablehlo.broadcast_in_dim %1951, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1953 = stablehlo.subtract %1950, %1952 : tensor<1x32x2048x2048xf32>
    %1954 = stablehlo.exponential %1953 : tensor<1x32x2048x2048xf32>
    %1955 = stablehlo.reduce(%1954 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1956 = stablehlo.broadcast_in_dim %1955, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %1957 = stablehlo.divide %1954, %1956 : tensor<1x32x2048x2048xf32>
    %1958 = stablehlo.reshape %1957 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %1959 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %1960 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %1961 = stablehlo.add %arg198, %1960 : tensor<2048xi64>
    %1962 = stablehlo.select %1959, %1961, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %1963 = stablehlo.reshape %1962 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %1964 = stablehlo.transpose %arg92, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1965 = stablehlo.dot %1901, %1964, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1966 = stablehlo.reshape %1965 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %1967 = "stablehlo.scatter"(%arg288, %1963, %1966) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %1968 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %1969 = "stablehlo.gather"(%1967, %1968) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %1970 = stablehlo.transpose %1969, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %1971 = stablehlo.dot_general %1958, %1970, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %1972 = stablehlo.reshape %1971 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %1973 = stablehlo.transpose %1972, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %1974 = stablehlo.reshape %1973 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %1975 = stablehlo.transpose %arg91, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %1976 = stablehlo.dot %1974, %1975, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %1977 = stablehlo.add %1889, %1976 : tensor<2048x4096xf32>
    %1978 = stablehlo.power %1977, %1 : tensor<2048x4096xf32>
    %1979 = stablehlo.reduce(%1978 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %1980 = stablehlo.multiply %1979, %0 : tensor<2048xf32>
    %1981 = stablehlo.reshape %1980 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %1982 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %1983 = stablehlo.add %1981, %1982 : tensor<2048x1xf32>
    %1984 = stablehlo.rsqrt %1983 : tensor<2048x1xf32>
    %1985 = stablehlo.reshape %1984 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %1986 = stablehlo.broadcast_in_dim %1985, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %1987 = stablehlo.multiply %1977, %1986 : tensor<2048x4096xf32>
    %1988 = stablehlo.broadcast_in_dim %arg90, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %1989 = stablehlo.multiply %1987, %1988 : tensor<2048x4096xf32>
    %1990 = stablehlo.transpose %arg292, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1991 = stablehlo.dot %1989, %1990, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1992 = stablehlo.logistic %1991 : tensor<2048x11008xf32>
    %1993 = stablehlo.multiply %1991, %1992 : tensor<2048x11008xf32>
    %1994 = stablehlo.transpose %arg89, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %1995 = stablehlo.dot %1989, %1994, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %1996 = stablehlo.multiply %1993, %1995 : tensor<2048x11008xf32>
    %1997 = stablehlo.transpose %arg88, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %1998 = stablehlo.dot %1996, %1997, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %1999 = stablehlo.add %1977, %1998 : tensor<2048x4096xf32>
    %2000 = stablehlo.power %1999, %1 : tensor<2048x4096xf32>
    %2001 = stablehlo.reduce(%2000 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2002 = stablehlo.multiply %2001, %0 : tensor<2048xf32>
    %2003 = stablehlo.reshape %2002 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2004 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2005 = stablehlo.add %2003, %2004 : tensor<2048x1xf32>
    %2006 = stablehlo.rsqrt %2005 : tensor<2048x1xf32>
    %2007 = stablehlo.reshape %2006 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2008 = stablehlo.broadcast_in_dim %2007, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2009 = stablehlo.multiply %1999, %2008 : tensor<2048x4096xf32>
    %2010 = stablehlo.broadcast_in_dim %arg87, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2011 = stablehlo.multiply %2009, %2010 : tensor<2048x4096xf32>
    %2012 = stablehlo.transpose %arg296, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2013 = stablehlo.dot %2011, %2012, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2014 = stablehlo.reshape %2013 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2015 = stablehlo.slice %2014 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2016 = stablehlo.reshape %2015 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2017 = stablehlo.slice %2014 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2018 = stablehlo.reshape %2017 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2019 = stablehlo.complex %2016, %2018 : tensor<2048x32x64xcomplex<f32>>
    %2020 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2021 = stablehlo.multiply %2019, %2020 : tensor<2048x32x64xcomplex<f32>>
    %2022 = stablehlo.real %2021 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2023 = stablehlo.reshape %2022 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2024 = stablehlo.imag %2021 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2025 = stablehlo.reshape %2024 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2026 = stablehlo.concatenate %2023, %2025, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2027 = stablehlo.reshape %2026 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2028 = stablehlo.transpose %2027, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2029 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2030 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2031 = stablehlo.add %arg198, %2030 : tensor<2048xi64>
    %2032 = stablehlo.select %2029, %2031, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2033 = stablehlo.reshape %2032 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2034 = stablehlo.transpose %arg294, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2035 = stablehlo.dot %2011, %2034, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2036 = stablehlo.reshape %2035 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2037 = stablehlo.slice %2036 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2038 = stablehlo.reshape %2037 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2039 = stablehlo.slice %2036 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2040 = stablehlo.reshape %2039 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2041 = stablehlo.complex %2038, %2040 : tensor<2048x32x64xcomplex<f32>>
    %2042 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2043 = stablehlo.multiply %2041, %2042 : tensor<2048x32x64xcomplex<f32>>
    %2044 = stablehlo.real %2043 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2045 = stablehlo.reshape %2044 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2046 = stablehlo.imag %2043 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2047 = stablehlo.reshape %2046 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2048 = stablehlo.concatenate %2045, %2047, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2049 = stablehlo.reshape %2048 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2050 = "stablehlo.scatter"(%arg295, %2033, %2049) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2051 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2052 = "stablehlo.gather"(%2050, %2051) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2053 = stablehlo.transpose %2052, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %2054 = stablehlo.dot_general %2028, %2053, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %2055 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %2056 = stablehlo.divide %2054, %2055 : tensor<32x2048x2048xf32>
    %2057 = stablehlo.reshape %2056 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2058 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %2059 = stablehlo.broadcast_in_dim %2058, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2060 = stablehlo.add %2057, %2059 : tensor<1x32x2048x2048xf32>
    %2061 = stablehlo.reduce(%2060 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2062 = stablehlo.broadcast_in_dim %2061, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2063 = stablehlo.subtract %2060, %2062 : tensor<1x32x2048x2048xf32>
    %2064 = stablehlo.exponential %2063 : tensor<1x32x2048x2048xf32>
    %2065 = stablehlo.reduce(%2064 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2066 = stablehlo.broadcast_in_dim %2065, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2067 = stablehlo.divide %2064, %2066 : tensor<1x32x2048x2048xf32>
    %2068 = stablehlo.reshape %2067 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %2069 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2070 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2071 = stablehlo.add %arg198, %2070 : tensor<2048xi64>
    %2072 = stablehlo.select %2069, %2071, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2073 = stablehlo.reshape %2072 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2074 = stablehlo.transpose %arg86, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2075 = stablehlo.dot %2011, %2074, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2076 = stablehlo.reshape %2075 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %2077 = "stablehlo.scatter"(%arg293, %2073, %2076) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2078 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2079 = "stablehlo.gather"(%2077, %2078) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2080 = stablehlo.transpose %2079, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2081 = stablehlo.dot_general %2068, %2080, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %2082 = stablehlo.reshape %2081 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %2083 = stablehlo.transpose %2082, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %2084 = stablehlo.reshape %2083 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %2085 = stablehlo.transpose %arg85, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2086 = stablehlo.dot %2084, %2085, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2087 = stablehlo.add %1999, %2086 : tensor<2048x4096xf32>
    %2088 = stablehlo.power %2087, %1 : tensor<2048x4096xf32>
    %2089 = stablehlo.reduce(%2088 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2090 = stablehlo.multiply %2089, %0 : tensor<2048xf32>
    %2091 = stablehlo.reshape %2090 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2092 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2093 = stablehlo.add %2091, %2092 : tensor<2048x1xf32>
    %2094 = stablehlo.rsqrt %2093 : tensor<2048x1xf32>
    %2095 = stablehlo.reshape %2094 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2096 = stablehlo.broadcast_in_dim %2095, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2097 = stablehlo.multiply %2087, %2096 : tensor<2048x4096xf32>
    %2098 = stablehlo.broadcast_in_dim %arg84, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2099 = stablehlo.multiply %2097, %2098 : tensor<2048x4096xf32>
    %2100 = stablehlo.transpose %arg297, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2101 = stablehlo.dot %2099, %2100, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2102 = stablehlo.logistic %2101 : tensor<2048x11008xf32>
    %2103 = stablehlo.multiply %2101, %2102 : tensor<2048x11008xf32>
    %2104 = stablehlo.transpose %arg83, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2105 = stablehlo.dot %2099, %2104, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2106 = stablehlo.multiply %2103, %2105 : tensor<2048x11008xf32>
    %2107 = stablehlo.transpose %arg82, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %2108 = stablehlo.dot %2106, %2107, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %2109 = stablehlo.add %2087, %2108 : tensor<2048x4096xf32>
    %2110 = stablehlo.power %2109, %1 : tensor<2048x4096xf32>
    %2111 = stablehlo.reduce(%2110 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2112 = stablehlo.multiply %2111, %0 : tensor<2048xf32>
    %2113 = stablehlo.reshape %2112 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2114 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2115 = stablehlo.add %2113, %2114 : tensor<2048x1xf32>
    %2116 = stablehlo.rsqrt %2115 : tensor<2048x1xf32>
    %2117 = stablehlo.reshape %2116 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2118 = stablehlo.broadcast_in_dim %2117, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2119 = stablehlo.multiply %2109, %2118 : tensor<2048x4096xf32>
    %2120 = stablehlo.broadcast_in_dim %arg81, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2121 = stablehlo.multiply %2119, %2120 : tensor<2048x4096xf32>
    %2122 = stablehlo.transpose %arg301, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2123 = stablehlo.dot %2121, %2122, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2124 = stablehlo.reshape %2123 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2125 = stablehlo.slice %2124 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2126 = stablehlo.reshape %2125 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2127 = stablehlo.slice %2124 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2128 = stablehlo.reshape %2127 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2129 = stablehlo.complex %2126, %2128 : tensor<2048x32x64xcomplex<f32>>
    %2130 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2131 = stablehlo.multiply %2129, %2130 : tensor<2048x32x64xcomplex<f32>>
    %2132 = stablehlo.real %2131 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2133 = stablehlo.reshape %2132 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2134 = stablehlo.imag %2131 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2135 = stablehlo.reshape %2134 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2136 = stablehlo.concatenate %2133, %2135, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2137 = stablehlo.reshape %2136 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2138 = stablehlo.transpose %2137, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2139 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2140 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2141 = stablehlo.add %arg198, %2140 : tensor<2048xi64>
    %2142 = stablehlo.select %2139, %2141, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2143 = stablehlo.reshape %2142 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2144 = stablehlo.transpose %arg299, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2145 = stablehlo.dot %2121, %2144, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2146 = stablehlo.reshape %2145 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2147 = stablehlo.slice %2146 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2148 = stablehlo.reshape %2147 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2149 = stablehlo.slice %2146 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2150 = stablehlo.reshape %2149 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2151 = stablehlo.complex %2148, %2150 : tensor<2048x32x64xcomplex<f32>>
    %2152 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2153 = stablehlo.multiply %2151, %2152 : tensor<2048x32x64xcomplex<f32>>
    %2154 = stablehlo.real %2153 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2155 = stablehlo.reshape %2154 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2156 = stablehlo.imag %2153 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2157 = stablehlo.reshape %2156 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2158 = stablehlo.concatenate %2155, %2157, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2159 = stablehlo.reshape %2158 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2160 = "stablehlo.scatter"(%arg300, %2143, %2159) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2161 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2162 = "stablehlo.gather"(%2160, %2161) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2163 = stablehlo.transpose %2162, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %2164 = stablehlo.dot_general %2138, %2163, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %2165 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %2166 = stablehlo.divide %2164, %2165 : tensor<32x2048x2048xf32>
    %2167 = stablehlo.reshape %2166 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2168 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %2169 = stablehlo.broadcast_in_dim %2168, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2170 = stablehlo.add %2167, %2169 : tensor<1x32x2048x2048xf32>
    %2171 = stablehlo.reduce(%2170 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2172 = stablehlo.broadcast_in_dim %2171, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2173 = stablehlo.subtract %2170, %2172 : tensor<1x32x2048x2048xf32>
    %2174 = stablehlo.exponential %2173 : tensor<1x32x2048x2048xf32>
    %2175 = stablehlo.reduce(%2174 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2176 = stablehlo.broadcast_in_dim %2175, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2177 = stablehlo.divide %2174, %2176 : tensor<1x32x2048x2048xf32>
    %2178 = stablehlo.reshape %2177 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %2179 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2180 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2181 = stablehlo.add %arg198, %2180 : tensor<2048xi64>
    %2182 = stablehlo.select %2179, %2181, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2183 = stablehlo.reshape %2182 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2184 = stablehlo.transpose %arg80, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2185 = stablehlo.dot %2121, %2184, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2186 = stablehlo.reshape %2185 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %2187 = "stablehlo.scatter"(%arg298, %2183, %2186) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2188 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2189 = "stablehlo.gather"(%2187, %2188) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2190 = stablehlo.transpose %2189, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2191 = stablehlo.dot_general %2178, %2190, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %2192 = stablehlo.reshape %2191 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %2193 = stablehlo.transpose %2192, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %2194 = stablehlo.reshape %2193 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %2195 = stablehlo.transpose %arg79, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2196 = stablehlo.dot %2194, %2195, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2197 = stablehlo.add %2109, %2196 : tensor<2048x4096xf32>
    %2198 = stablehlo.power %2197, %1 : tensor<2048x4096xf32>
    %2199 = stablehlo.reduce(%2198 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2200 = stablehlo.multiply %2199, %0 : tensor<2048xf32>
    %2201 = stablehlo.reshape %2200 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2202 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2203 = stablehlo.add %2201, %2202 : tensor<2048x1xf32>
    %2204 = stablehlo.rsqrt %2203 : tensor<2048x1xf32>
    %2205 = stablehlo.reshape %2204 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2206 = stablehlo.broadcast_in_dim %2205, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2207 = stablehlo.multiply %2197, %2206 : tensor<2048x4096xf32>
    %2208 = stablehlo.broadcast_in_dim %arg78, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2209 = stablehlo.multiply %2207, %2208 : tensor<2048x4096xf32>
    %2210 = stablehlo.transpose %arg302, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2211 = stablehlo.dot %2209, %2210, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2212 = stablehlo.logistic %2211 : tensor<2048x11008xf32>
    %2213 = stablehlo.multiply %2211, %2212 : tensor<2048x11008xf32>
    %2214 = stablehlo.transpose %arg77, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2215 = stablehlo.dot %2209, %2214, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2216 = stablehlo.multiply %2213, %2215 : tensor<2048x11008xf32>
    %2217 = stablehlo.transpose %arg76, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %2218 = stablehlo.dot %2216, %2217, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %2219 = stablehlo.add %2197, %2218 : tensor<2048x4096xf32>
    %2220 = stablehlo.power %2219, %1 : tensor<2048x4096xf32>
    %2221 = stablehlo.reduce(%2220 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2222 = stablehlo.multiply %2221, %0 : tensor<2048xf32>
    %2223 = stablehlo.reshape %2222 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2224 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2225 = stablehlo.add %2223, %2224 : tensor<2048x1xf32>
    %2226 = stablehlo.rsqrt %2225 : tensor<2048x1xf32>
    %2227 = stablehlo.reshape %2226 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2228 = stablehlo.broadcast_in_dim %2227, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2229 = stablehlo.multiply %2219, %2228 : tensor<2048x4096xf32>
    %2230 = stablehlo.broadcast_in_dim %arg75, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2231 = stablehlo.multiply %2229, %2230 : tensor<2048x4096xf32>
    %2232 = stablehlo.transpose %arg306, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2233 = stablehlo.dot %2231, %2232, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2234 = stablehlo.reshape %2233 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2235 = stablehlo.slice %2234 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2236 = stablehlo.reshape %2235 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2237 = stablehlo.slice %2234 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2238 = stablehlo.reshape %2237 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2239 = stablehlo.complex %2236, %2238 : tensor<2048x32x64xcomplex<f32>>
    %2240 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2241 = stablehlo.multiply %2239, %2240 : tensor<2048x32x64xcomplex<f32>>
    %2242 = stablehlo.real %2241 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2243 = stablehlo.reshape %2242 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2244 = stablehlo.imag %2241 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2245 = stablehlo.reshape %2244 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2246 = stablehlo.concatenate %2243, %2245, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2247 = stablehlo.reshape %2246 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2248 = stablehlo.transpose %2247, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2249 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2250 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2251 = stablehlo.add %arg198, %2250 : tensor<2048xi64>
    %2252 = stablehlo.select %2249, %2251, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2253 = stablehlo.reshape %2252 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2254 = stablehlo.transpose %arg304, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2255 = stablehlo.dot %2231, %2254, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2256 = stablehlo.reshape %2255 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2257 = stablehlo.slice %2256 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2258 = stablehlo.reshape %2257 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2259 = stablehlo.slice %2256 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2260 = stablehlo.reshape %2259 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2261 = stablehlo.complex %2258, %2260 : tensor<2048x32x64xcomplex<f32>>
    %2262 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2263 = stablehlo.multiply %2261, %2262 : tensor<2048x32x64xcomplex<f32>>
    %2264 = stablehlo.real %2263 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2265 = stablehlo.reshape %2264 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2266 = stablehlo.imag %2263 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2267 = stablehlo.reshape %2266 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2268 = stablehlo.concatenate %2265, %2267, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2269 = stablehlo.reshape %2268 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2270 = "stablehlo.scatter"(%arg305, %2253, %2269) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2271 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2272 = "stablehlo.gather"(%2270, %2271) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2273 = stablehlo.transpose %2272, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %2274 = stablehlo.dot_general %2248, %2273, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %2275 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %2276 = stablehlo.divide %2274, %2275 : tensor<32x2048x2048xf32>
    %2277 = stablehlo.reshape %2276 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2278 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %2279 = stablehlo.broadcast_in_dim %2278, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2280 = stablehlo.add %2277, %2279 : tensor<1x32x2048x2048xf32>
    %2281 = stablehlo.reduce(%2280 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2282 = stablehlo.broadcast_in_dim %2281, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2283 = stablehlo.subtract %2280, %2282 : tensor<1x32x2048x2048xf32>
    %2284 = stablehlo.exponential %2283 : tensor<1x32x2048x2048xf32>
    %2285 = stablehlo.reduce(%2284 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2286 = stablehlo.broadcast_in_dim %2285, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2287 = stablehlo.divide %2284, %2286 : tensor<1x32x2048x2048xf32>
    %2288 = stablehlo.reshape %2287 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %2289 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2290 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2291 = stablehlo.add %arg198, %2290 : tensor<2048xi64>
    %2292 = stablehlo.select %2289, %2291, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2293 = stablehlo.reshape %2292 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2294 = stablehlo.transpose %arg74, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2295 = stablehlo.dot %2231, %2294, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2296 = stablehlo.reshape %2295 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %2297 = "stablehlo.scatter"(%arg303, %2293, %2296) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2298 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2299 = "stablehlo.gather"(%2297, %2298) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2300 = stablehlo.transpose %2299, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2301 = stablehlo.dot_general %2288, %2300, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %2302 = stablehlo.reshape %2301 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %2303 = stablehlo.transpose %2302, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %2304 = stablehlo.reshape %2303 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %2305 = stablehlo.transpose %arg73, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2306 = stablehlo.dot %2304, %2305, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2307 = stablehlo.add %2219, %2306 : tensor<2048x4096xf32>
    %2308 = stablehlo.power %2307, %1 : tensor<2048x4096xf32>
    %2309 = stablehlo.reduce(%2308 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2310 = stablehlo.multiply %2309, %0 : tensor<2048xf32>
    %2311 = stablehlo.reshape %2310 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2312 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2313 = stablehlo.add %2311, %2312 : tensor<2048x1xf32>
    %2314 = stablehlo.rsqrt %2313 : tensor<2048x1xf32>
    %2315 = stablehlo.reshape %2314 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2316 = stablehlo.broadcast_in_dim %2315, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2317 = stablehlo.multiply %2307, %2316 : tensor<2048x4096xf32>
    %2318 = stablehlo.broadcast_in_dim %arg72, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2319 = stablehlo.multiply %2317, %2318 : tensor<2048x4096xf32>
    %2320 = stablehlo.transpose %arg307, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2321 = stablehlo.dot %2319, %2320, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2322 = stablehlo.logistic %2321 : tensor<2048x11008xf32>
    %2323 = stablehlo.multiply %2321, %2322 : tensor<2048x11008xf32>
    %2324 = stablehlo.transpose %arg71, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2325 = stablehlo.dot %2319, %2324, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2326 = stablehlo.multiply %2323, %2325 : tensor<2048x11008xf32>
    %2327 = stablehlo.transpose %arg70, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %2328 = stablehlo.dot %2326, %2327, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %2329 = stablehlo.add %2307, %2328 : tensor<2048x4096xf32>
    %2330 = stablehlo.power %2329, %1 : tensor<2048x4096xf32>
    %2331 = stablehlo.reduce(%2330 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2332 = stablehlo.multiply %2331, %0 : tensor<2048xf32>
    %2333 = stablehlo.reshape %2332 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2334 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2335 = stablehlo.add %2333, %2334 : tensor<2048x1xf32>
    %2336 = stablehlo.rsqrt %2335 : tensor<2048x1xf32>
    %2337 = stablehlo.reshape %2336 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2338 = stablehlo.broadcast_in_dim %2337, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2339 = stablehlo.multiply %2329, %2338 : tensor<2048x4096xf32>
    %2340 = stablehlo.broadcast_in_dim %arg69, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2341 = stablehlo.multiply %2339, %2340 : tensor<2048x4096xf32>
    %2342 = stablehlo.transpose %arg311, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2343 = stablehlo.dot %2341, %2342, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2344 = stablehlo.reshape %2343 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2345 = stablehlo.slice %2344 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2346 = stablehlo.reshape %2345 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2347 = stablehlo.slice %2344 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2348 = stablehlo.reshape %2347 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2349 = stablehlo.complex %2346, %2348 : tensor<2048x32x64xcomplex<f32>>
    %2350 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2351 = stablehlo.multiply %2349, %2350 : tensor<2048x32x64xcomplex<f32>>
    %2352 = stablehlo.real %2351 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2353 = stablehlo.reshape %2352 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2354 = stablehlo.imag %2351 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2355 = stablehlo.reshape %2354 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2356 = stablehlo.concatenate %2353, %2355, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2357 = stablehlo.reshape %2356 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2358 = stablehlo.transpose %2357, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2359 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2360 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2361 = stablehlo.add %arg198, %2360 : tensor<2048xi64>
    %2362 = stablehlo.select %2359, %2361, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2363 = stablehlo.reshape %2362 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2364 = stablehlo.transpose %arg309, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2365 = stablehlo.dot %2341, %2364, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2366 = stablehlo.reshape %2365 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2367 = stablehlo.slice %2366 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2368 = stablehlo.reshape %2367 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2369 = stablehlo.slice %2366 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2370 = stablehlo.reshape %2369 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2371 = stablehlo.complex %2368, %2370 : tensor<2048x32x64xcomplex<f32>>
    %2372 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2373 = stablehlo.multiply %2371, %2372 : tensor<2048x32x64xcomplex<f32>>
    %2374 = stablehlo.real %2373 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2375 = stablehlo.reshape %2374 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2376 = stablehlo.imag %2373 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2377 = stablehlo.reshape %2376 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2378 = stablehlo.concatenate %2375, %2377, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2379 = stablehlo.reshape %2378 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2380 = "stablehlo.scatter"(%arg310, %2363, %2379) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2381 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2382 = "stablehlo.gather"(%2380, %2381) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2383 = stablehlo.transpose %2382, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %2384 = stablehlo.dot_general %2358, %2383, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %2385 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %2386 = stablehlo.divide %2384, %2385 : tensor<32x2048x2048xf32>
    %2387 = stablehlo.reshape %2386 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2388 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %2389 = stablehlo.broadcast_in_dim %2388, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2390 = stablehlo.add %2387, %2389 : tensor<1x32x2048x2048xf32>
    %2391 = stablehlo.reduce(%2390 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2392 = stablehlo.broadcast_in_dim %2391, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2393 = stablehlo.subtract %2390, %2392 : tensor<1x32x2048x2048xf32>
    %2394 = stablehlo.exponential %2393 : tensor<1x32x2048x2048xf32>
    %2395 = stablehlo.reduce(%2394 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2396 = stablehlo.broadcast_in_dim %2395, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2397 = stablehlo.divide %2394, %2396 : tensor<1x32x2048x2048xf32>
    %2398 = stablehlo.reshape %2397 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %2399 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2400 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2401 = stablehlo.add %arg198, %2400 : tensor<2048xi64>
    %2402 = stablehlo.select %2399, %2401, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2403 = stablehlo.reshape %2402 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2404 = stablehlo.transpose %arg68, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2405 = stablehlo.dot %2341, %2404, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2406 = stablehlo.reshape %2405 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %2407 = "stablehlo.scatter"(%arg308, %2403, %2406) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2408 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2409 = "stablehlo.gather"(%2407, %2408) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2410 = stablehlo.transpose %2409, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2411 = stablehlo.dot_general %2398, %2410, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %2412 = stablehlo.reshape %2411 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %2413 = stablehlo.transpose %2412, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %2414 = stablehlo.reshape %2413 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %2415 = stablehlo.transpose %arg67, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2416 = stablehlo.dot %2414, %2415, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2417 = stablehlo.add %2329, %2416 : tensor<2048x4096xf32>
    %2418 = stablehlo.power %2417, %1 : tensor<2048x4096xf32>
    %2419 = stablehlo.reduce(%2418 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2420 = stablehlo.multiply %2419, %0 : tensor<2048xf32>
    %2421 = stablehlo.reshape %2420 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2422 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2423 = stablehlo.add %2421, %2422 : tensor<2048x1xf32>
    %2424 = stablehlo.rsqrt %2423 : tensor<2048x1xf32>
    %2425 = stablehlo.reshape %2424 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2426 = stablehlo.broadcast_in_dim %2425, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2427 = stablehlo.multiply %2417, %2426 : tensor<2048x4096xf32>
    %2428 = stablehlo.broadcast_in_dim %arg66, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2429 = stablehlo.multiply %2427, %2428 : tensor<2048x4096xf32>
    %2430 = stablehlo.transpose %arg312, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2431 = stablehlo.dot %2429, %2430, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2432 = stablehlo.logistic %2431 : tensor<2048x11008xf32>
    %2433 = stablehlo.multiply %2431, %2432 : tensor<2048x11008xf32>
    %2434 = stablehlo.transpose %arg65, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2435 = stablehlo.dot %2429, %2434, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2436 = stablehlo.multiply %2433, %2435 : tensor<2048x11008xf32>
    %2437 = stablehlo.transpose %arg64, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %2438 = stablehlo.dot %2436, %2437, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %2439 = stablehlo.add %2417, %2438 : tensor<2048x4096xf32>
    %2440 = stablehlo.power %2439, %1 : tensor<2048x4096xf32>
    %2441 = stablehlo.reduce(%2440 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2442 = stablehlo.multiply %2441, %0 : tensor<2048xf32>
    %2443 = stablehlo.reshape %2442 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2444 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2445 = stablehlo.add %2443, %2444 : tensor<2048x1xf32>
    %2446 = stablehlo.rsqrt %2445 : tensor<2048x1xf32>
    %2447 = stablehlo.reshape %2446 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2448 = stablehlo.broadcast_in_dim %2447, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2449 = stablehlo.multiply %2439, %2448 : tensor<2048x4096xf32>
    %2450 = stablehlo.broadcast_in_dim %arg63, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2451 = stablehlo.multiply %2449, %2450 : tensor<2048x4096xf32>
    %2452 = stablehlo.transpose %arg316, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2453 = stablehlo.dot %2451, %2452, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2454 = stablehlo.reshape %2453 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2455 = stablehlo.slice %2454 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2456 = stablehlo.reshape %2455 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2457 = stablehlo.slice %2454 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2458 = stablehlo.reshape %2457 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2459 = stablehlo.complex %2456, %2458 : tensor<2048x32x64xcomplex<f32>>
    %2460 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2461 = stablehlo.multiply %2459, %2460 : tensor<2048x32x64xcomplex<f32>>
    %2462 = stablehlo.real %2461 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2463 = stablehlo.reshape %2462 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2464 = stablehlo.imag %2461 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2465 = stablehlo.reshape %2464 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2466 = stablehlo.concatenate %2463, %2465, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2467 = stablehlo.reshape %2466 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2468 = stablehlo.transpose %2467, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2469 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2470 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2471 = stablehlo.add %arg198, %2470 : tensor<2048xi64>
    %2472 = stablehlo.select %2469, %2471, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2473 = stablehlo.reshape %2472 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2474 = stablehlo.transpose %arg314, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2475 = stablehlo.dot %2451, %2474, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2476 = stablehlo.reshape %2475 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2477 = stablehlo.slice %2476 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2478 = stablehlo.reshape %2477 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2479 = stablehlo.slice %2476 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2480 = stablehlo.reshape %2479 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2481 = stablehlo.complex %2478, %2480 : tensor<2048x32x64xcomplex<f32>>
    %2482 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2483 = stablehlo.multiply %2481, %2482 : tensor<2048x32x64xcomplex<f32>>
    %2484 = stablehlo.real %2483 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2485 = stablehlo.reshape %2484 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2486 = stablehlo.imag %2483 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2487 = stablehlo.reshape %2486 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2488 = stablehlo.concatenate %2485, %2487, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2489 = stablehlo.reshape %2488 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2490 = "stablehlo.scatter"(%arg315, %2473, %2489) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2491 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2492 = "stablehlo.gather"(%2490, %2491) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2493 = stablehlo.transpose %2492, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %2494 = stablehlo.dot_general %2468, %2493, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %2495 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %2496 = stablehlo.divide %2494, %2495 : tensor<32x2048x2048xf32>
    %2497 = stablehlo.reshape %2496 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2498 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %2499 = stablehlo.broadcast_in_dim %2498, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2500 = stablehlo.add %2497, %2499 : tensor<1x32x2048x2048xf32>
    %2501 = stablehlo.reduce(%2500 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2502 = stablehlo.broadcast_in_dim %2501, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2503 = stablehlo.subtract %2500, %2502 : tensor<1x32x2048x2048xf32>
    %2504 = stablehlo.exponential %2503 : tensor<1x32x2048x2048xf32>
    %2505 = stablehlo.reduce(%2504 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2506 = stablehlo.broadcast_in_dim %2505, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2507 = stablehlo.divide %2504, %2506 : tensor<1x32x2048x2048xf32>
    %2508 = stablehlo.reshape %2507 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %2509 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2510 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2511 = stablehlo.add %arg198, %2510 : tensor<2048xi64>
    %2512 = stablehlo.select %2509, %2511, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2513 = stablehlo.reshape %2512 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2514 = stablehlo.transpose %arg62, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2515 = stablehlo.dot %2451, %2514, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2516 = stablehlo.reshape %2515 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %2517 = "stablehlo.scatter"(%arg313, %2513, %2516) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2518 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2519 = "stablehlo.gather"(%2517, %2518) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2520 = stablehlo.transpose %2519, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2521 = stablehlo.dot_general %2508, %2520, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %2522 = stablehlo.reshape %2521 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %2523 = stablehlo.transpose %2522, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %2524 = stablehlo.reshape %2523 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %2525 = stablehlo.transpose %arg61, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2526 = stablehlo.dot %2524, %2525, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2527 = stablehlo.add %2439, %2526 : tensor<2048x4096xf32>
    %2528 = stablehlo.power %2527, %1 : tensor<2048x4096xf32>
    %2529 = stablehlo.reduce(%2528 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2530 = stablehlo.multiply %2529, %0 : tensor<2048xf32>
    %2531 = stablehlo.reshape %2530 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2532 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2533 = stablehlo.add %2531, %2532 : tensor<2048x1xf32>
    %2534 = stablehlo.rsqrt %2533 : tensor<2048x1xf32>
    %2535 = stablehlo.reshape %2534 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2536 = stablehlo.broadcast_in_dim %2535, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2537 = stablehlo.multiply %2527, %2536 : tensor<2048x4096xf32>
    %2538 = stablehlo.broadcast_in_dim %arg60, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2539 = stablehlo.multiply %2537, %2538 : tensor<2048x4096xf32>
    %2540 = stablehlo.transpose %arg317, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2541 = stablehlo.dot %2539, %2540, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2542 = stablehlo.logistic %2541 : tensor<2048x11008xf32>
    %2543 = stablehlo.multiply %2541, %2542 : tensor<2048x11008xf32>
    %2544 = stablehlo.transpose %arg59, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2545 = stablehlo.dot %2539, %2544, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2546 = stablehlo.multiply %2543, %2545 : tensor<2048x11008xf32>
    %2547 = stablehlo.transpose %arg58, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %2548 = stablehlo.dot %2546, %2547, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %2549 = stablehlo.add %2527, %2548 : tensor<2048x4096xf32>
    %2550 = stablehlo.power %2549, %1 : tensor<2048x4096xf32>
    %2551 = stablehlo.reduce(%2550 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2552 = stablehlo.multiply %2551, %0 : tensor<2048xf32>
    %2553 = stablehlo.reshape %2552 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2554 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2555 = stablehlo.add %2553, %2554 : tensor<2048x1xf32>
    %2556 = stablehlo.rsqrt %2555 : tensor<2048x1xf32>
    %2557 = stablehlo.reshape %2556 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2558 = stablehlo.broadcast_in_dim %2557, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2559 = stablehlo.multiply %2549, %2558 : tensor<2048x4096xf32>
    %2560 = stablehlo.broadcast_in_dim %arg57, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2561 = stablehlo.multiply %2559, %2560 : tensor<2048x4096xf32>
    %2562 = stablehlo.transpose %arg321, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2563 = stablehlo.dot %2561, %2562, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2564 = stablehlo.reshape %2563 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2565 = stablehlo.slice %2564 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2566 = stablehlo.reshape %2565 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2567 = stablehlo.slice %2564 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2568 = stablehlo.reshape %2567 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2569 = stablehlo.complex %2566, %2568 : tensor<2048x32x64xcomplex<f32>>
    %2570 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2571 = stablehlo.multiply %2569, %2570 : tensor<2048x32x64xcomplex<f32>>
    %2572 = stablehlo.real %2571 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2573 = stablehlo.reshape %2572 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2574 = stablehlo.imag %2571 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2575 = stablehlo.reshape %2574 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2576 = stablehlo.concatenate %2573, %2575, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2577 = stablehlo.reshape %2576 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2578 = stablehlo.transpose %2577, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2579 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2580 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2581 = stablehlo.add %arg198, %2580 : tensor<2048xi64>
    %2582 = stablehlo.select %2579, %2581, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2583 = stablehlo.reshape %2582 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2584 = stablehlo.transpose %arg319, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2585 = stablehlo.dot %2561, %2584, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2586 = stablehlo.reshape %2585 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2587 = stablehlo.slice %2586 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2588 = stablehlo.reshape %2587 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2589 = stablehlo.slice %2586 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2590 = stablehlo.reshape %2589 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2591 = stablehlo.complex %2588, %2590 : tensor<2048x32x64xcomplex<f32>>
    %2592 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2593 = stablehlo.multiply %2591, %2592 : tensor<2048x32x64xcomplex<f32>>
    %2594 = stablehlo.real %2593 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2595 = stablehlo.reshape %2594 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2596 = stablehlo.imag %2593 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2597 = stablehlo.reshape %2596 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2598 = stablehlo.concatenate %2595, %2597, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2599 = stablehlo.reshape %2598 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2600 = "stablehlo.scatter"(%arg320, %2583, %2599) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2601 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2602 = "stablehlo.gather"(%2600, %2601) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2603 = stablehlo.transpose %2602, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %2604 = stablehlo.dot_general %2578, %2603, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %2605 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %2606 = stablehlo.divide %2604, %2605 : tensor<32x2048x2048xf32>
    %2607 = stablehlo.reshape %2606 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2608 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %2609 = stablehlo.broadcast_in_dim %2608, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2610 = stablehlo.add %2607, %2609 : tensor<1x32x2048x2048xf32>
    %2611 = stablehlo.reduce(%2610 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2612 = stablehlo.broadcast_in_dim %2611, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2613 = stablehlo.subtract %2610, %2612 : tensor<1x32x2048x2048xf32>
    %2614 = stablehlo.exponential %2613 : tensor<1x32x2048x2048xf32>
    %2615 = stablehlo.reduce(%2614 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2616 = stablehlo.broadcast_in_dim %2615, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2617 = stablehlo.divide %2614, %2616 : tensor<1x32x2048x2048xf32>
    %2618 = stablehlo.reshape %2617 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %2619 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2620 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2621 = stablehlo.add %arg198, %2620 : tensor<2048xi64>
    %2622 = stablehlo.select %2619, %2621, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2623 = stablehlo.reshape %2622 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2624 = stablehlo.transpose %arg56, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2625 = stablehlo.dot %2561, %2624, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2626 = stablehlo.reshape %2625 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %2627 = "stablehlo.scatter"(%arg318, %2623, %2626) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2628 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2629 = "stablehlo.gather"(%2627, %2628) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2630 = stablehlo.transpose %2629, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2631 = stablehlo.dot_general %2618, %2630, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %2632 = stablehlo.reshape %2631 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %2633 = stablehlo.transpose %2632, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %2634 = stablehlo.reshape %2633 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %2635 = stablehlo.transpose %arg55, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2636 = stablehlo.dot %2634, %2635, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2637 = stablehlo.add %2549, %2636 : tensor<2048x4096xf32>
    %2638 = stablehlo.power %2637, %1 : tensor<2048x4096xf32>
    %2639 = stablehlo.reduce(%2638 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2640 = stablehlo.multiply %2639, %0 : tensor<2048xf32>
    %2641 = stablehlo.reshape %2640 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2642 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2643 = stablehlo.add %2641, %2642 : tensor<2048x1xf32>
    %2644 = stablehlo.rsqrt %2643 : tensor<2048x1xf32>
    %2645 = stablehlo.reshape %2644 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2646 = stablehlo.broadcast_in_dim %2645, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2647 = stablehlo.multiply %2637, %2646 : tensor<2048x4096xf32>
    %2648 = stablehlo.broadcast_in_dim %arg54, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2649 = stablehlo.multiply %2647, %2648 : tensor<2048x4096xf32>
    %2650 = stablehlo.transpose %arg322, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2651 = stablehlo.dot %2649, %2650, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2652 = stablehlo.logistic %2651 : tensor<2048x11008xf32>
    %2653 = stablehlo.multiply %2651, %2652 : tensor<2048x11008xf32>
    %2654 = stablehlo.transpose %arg53, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2655 = stablehlo.dot %2649, %2654, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2656 = stablehlo.multiply %2653, %2655 : tensor<2048x11008xf32>
    %2657 = stablehlo.transpose %arg52, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %2658 = stablehlo.dot %2656, %2657, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %2659 = stablehlo.add %2637, %2658 : tensor<2048x4096xf32>
    %2660 = stablehlo.power %2659, %1 : tensor<2048x4096xf32>
    %2661 = stablehlo.reduce(%2660 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2662 = stablehlo.multiply %2661, %0 : tensor<2048xf32>
    %2663 = stablehlo.reshape %2662 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2664 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2665 = stablehlo.add %2663, %2664 : tensor<2048x1xf32>
    %2666 = stablehlo.rsqrt %2665 : tensor<2048x1xf32>
    %2667 = stablehlo.reshape %2666 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2668 = stablehlo.broadcast_in_dim %2667, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2669 = stablehlo.multiply %2659, %2668 : tensor<2048x4096xf32>
    %2670 = stablehlo.broadcast_in_dim %arg51, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2671 = stablehlo.multiply %2669, %2670 : tensor<2048x4096xf32>
    %2672 = stablehlo.transpose %arg326, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2673 = stablehlo.dot %2671, %2672, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2674 = stablehlo.reshape %2673 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2675 = stablehlo.slice %2674 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2676 = stablehlo.reshape %2675 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2677 = stablehlo.slice %2674 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2678 = stablehlo.reshape %2677 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2679 = stablehlo.complex %2676, %2678 : tensor<2048x32x64xcomplex<f32>>
    %2680 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2681 = stablehlo.multiply %2679, %2680 : tensor<2048x32x64xcomplex<f32>>
    %2682 = stablehlo.real %2681 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2683 = stablehlo.reshape %2682 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2684 = stablehlo.imag %2681 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2685 = stablehlo.reshape %2684 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2686 = stablehlo.concatenate %2683, %2685, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2687 = stablehlo.reshape %2686 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2688 = stablehlo.transpose %2687, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2689 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2690 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2691 = stablehlo.add %arg198, %2690 : tensor<2048xi64>
    %2692 = stablehlo.select %2689, %2691, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2693 = stablehlo.reshape %2692 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2694 = stablehlo.transpose %arg324, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2695 = stablehlo.dot %2671, %2694, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2696 = stablehlo.reshape %2695 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2697 = stablehlo.slice %2696 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2698 = stablehlo.reshape %2697 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2699 = stablehlo.slice %2696 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2700 = stablehlo.reshape %2699 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2701 = stablehlo.complex %2698, %2700 : tensor<2048x32x64xcomplex<f32>>
    %2702 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2703 = stablehlo.multiply %2701, %2702 : tensor<2048x32x64xcomplex<f32>>
    %2704 = stablehlo.real %2703 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2705 = stablehlo.reshape %2704 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2706 = stablehlo.imag %2703 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2707 = stablehlo.reshape %2706 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2708 = stablehlo.concatenate %2705, %2707, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2709 = stablehlo.reshape %2708 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2710 = "stablehlo.scatter"(%arg325, %2693, %2709) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2711 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2712 = "stablehlo.gather"(%2710, %2711) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2713 = stablehlo.transpose %2712, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %2714 = stablehlo.dot_general %2688, %2713, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %2715 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %2716 = stablehlo.divide %2714, %2715 : tensor<32x2048x2048xf32>
    %2717 = stablehlo.reshape %2716 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2718 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %2719 = stablehlo.broadcast_in_dim %2718, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2720 = stablehlo.add %2717, %2719 : tensor<1x32x2048x2048xf32>
    %2721 = stablehlo.reduce(%2720 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2722 = stablehlo.broadcast_in_dim %2721, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2723 = stablehlo.subtract %2720, %2722 : tensor<1x32x2048x2048xf32>
    %2724 = stablehlo.exponential %2723 : tensor<1x32x2048x2048xf32>
    %2725 = stablehlo.reduce(%2724 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2726 = stablehlo.broadcast_in_dim %2725, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2727 = stablehlo.divide %2724, %2726 : tensor<1x32x2048x2048xf32>
    %2728 = stablehlo.reshape %2727 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %2729 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2730 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2731 = stablehlo.add %arg198, %2730 : tensor<2048xi64>
    %2732 = stablehlo.select %2729, %2731, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2733 = stablehlo.reshape %2732 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2734 = stablehlo.transpose %arg50, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2735 = stablehlo.dot %2671, %2734, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2736 = stablehlo.reshape %2735 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %2737 = "stablehlo.scatter"(%arg323, %2733, %2736) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2738 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2739 = "stablehlo.gather"(%2737, %2738) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2740 = stablehlo.transpose %2739, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2741 = stablehlo.dot_general %2728, %2740, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %2742 = stablehlo.reshape %2741 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %2743 = stablehlo.transpose %2742, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %2744 = stablehlo.reshape %2743 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %2745 = stablehlo.transpose %arg49, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2746 = stablehlo.dot %2744, %2745, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2747 = stablehlo.add %2659, %2746 : tensor<2048x4096xf32>
    %2748 = stablehlo.power %2747, %1 : tensor<2048x4096xf32>
    %2749 = stablehlo.reduce(%2748 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2750 = stablehlo.multiply %2749, %0 : tensor<2048xf32>
    %2751 = stablehlo.reshape %2750 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2752 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2753 = stablehlo.add %2751, %2752 : tensor<2048x1xf32>
    %2754 = stablehlo.rsqrt %2753 : tensor<2048x1xf32>
    %2755 = stablehlo.reshape %2754 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2756 = stablehlo.broadcast_in_dim %2755, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2757 = stablehlo.multiply %2747, %2756 : tensor<2048x4096xf32>
    %2758 = stablehlo.broadcast_in_dim %arg48, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2759 = stablehlo.multiply %2757, %2758 : tensor<2048x4096xf32>
    %2760 = stablehlo.transpose %arg327, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2761 = stablehlo.dot %2759, %2760, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2762 = stablehlo.logistic %2761 : tensor<2048x11008xf32>
    %2763 = stablehlo.multiply %2761, %2762 : tensor<2048x11008xf32>
    %2764 = stablehlo.transpose %arg47, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2765 = stablehlo.dot %2759, %2764, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2766 = stablehlo.multiply %2763, %2765 : tensor<2048x11008xf32>
    %2767 = stablehlo.transpose %arg46, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %2768 = stablehlo.dot %2766, %2767, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %2769 = stablehlo.add %2747, %2768 : tensor<2048x4096xf32>
    %2770 = stablehlo.power %2769, %1 : tensor<2048x4096xf32>
    %2771 = stablehlo.reduce(%2770 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2772 = stablehlo.multiply %2771, %0 : tensor<2048xf32>
    %2773 = stablehlo.reshape %2772 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2774 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2775 = stablehlo.add %2773, %2774 : tensor<2048x1xf32>
    %2776 = stablehlo.rsqrt %2775 : tensor<2048x1xf32>
    %2777 = stablehlo.reshape %2776 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2778 = stablehlo.broadcast_in_dim %2777, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2779 = stablehlo.multiply %2769, %2778 : tensor<2048x4096xf32>
    %2780 = stablehlo.broadcast_in_dim %arg45, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2781 = stablehlo.multiply %2779, %2780 : tensor<2048x4096xf32>
    %2782 = stablehlo.transpose %arg331, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2783 = stablehlo.dot %2781, %2782, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2784 = stablehlo.reshape %2783 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2785 = stablehlo.slice %2784 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2786 = stablehlo.reshape %2785 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2787 = stablehlo.slice %2784 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2788 = stablehlo.reshape %2787 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2789 = stablehlo.complex %2786, %2788 : tensor<2048x32x64xcomplex<f32>>
    %2790 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2791 = stablehlo.multiply %2789, %2790 : tensor<2048x32x64xcomplex<f32>>
    %2792 = stablehlo.real %2791 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2793 = stablehlo.reshape %2792 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2794 = stablehlo.imag %2791 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2795 = stablehlo.reshape %2794 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2796 = stablehlo.concatenate %2793, %2795, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2797 = stablehlo.reshape %2796 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2798 = stablehlo.transpose %2797, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2799 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2800 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2801 = stablehlo.add %arg198, %2800 : tensor<2048xi64>
    %2802 = stablehlo.select %2799, %2801, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2803 = stablehlo.reshape %2802 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2804 = stablehlo.transpose %arg329, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2805 = stablehlo.dot %2781, %2804, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2806 = stablehlo.reshape %2805 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2807 = stablehlo.slice %2806 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2808 = stablehlo.reshape %2807 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2809 = stablehlo.slice %2806 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2810 = stablehlo.reshape %2809 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2811 = stablehlo.complex %2808, %2810 : tensor<2048x32x64xcomplex<f32>>
    %2812 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2813 = stablehlo.multiply %2811, %2812 : tensor<2048x32x64xcomplex<f32>>
    %2814 = stablehlo.real %2813 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2815 = stablehlo.reshape %2814 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2816 = stablehlo.imag %2813 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2817 = stablehlo.reshape %2816 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2818 = stablehlo.concatenate %2815, %2817, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2819 = stablehlo.reshape %2818 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2820 = "stablehlo.scatter"(%arg330, %2803, %2819) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2821 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2822 = "stablehlo.gather"(%2820, %2821) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2823 = stablehlo.transpose %2822, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %2824 = stablehlo.dot_general %2798, %2823, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %2825 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %2826 = stablehlo.divide %2824, %2825 : tensor<32x2048x2048xf32>
    %2827 = stablehlo.reshape %2826 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2828 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %2829 = stablehlo.broadcast_in_dim %2828, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2830 = stablehlo.add %2827, %2829 : tensor<1x32x2048x2048xf32>
    %2831 = stablehlo.reduce(%2830 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2832 = stablehlo.broadcast_in_dim %2831, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2833 = stablehlo.subtract %2830, %2832 : tensor<1x32x2048x2048xf32>
    %2834 = stablehlo.exponential %2833 : tensor<1x32x2048x2048xf32>
    %2835 = stablehlo.reduce(%2834 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2836 = stablehlo.broadcast_in_dim %2835, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2837 = stablehlo.divide %2834, %2836 : tensor<1x32x2048x2048xf32>
    %2838 = stablehlo.reshape %2837 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %2839 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2840 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2841 = stablehlo.add %arg198, %2840 : tensor<2048xi64>
    %2842 = stablehlo.select %2839, %2841, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2843 = stablehlo.reshape %2842 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2844 = stablehlo.transpose %arg44, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2845 = stablehlo.dot %2781, %2844, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2846 = stablehlo.reshape %2845 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %2847 = "stablehlo.scatter"(%arg328, %2843, %2846) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2848 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2849 = "stablehlo.gather"(%2847, %2848) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2850 = stablehlo.transpose %2849, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2851 = stablehlo.dot_general %2838, %2850, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %2852 = stablehlo.reshape %2851 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %2853 = stablehlo.transpose %2852, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %2854 = stablehlo.reshape %2853 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %2855 = stablehlo.transpose %arg43, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2856 = stablehlo.dot %2854, %2855, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2857 = stablehlo.add %2769, %2856 : tensor<2048x4096xf32>
    %2858 = stablehlo.power %2857, %1 : tensor<2048x4096xf32>
    %2859 = stablehlo.reduce(%2858 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2860 = stablehlo.multiply %2859, %0 : tensor<2048xf32>
    %2861 = stablehlo.reshape %2860 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2862 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2863 = stablehlo.add %2861, %2862 : tensor<2048x1xf32>
    %2864 = stablehlo.rsqrt %2863 : tensor<2048x1xf32>
    %2865 = stablehlo.reshape %2864 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2866 = stablehlo.broadcast_in_dim %2865, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2867 = stablehlo.multiply %2857, %2866 : tensor<2048x4096xf32>
    %2868 = stablehlo.broadcast_in_dim %arg42, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2869 = stablehlo.multiply %2867, %2868 : tensor<2048x4096xf32>
    %2870 = stablehlo.transpose %arg332, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2871 = stablehlo.dot %2869, %2870, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2872 = stablehlo.logistic %2871 : tensor<2048x11008xf32>
    %2873 = stablehlo.multiply %2871, %2872 : tensor<2048x11008xf32>
    %2874 = stablehlo.transpose %arg41, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2875 = stablehlo.dot %2869, %2874, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2876 = stablehlo.multiply %2873, %2875 : tensor<2048x11008xf32>
    %2877 = stablehlo.transpose %arg40, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %2878 = stablehlo.dot %2876, %2877, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %2879 = stablehlo.add %2857, %2878 : tensor<2048x4096xf32>
    %2880 = stablehlo.power %2879, %1 : tensor<2048x4096xf32>
    %2881 = stablehlo.reduce(%2880 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2882 = stablehlo.multiply %2881, %0 : tensor<2048xf32>
    %2883 = stablehlo.reshape %2882 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2884 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2885 = stablehlo.add %2883, %2884 : tensor<2048x1xf32>
    %2886 = stablehlo.rsqrt %2885 : tensor<2048x1xf32>
    %2887 = stablehlo.reshape %2886 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2888 = stablehlo.broadcast_in_dim %2887, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2889 = stablehlo.multiply %2879, %2888 : tensor<2048x4096xf32>
    %2890 = stablehlo.broadcast_in_dim %arg39, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2891 = stablehlo.multiply %2889, %2890 : tensor<2048x4096xf32>
    %2892 = stablehlo.transpose %arg336, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2893 = stablehlo.dot %2891, %2892, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2894 = stablehlo.reshape %2893 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2895 = stablehlo.slice %2894 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2896 = stablehlo.reshape %2895 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2897 = stablehlo.slice %2894 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2898 = stablehlo.reshape %2897 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2899 = stablehlo.complex %2896, %2898 : tensor<2048x32x64xcomplex<f32>>
    %2900 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2901 = stablehlo.multiply %2899, %2900 : tensor<2048x32x64xcomplex<f32>>
    %2902 = stablehlo.real %2901 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2903 = stablehlo.reshape %2902 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2904 = stablehlo.imag %2901 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2905 = stablehlo.reshape %2904 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2906 = stablehlo.concatenate %2903, %2905, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2907 = stablehlo.reshape %2906 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2908 = stablehlo.transpose %2907, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2909 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2910 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2911 = stablehlo.add %arg198, %2910 : tensor<2048xi64>
    %2912 = stablehlo.select %2909, %2911, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2913 = stablehlo.reshape %2912 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2914 = stablehlo.transpose %arg334, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2915 = stablehlo.dot %2891, %2914, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2916 = stablehlo.reshape %2915 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %2917 = stablehlo.slice %2916 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2918 = stablehlo.reshape %2917 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2919 = stablehlo.slice %2916 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %2920 = stablehlo.reshape %2919 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %2921 = stablehlo.complex %2918, %2920 : tensor<2048x32x64xcomplex<f32>>
    %2922 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %2923 = stablehlo.multiply %2921, %2922 : tensor<2048x32x64xcomplex<f32>>
    %2924 = stablehlo.real %2923 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2925 = stablehlo.reshape %2924 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2926 = stablehlo.imag %2923 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %2927 = stablehlo.reshape %2926 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %2928 = stablehlo.concatenate %2925, %2927, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %2929 = stablehlo.reshape %2928 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %2930 = "stablehlo.scatter"(%arg335, %2913, %2929) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2931 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2932 = "stablehlo.gather"(%2930, %2931) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2933 = stablehlo.transpose %2932, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %2934 = stablehlo.dot_general %2908, %2933, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %2935 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %2936 = stablehlo.divide %2934, %2935 : tensor<32x2048x2048xf32>
    %2937 = stablehlo.reshape %2936 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2938 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %2939 = stablehlo.broadcast_in_dim %2938, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2940 = stablehlo.add %2937, %2939 : tensor<1x32x2048x2048xf32>
    %2941 = stablehlo.reduce(%2940 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2942 = stablehlo.broadcast_in_dim %2941, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2943 = stablehlo.subtract %2940, %2942 : tensor<1x32x2048x2048xf32>
    %2944 = stablehlo.exponential %2943 : tensor<1x32x2048x2048xf32>
    %2945 = stablehlo.reduce(%2944 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2946 = stablehlo.broadcast_in_dim %2945, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %2947 = stablehlo.divide %2944, %2946 : tensor<1x32x2048x2048xf32>
    %2948 = stablehlo.reshape %2947 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %2949 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %2950 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %2951 = stablehlo.add %arg198, %2950 : tensor<2048xi64>
    %2952 = stablehlo.select %2949, %2951, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %2953 = stablehlo.reshape %2952 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %2954 = stablehlo.transpose %arg38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2955 = stablehlo.dot %2891, %2954, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2956 = stablehlo.reshape %2955 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %2957 = "stablehlo.scatter"(%arg333, %2953, %2956) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %2958 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %2959 = "stablehlo.gather"(%2957, %2958) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %2960 = stablehlo.transpose %2959, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %2961 = stablehlo.dot_general %2948, %2960, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %2962 = stablehlo.reshape %2961 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %2963 = stablehlo.transpose %2962, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %2964 = stablehlo.reshape %2963 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %2965 = stablehlo.transpose %arg37, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %2966 = stablehlo.dot %2964, %2965, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %2967 = stablehlo.add %2879, %2966 : tensor<2048x4096xf32>
    %2968 = stablehlo.power %2967, %1 : tensor<2048x4096xf32>
    %2969 = stablehlo.reduce(%2968 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2970 = stablehlo.multiply %2969, %0 : tensor<2048xf32>
    %2971 = stablehlo.reshape %2970 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2972 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2973 = stablehlo.add %2971, %2972 : tensor<2048x1xf32>
    %2974 = stablehlo.rsqrt %2973 : tensor<2048x1xf32>
    %2975 = stablehlo.reshape %2974 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2976 = stablehlo.broadcast_in_dim %2975, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2977 = stablehlo.multiply %2967, %2976 : tensor<2048x4096xf32>
    %2978 = stablehlo.broadcast_in_dim %arg36, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %2979 = stablehlo.multiply %2977, %2978 : tensor<2048x4096xf32>
    %2980 = stablehlo.transpose %arg337, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2981 = stablehlo.dot %2979, %2980, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2982 = stablehlo.logistic %2981 : tensor<2048x11008xf32>
    %2983 = stablehlo.multiply %2981, %2982 : tensor<2048x11008xf32>
    %2984 = stablehlo.transpose %arg35, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %2985 = stablehlo.dot %2979, %2984, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %2986 = stablehlo.multiply %2983, %2985 : tensor<2048x11008xf32>
    %2987 = stablehlo.transpose %arg34, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %2988 = stablehlo.dot %2986, %2987, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %2989 = stablehlo.add %2967, %2988 : tensor<2048x4096xf32>
    %2990 = stablehlo.power %2989, %1 : tensor<2048x4096xf32>
    %2991 = stablehlo.reduce(%2990 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %2992 = stablehlo.multiply %2991, %0 : tensor<2048xf32>
    %2993 = stablehlo.reshape %2992 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %2994 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %2995 = stablehlo.add %2993, %2994 : tensor<2048x1xf32>
    %2996 = stablehlo.rsqrt %2995 : tensor<2048x1xf32>
    %2997 = stablehlo.reshape %2996 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %2998 = stablehlo.broadcast_in_dim %2997, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %2999 = stablehlo.multiply %2989, %2998 : tensor<2048x4096xf32>
    %3000 = stablehlo.broadcast_in_dim %arg33, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %3001 = stablehlo.multiply %2999, %3000 : tensor<2048x4096xf32>
    %3002 = stablehlo.transpose %arg341, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3003 = stablehlo.dot %3001, %3002, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3004 = stablehlo.reshape %3003 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %3005 = stablehlo.slice %3004 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3006 = stablehlo.reshape %3005 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3007 = stablehlo.slice %3004 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3008 = stablehlo.reshape %3007 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3009 = stablehlo.complex %3006, %3008 : tensor<2048x32x64xcomplex<f32>>
    %3010 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %3011 = stablehlo.multiply %3009, %3010 : tensor<2048x32x64xcomplex<f32>>
    %3012 = stablehlo.real %3011 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3013 = stablehlo.reshape %3012 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3014 = stablehlo.imag %3011 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3015 = stablehlo.reshape %3014 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3016 = stablehlo.concatenate %3013, %3015, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %3017 = stablehlo.reshape %3016 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %3018 = stablehlo.transpose %3017, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %3019 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %3020 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %3021 = stablehlo.add %arg198, %3020 : tensor<2048xi64>
    %3022 = stablehlo.select %3019, %3021, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %3023 = stablehlo.reshape %3022 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %3024 = stablehlo.transpose %arg339, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3025 = stablehlo.dot %3001, %3024, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3026 = stablehlo.reshape %3025 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %3027 = stablehlo.slice %3026 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3028 = stablehlo.reshape %3027 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3029 = stablehlo.slice %3026 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3030 = stablehlo.reshape %3029 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3031 = stablehlo.complex %3028, %3030 : tensor<2048x32x64xcomplex<f32>>
    %3032 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %3033 = stablehlo.multiply %3031, %3032 : tensor<2048x32x64xcomplex<f32>>
    %3034 = stablehlo.real %3033 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3035 = stablehlo.reshape %3034 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3036 = stablehlo.imag %3033 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3037 = stablehlo.reshape %3036 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3038 = stablehlo.concatenate %3035, %3037, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %3039 = stablehlo.reshape %3038 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %3040 = "stablehlo.scatter"(%arg340, %3023, %3039) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %3041 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %3042 = "stablehlo.gather"(%3040, %3041) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %3043 = stablehlo.transpose %3042, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %3044 = stablehlo.dot_general %3018, %3043, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %3045 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %3046 = stablehlo.divide %3044, %3045 : tensor<32x2048x2048xf32>
    %3047 = stablehlo.reshape %3046 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3048 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %3049 = stablehlo.broadcast_in_dim %3048, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3050 = stablehlo.add %3047, %3049 : tensor<1x32x2048x2048xf32>
    %3051 = stablehlo.reduce(%3050 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3052 = stablehlo.broadcast_in_dim %3051, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3053 = stablehlo.subtract %3050, %3052 : tensor<1x32x2048x2048xf32>
    %3054 = stablehlo.exponential %3053 : tensor<1x32x2048x2048xf32>
    %3055 = stablehlo.reduce(%3054 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3056 = stablehlo.broadcast_in_dim %3055, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3057 = stablehlo.divide %3054, %3056 : tensor<1x32x2048x2048xf32>
    %3058 = stablehlo.reshape %3057 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %3059 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %3060 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %3061 = stablehlo.add %arg198, %3060 : tensor<2048xi64>
    %3062 = stablehlo.select %3059, %3061, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %3063 = stablehlo.reshape %3062 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %3064 = stablehlo.transpose %arg32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3065 = stablehlo.dot %3001, %3064, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3066 = stablehlo.reshape %3065 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %3067 = "stablehlo.scatter"(%arg338, %3063, %3066) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %3068 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %3069 = "stablehlo.gather"(%3067, %3068) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %3070 = stablehlo.transpose %3069, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %3071 = stablehlo.dot_general %3058, %3070, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %3072 = stablehlo.reshape %3071 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %3073 = stablehlo.transpose %3072, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %3074 = stablehlo.reshape %3073 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %3075 = stablehlo.transpose %arg31, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3076 = stablehlo.dot %3074, %3075, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3077 = stablehlo.add %2989, %3076 : tensor<2048x4096xf32>
    %3078 = stablehlo.power %3077, %1 : tensor<2048x4096xf32>
    %3079 = stablehlo.reduce(%3078 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3080 = stablehlo.multiply %3079, %0 : tensor<2048xf32>
    %3081 = stablehlo.reshape %3080 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %3082 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %3083 = stablehlo.add %3081, %3082 : tensor<2048x1xf32>
    %3084 = stablehlo.rsqrt %3083 : tensor<2048x1xf32>
    %3085 = stablehlo.reshape %3084 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %3086 = stablehlo.broadcast_in_dim %3085, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %3087 = stablehlo.multiply %3077, %3086 : tensor<2048x4096xf32>
    %3088 = stablehlo.broadcast_in_dim %arg30, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %3089 = stablehlo.multiply %3087, %3088 : tensor<2048x4096xf32>
    %3090 = stablehlo.transpose %arg342, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %3091 = stablehlo.dot %3089, %3090, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %3092 = stablehlo.logistic %3091 : tensor<2048x11008xf32>
    %3093 = stablehlo.multiply %3091, %3092 : tensor<2048x11008xf32>
    %3094 = stablehlo.transpose %arg29, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %3095 = stablehlo.dot %3089, %3094, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %3096 = stablehlo.multiply %3093, %3095 : tensor<2048x11008xf32>
    %3097 = stablehlo.transpose %arg28, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %3098 = stablehlo.dot %3096, %3097, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %3099 = stablehlo.add %3077, %3098 : tensor<2048x4096xf32>
    %3100 = stablehlo.power %3099, %1 : tensor<2048x4096xf32>
    %3101 = stablehlo.reduce(%3100 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3102 = stablehlo.multiply %3101, %0 : tensor<2048xf32>
    %3103 = stablehlo.reshape %3102 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %3104 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %3105 = stablehlo.add %3103, %3104 : tensor<2048x1xf32>
    %3106 = stablehlo.rsqrt %3105 : tensor<2048x1xf32>
    %3107 = stablehlo.reshape %3106 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %3108 = stablehlo.broadcast_in_dim %3107, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %3109 = stablehlo.multiply %3099, %3108 : tensor<2048x4096xf32>
    %3110 = stablehlo.broadcast_in_dim %arg27, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %3111 = stablehlo.multiply %3109, %3110 : tensor<2048x4096xf32>
    %3112 = stablehlo.transpose %arg346, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3113 = stablehlo.dot %3111, %3112, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3114 = stablehlo.reshape %3113 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %3115 = stablehlo.slice %3114 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3116 = stablehlo.reshape %3115 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3117 = stablehlo.slice %3114 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3118 = stablehlo.reshape %3117 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3119 = stablehlo.complex %3116, %3118 : tensor<2048x32x64xcomplex<f32>>
    %3120 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %3121 = stablehlo.multiply %3119, %3120 : tensor<2048x32x64xcomplex<f32>>
    %3122 = stablehlo.real %3121 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3123 = stablehlo.reshape %3122 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3124 = stablehlo.imag %3121 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3125 = stablehlo.reshape %3124 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3126 = stablehlo.concatenate %3123, %3125, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %3127 = stablehlo.reshape %3126 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %3128 = stablehlo.transpose %3127, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %3129 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %3130 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %3131 = stablehlo.add %arg198, %3130 : tensor<2048xi64>
    %3132 = stablehlo.select %3129, %3131, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %3133 = stablehlo.reshape %3132 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %3134 = stablehlo.transpose %arg344, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3135 = stablehlo.dot %3111, %3134, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3136 = stablehlo.reshape %3135 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %3137 = stablehlo.slice %3136 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3138 = stablehlo.reshape %3137 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3139 = stablehlo.slice %3136 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3140 = stablehlo.reshape %3139 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3141 = stablehlo.complex %3138, %3140 : tensor<2048x32x64xcomplex<f32>>
    %3142 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %3143 = stablehlo.multiply %3141, %3142 : tensor<2048x32x64xcomplex<f32>>
    %3144 = stablehlo.real %3143 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3145 = stablehlo.reshape %3144 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3146 = stablehlo.imag %3143 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3147 = stablehlo.reshape %3146 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3148 = stablehlo.concatenate %3145, %3147, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %3149 = stablehlo.reshape %3148 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %3150 = "stablehlo.scatter"(%arg345, %3133, %3149) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %3151 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %3152 = "stablehlo.gather"(%3150, %3151) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %3153 = stablehlo.transpose %3152, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %3154 = stablehlo.dot_general %3128, %3153, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %3155 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %3156 = stablehlo.divide %3154, %3155 : tensor<32x2048x2048xf32>
    %3157 = stablehlo.reshape %3156 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3158 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %3159 = stablehlo.broadcast_in_dim %3158, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3160 = stablehlo.add %3157, %3159 : tensor<1x32x2048x2048xf32>
    %3161 = stablehlo.reduce(%3160 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3162 = stablehlo.broadcast_in_dim %3161, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3163 = stablehlo.subtract %3160, %3162 : tensor<1x32x2048x2048xf32>
    %3164 = stablehlo.exponential %3163 : tensor<1x32x2048x2048xf32>
    %3165 = stablehlo.reduce(%3164 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3166 = stablehlo.broadcast_in_dim %3165, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3167 = stablehlo.divide %3164, %3166 : tensor<1x32x2048x2048xf32>
    %3168 = stablehlo.reshape %3167 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %3169 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %3170 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %3171 = stablehlo.add %arg198, %3170 : tensor<2048xi64>
    %3172 = stablehlo.select %3169, %3171, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %3173 = stablehlo.reshape %3172 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %3174 = stablehlo.transpose %arg26, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3175 = stablehlo.dot %3111, %3174, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3176 = stablehlo.reshape %3175 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %3177 = "stablehlo.scatter"(%arg343, %3173, %3176) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %3178 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %3179 = "stablehlo.gather"(%3177, %3178) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %3180 = stablehlo.transpose %3179, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %3181 = stablehlo.dot_general %3168, %3180, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %3182 = stablehlo.reshape %3181 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %3183 = stablehlo.transpose %3182, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %3184 = stablehlo.reshape %3183 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %3185 = stablehlo.transpose %arg25, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3186 = stablehlo.dot %3184, %3185, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3187 = stablehlo.add %3099, %3186 : tensor<2048x4096xf32>
    %3188 = stablehlo.power %3187, %1 : tensor<2048x4096xf32>
    %3189 = stablehlo.reduce(%3188 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3190 = stablehlo.multiply %3189, %0 : tensor<2048xf32>
    %3191 = stablehlo.reshape %3190 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %3192 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %3193 = stablehlo.add %3191, %3192 : tensor<2048x1xf32>
    %3194 = stablehlo.rsqrt %3193 : tensor<2048x1xf32>
    %3195 = stablehlo.reshape %3194 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %3196 = stablehlo.broadcast_in_dim %3195, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %3197 = stablehlo.multiply %3187, %3196 : tensor<2048x4096xf32>
    %3198 = stablehlo.broadcast_in_dim %arg24, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %3199 = stablehlo.multiply %3197, %3198 : tensor<2048x4096xf32>
    %3200 = stablehlo.transpose %arg347, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %3201 = stablehlo.dot %3199, %3200, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %3202 = stablehlo.logistic %3201 : tensor<2048x11008xf32>
    %3203 = stablehlo.multiply %3201, %3202 : tensor<2048x11008xf32>
    %3204 = stablehlo.transpose %arg23, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %3205 = stablehlo.dot %3199, %3204, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %3206 = stablehlo.multiply %3203, %3205 : tensor<2048x11008xf32>
    %3207 = stablehlo.transpose %arg22, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %3208 = stablehlo.dot %3206, %3207, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %3209 = stablehlo.add %3187, %3208 : tensor<2048x4096xf32>
    %3210 = stablehlo.power %3209, %1 : tensor<2048x4096xf32>
    %3211 = stablehlo.reduce(%3210 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3212 = stablehlo.multiply %3211, %0 : tensor<2048xf32>
    %3213 = stablehlo.reshape %3212 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %3214 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %3215 = stablehlo.add %3213, %3214 : tensor<2048x1xf32>
    %3216 = stablehlo.rsqrt %3215 : tensor<2048x1xf32>
    %3217 = stablehlo.reshape %3216 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %3218 = stablehlo.broadcast_in_dim %3217, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %3219 = stablehlo.multiply %3209, %3218 : tensor<2048x4096xf32>
    %3220 = stablehlo.broadcast_in_dim %arg21, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %3221 = stablehlo.multiply %3219, %3220 : tensor<2048x4096xf32>
    %3222 = stablehlo.transpose %arg351, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3223 = stablehlo.dot %3221, %3222, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3224 = stablehlo.reshape %3223 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %3225 = stablehlo.slice %3224 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3226 = stablehlo.reshape %3225 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3227 = stablehlo.slice %3224 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3228 = stablehlo.reshape %3227 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3229 = stablehlo.complex %3226, %3228 : tensor<2048x32x64xcomplex<f32>>
    %3230 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %3231 = stablehlo.multiply %3229, %3230 : tensor<2048x32x64xcomplex<f32>>
    %3232 = stablehlo.real %3231 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3233 = stablehlo.reshape %3232 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3234 = stablehlo.imag %3231 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3235 = stablehlo.reshape %3234 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3236 = stablehlo.concatenate %3233, %3235, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %3237 = stablehlo.reshape %3236 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %3238 = stablehlo.transpose %3237, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %3239 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %3240 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %3241 = stablehlo.add %arg198, %3240 : tensor<2048xi64>
    %3242 = stablehlo.select %3239, %3241, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %3243 = stablehlo.reshape %3242 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %3244 = stablehlo.transpose %arg349, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3245 = stablehlo.dot %3221, %3244, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3246 = stablehlo.reshape %3245 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %3247 = stablehlo.slice %3246 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3248 = stablehlo.reshape %3247 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3249 = stablehlo.slice %3246 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3250 = stablehlo.reshape %3249 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3251 = stablehlo.complex %3248, %3250 : tensor<2048x32x64xcomplex<f32>>
    %3252 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %3253 = stablehlo.multiply %3251, %3252 : tensor<2048x32x64xcomplex<f32>>
    %3254 = stablehlo.real %3253 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3255 = stablehlo.reshape %3254 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3256 = stablehlo.imag %3253 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3257 = stablehlo.reshape %3256 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3258 = stablehlo.concatenate %3255, %3257, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %3259 = stablehlo.reshape %3258 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %3260 = "stablehlo.scatter"(%arg350, %3243, %3259) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %3261 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %3262 = "stablehlo.gather"(%3260, %3261) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %3263 = stablehlo.transpose %3262, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %3264 = stablehlo.dot_general %3238, %3263, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %3265 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %3266 = stablehlo.divide %3264, %3265 : tensor<32x2048x2048xf32>
    %3267 = stablehlo.reshape %3266 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3268 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %3269 = stablehlo.broadcast_in_dim %3268, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3270 = stablehlo.add %3267, %3269 : tensor<1x32x2048x2048xf32>
    %3271 = stablehlo.reduce(%3270 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3272 = stablehlo.broadcast_in_dim %3271, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3273 = stablehlo.subtract %3270, %3272 : tensor<1x32x2048x2048xf32>
    %3274 = stablehlo.exponential %3273 : tensor<1x32x2048x2048xf32>
    %3275 = stablehlo.reduce(%3274 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3276 = stablehlo.broadcast_in_dim %3275, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3277 = stablehlo.divide %3274, %3276 : tensor<1x32x2048x2048xf32>
    %3278 = stablehlo.reshape %3277 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %3279 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %3280 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %3281 = stablehlo.add %arg198, %3280 : tensor<2048xi64>
    %3282 = stablehlo.select %3279, %3281, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %3283 = stablehlo.reshape %3282 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %3284 = stablehlo.transpose %arg20, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3285 = stablehlo.dot %3221, %3284, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3286 = stablehlo.reshape %3285 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %3287 = "stablehlo.scatter"(%arg348, %3283, %3286) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %3288 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %3289 = "stablehlo.gather"(%3287, %3288) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %3290 = stablehlo.transpose %3289, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %3291 = stablehlo.dot_general %3278, %3290, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %3292 = stablehlo.reshape %3291 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %3293 = stablehlo.transpose %3292, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %3294 = stablehlo.reshape %3293 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %3295 = stablehlo.transpose %arg19, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3296 = stablehlo.dot %3294, %3295, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3297 = stablehlo.add %3209, %3296 : tensor<2048x4096xf32>
    %3298 = stablehlo.power %3297, %1 : tensor<2048x4096xf32>
    %3299 = stablehlo.reduce(%3298 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3300 = stablehlo.multiply %3299, %0 : tensor<2048xf32>
    %3301 = stablehlo.reshape %3300 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %3302 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %3303 = stablehlo.add %3301, %3302 : tensor<2048x1xf32>
    %3304 = stablehlo.rsqrt %3303 : tensor<2048x1xf32>
    %3305 = stablehlo.reshape %3304 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %3306 = stablehlo.broadcast_in_dim %3305, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %3307 = stablehlo.multiply %3297, %3306 : tensor<2048x4096xf32>
    %3308 = stablehlo.broadcast_in_dim %arg18, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %3309 = stablehlo.multiply %3307, %3308 : tensor<2048x4096xf32>
    %3310 = stablehlo.transpose %arg352, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %3311 = stablehlo.dot %3309, %3310, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %3312 = stablehlo.logistic %3311 : tensor<2048x11008xf32>
    %3313 = stablehlo.multiply %3311, %3312 : tensor<2048x11008xf32>
    %3314 = stablehlo.transpose %arg17, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %3315 = stablehlo.dot %3309, %3314, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %3316 = stablehlo.multiply %3313, %3315 : tensor<2048x11008xf32>
    %3317 = stablehlo.transpose %arg16, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %3318 = stablehlo.dot %3316, %3317, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %3319 = stablehlo.add %3297, %3318 : tensor<2048x4096xf32>
    %3320 = stablehlo.power %3319, %1 : tensor<2048x4096xf32>
    %3321 = stablehlo.reduce(%3320 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3322 = stablehlo.multiply %3321, %0 : tensor<2048xf32>
    %3323 = stablehlo.reshape %3322 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %3324 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %3325 = stablehlo.add %3323, %3324 : tensor<2048x1xf32>
    %3326 = stablehlo.rsqrt %3325 : tensor<2048x1xf32>
    %3327 = stablehlo.reshape %3326 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %3328 = stablehlo.broadcast_in_dim %3327, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %3329 = stablehlo.multiply %3319, %3328 : tensor<2048x4096xf32>
    %3330 = stablehlo.broadcast_in_dim %arg15, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %3331 = stablehlo.multiply %3329, %3330 : tensor<2048x4096xf32>
    %3332 = stablehlo.transpose %arg356, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3333 = stablehlo.dot %3331, %3332, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3334 = stablehlo.reshape %3333 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %3335 = stablehlo.slice %3334 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3336 = stablehlo.reshape %3335 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3337 = stablehlo.slice %3334 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3338 = stablehlo.reshape %3337 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3339 = stablehlo.complex %3336, %3338 : tensor<2048x32x64xcomplex<f32>>
    %3340 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %3341 = stablehlo.multiply %3339, %3340 : tensor<2048x32x64xcomplex<f32>>
    %3342 = stablehlo.real %3341 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3343 = stablehlo.reshape %3342 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3344 = stablehlo.imag %3341 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3345 = stablehlo.reshape %3344 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3346 = stablehlo.concatenate %3343, %3345, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %3347 = stablehlo.reshape %3346 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %3348 = stablehlo.transpose %3347, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %3349 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %3350 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %3351 = stablehlo.add %arg198, %3350 : tensor<2048xi64>
    %3352 = stablehlo.select %3349, %3351, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %3353 = stablehlo.reshape %3352 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %3354 = stablehlo.transpose %arg354, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3355 = stablehlo.dot %3331, %3354, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3356 = stablehlo.reshape %3355 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %3357 = stablehlo.slice %3356 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3358 = stablehlo.reshape %3357 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3359 = stablehlo.slice %3356 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3360 = stablehlo.reshape %3359 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3361 = stablehlo.complex %3358, %3360 : tensor<2048x32x64xcomplex<f32>>
    %3362 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %3363 = stablehlo.multiply %3361, %3362 : tensor<2048x32x64xcomplex<f32>>
    %3364 = stablehlo.real %3363 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3365 = stablehlo.reshape %3364 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3366 = stablehlo.imag %3363 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3367 = stablehlo.reshape %3366 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3368 = stablehlo.concatenate %3365, %3367, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %3369 = stablehlo.reshape %3368 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %3370 = "stablehlo.scatter"(%arg355, %3353, %3369) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %3371 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %3372 = "stablehlo.gather"(%3370, %3371) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %3373 = stablehlo.transpose %3372, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %3374 = stablehlo.dot_general %3348, %3373, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %3375 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %3376 = stablehlo.divide %3374, %3375 : tensor<32x2048x2048xf32>
    %3377 = stablehlo.reshape %3376 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3378 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %3379 = stablehlo.broadcast_in_dim %3378, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3380 = stablehlo.add %3377, %3379 : tensor<1x32x2048x2048xf32>
    %3381 = stablehlo.reduce(%3380 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3382 = stablehlo.broadcast_in_dim %3381, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3383 = stablehlo.subtract %3380, %3382 : tensor<1x32x2048x2048xf32>
    %3384 = stablehlo.exponential %3383 : tensor<1x32x2048x2048xf32>
    %3385 = stablehlo.reduce(%3384 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3386 = stablehlo.broadcast_in_dim %3385, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3387 = stablehlo.divide %3384, %3386 : tensor<1x32x2048x2048xf32>
    %3388 = stablehlo.reshape %3387 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %3389 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %3390 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %3391 = stablehlo.add %arg198, %3390 : tensor<2048xi64>
    %3392 = stablehlo.select %3389, %3391, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %3393 = stablehlo.reshape %3392 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %3394 = stablehlo.transpose %arg14, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3395 = stablehlo.dot %3331, %3394, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3396 = stablehlo.reshape %3395 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %3397 = "stablehlo.scatter"(%arg353, %3393, %3396) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %3398 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %3399 = "stablehlo.gather"(%3397, %3398) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %3400 = stablehlo.transpose %3399, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %3401 = stablehlo.dot_general %3388, %3400, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %3402 = stablehlo.reshape %3401 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %3403 = stablehlo.transpose %3402, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %3404 = stablehlo.reshape %3403 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %3405 = stablehlo.transpose %arg13, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3406 = stablehlo.dot %3404, %3405, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3407 = stablehlo.add %3319, %3406 : tensor<2048x4096xf32>
    %3408 = stablehlo.power %3407, %1 : tensor<2048x4096xf32>
    %3409 = stablehlo.reduce(%3408 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3410 = stablehlo.multiply %3409, %0 : tensor<2048xf32>
    %3411 = stablehlo.reshape %3410 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %3412 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %3413 = stablehlo.add %3411, %3412 : tensor<2048x1xf32>
    %3414 = stablehlo.rsqrt %3413 : tensor<2048x1xf32>
    %3415 = stablehlo.reshape %3414 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %3416 = stablehlo.broadcast_in_dim %3415, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %3417 = stablehlo.multiply %3407, %3416 : tensor<2048x4096xf32>
    %3418 = stablehlo.broadcast_in_dim %arg12, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %3419 = stablehlo.multiply %3417, %3418 : tensor<2048x4096xf32>
    %3420 = stablehlo.transpose %arg357, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %3421 = stablehlo.dot %3419, %3420, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %3422 = stablehlo.logistic %3421 : tensor<2048x11008xf32>
    %3423 = stablehlo.multiply %3421, %3422 : tensor<2048x11008xf32>
    %3424 = stablehlo.transpose %arg11, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %3425 = stablehlo.dot %3419, %3424, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %3426 = stablehlo.multiply %3423, %3425 : tensor<2048x11008xf32>
    %3427 = stablehlo.transpose %arg10, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %3428 = stablehlo.dot %3426, %3427, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %3429 = stablehlo.add %3407, %3428 : tensor<2048x4096xf32>
    %3430 = stablehlo.power %3429, %1 : tensor<2048x4096xf32>
    %3431 = stablehlo.reduce(%3430 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3432 = stablehlo.multiply %3431, %0 : tensor<2048xf32>
    %3433 = stablehlo.reshape %3432 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %3434 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %3435 = stablehlo.add %3433, %3434 : tensor<2048x1xf32>
    %3436 = stablehlo.rsqrt %3435 : tensor<2048x1xf32>
    %3437 = stablehlo.reshape %3436 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %3438 = stablehlo.broadcast_in_dim %3437, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %3439 = stablehlo.multiply %3429, %3438 : tensor<2048x4096xf32>
    %3440 = stablehlo.broadcast_in_dim %arg9, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %3441 = stablehlo.multiply %3439, %3440 : tensor<2048x4096xf32>
    %3442 = stablehlo.transpose %arg361, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3443 = stablehlo.dot %3441, %3442, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3444 = stablehlo.reshape %3443 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %3445 = stablehlo.slice %3444 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3446 = stablehlo.reshape %3445 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3447 = stablehlo.slice %3444 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3448 = stablehlo.reshape %3447 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3449 = stablehlo.complex %3446, %3448 : tensor<2048x32x64xcomplex<f32>>
    %3450 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %3451 = stablehlo.multiply %3449, %3450 : tensor<2048x32x64xcomplex<f32>>
    %3452 = stablehlo.real %3451 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3453 = stablehlo.reshape %3452 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3454 = stablehlo.imag %3451 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3455 = stablehlo.reshape %3454 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3456 = stablehlo.concatenate %3453, %3455, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %3457 = stablehlo.reshape %3456 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %3458 = stablehlo.transpose %3457, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %3459 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %3460 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %3461 = stablehlo.add %arg198, %3460 : tensor<2048xi64>
    %3462 = stablehlo.select %3459, %3461, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %3463 = stablehlo.reshape %3462 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %3464 = stablehlo.transpose %arg359, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3465 = stablehlo.dot %3441, %3464, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3466 = stablehlo.reshape %3465 : (tensor<2048x4096xf32>) -> tensor<2048x32x64x2xf32>
    %3467 = stablehlo.slice %3466 [0:2048, 0:32, 0:64, 0:1] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3468 = stablehlo.reshape %3467 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3469 = stablehlo.slice %3466 [0:2048, 0:32, 0:64, 1:2] : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x64x1xf32>
    %3470 = stablehlo.reshape %3469 : (tensor<2048x32x64x1xf32>) -> tensor<2048x32x64xf32>
    %3471 = stablehlo.complex %3468, %3470 : tensor<2048x32x64xcomplex<f32>>
    %3472 = stablehlo.broadcast_in_dim %31, dims = [0, 2] : (tensor<2048x64xcomplex<f32>>) -> tensor<2048x32x64xcomplex<f32>>
    %3473 = stablehlo.multiply %3471, %3472 : tensor<2048x32x64xcomplex<f32>>
    %3474 = stablehlo.real %3473 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3475 = stablehlo.reshape %3474 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3476 = stablehlo.imag %3473 : (tensor<2048x32x64xcomplex<f32>>) -> tensor<2048x32x64xf32>
    %3477 = stablehlo.reshape %3476 : (tensor<2048x32x64xf32>) -> tensor<2048x32x64x1xf32>
    %3478 = stablehlo.concatenate %3475, %3477, dim = 3 : (tensor<2048x32x64x1xf32>, tensor<2048x32x64x1xf32>) -> tensor<2048x32x64x2xf32>
    %3479 = stablehlo.reshape %3478 : (tensor<2048x32x64x2xf32>) -> tensor<2048x32x128xf32>
    %3480 = "stablehlo.scatter"(%arg360, %3463, %3479) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %3481 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %3482 = "stablehlo.gather"(%3480, %3481) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %3483 = stablehlo.transpose %3482, dims = [1, 2, 0] : (tensor<2048x32x128xf32>) -> tensor<32x128x2048xf32>
    %3484 = stablehlo.dot_general %3458, %3483, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x128xf32>, tensor<32x128x2048xf32>) -> tensor<32x2048x2048xf32>
    %3485 = stablehlo.broadcast_in_dim %arg202, dims = [] : (tensor<f32>) -> tensor<32x2048x2048xf32>
    %3486 = stablehlo.divide %3484, %3485 : tensor<32x2048x2048xf32>
    %3487 = stablehlo.reshape %3486 : (tensor<32x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3488 = stablehlo.reshape %77 : (tensor<1x1x2048x2048xf32>) -> tensor<1x2048x2048xf32>
    %3489 = stablehlo.broadcast_in_dim %3488, dims = [0, 2, 3] : (tensor<1x2048x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3490 = stablehlo.add %3487, %3489 : tensor<1x32x2048x2048xf32>
    %3491 = stablehlo.reduce(%3490 init: %6) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.maximum %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3492 = stablehlo.broadcast_in_dim %3491, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3493 = stablehlo.subtract %3490, %3492 : tensor<1x32x2048x2048xf32>
    %3494 = stablehlo.exponential %3493 : tensor<1x32x2048x2048xf32>
    %3495 = stablehlo.reduce(%3494 init: %7) across dimensions = [3] : (tensor<1x32x2048x2048xf32>, tensor<f32>) -> tensor<1x32x2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3496 = stablehlo.broadcast_in_dim %3495, dims = [0, 1, 2] : (tensor<1x32x2048xf32>) -> tensor<1x32x2048x2048xf32>
    %3497 = stablehlo.divide %3494, %3496 : tensor<1x32x2048x2048xf32>
    %3498 = stablehlo.reshape %3497 : (tensor<1x32x2048x2048xf32>) -> tensor<32x2048x2048xf32>
    %3499 = stablehlo.compare  LT, %arg198, %2 : (tensor<2048xi64>, tensor<2048xi64>) -> tensor<2048xi1>
    %3500 = stablehlo.broadcast_in_dim %arg199, dims = [] : (tensor<i64>) -> tensor<2048xi64>
    %3501 = stablehlo.add %arg198, %3500 : tensor<2048xi64>
    %3502 = stablehlo.select %3499, %3501, %arg198 : tensor<2048xi1>, tensor<2048xi64>
    %3503 = stablehlo.reshape %3502 : (tensor<2048xi64>) -> tensor<2048x1xi64>
    %3504 = stablehlo.transpose %arg8, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3505 = stablehlo.dot %3441, %3504, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3506 = stablehlo.reshape %3505 : (tensor<2048x4096xf32>) -> tensor<2048x32x128xf32>
    %3507 = "stablehlo.scatter"(%arg358, %3503, %3506) ({
    ^bb0(%arg363: tensor<f32>, %arg364: tensor<f32>):
      stablehlo.return %arg364 : tensor<f32>
    }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<2304x32x128xf32>, tensor<2048x1xi64>, tensor<2048x32x128xf32>) -> tensor<2304x32x128xf32>
    %3508 = stablehlo.convert %arg7 : (tensor<2048xi64>) -> tensor<2048xui32>
    %3509 = "stablehlo.gather"(%3507, %3508) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = dense<[1, 32, 128]> : tensor<3xi64>} : (tensor<2304x32x128xf32>, tensor<2048xui32>) -> tensor<2048x32x128xf32>
    %3510 = stablehlo.transpose %3509, dims = [1, 0, 2] {result_layout = dense<[2, 0, 1]> : tensor<3xindex>, xla_shape = "f32[32,2048,128]{2,0,1}"} : (tensor<2048x32x128xf32>) -> tensor<32x2048x128xf32>
    %3511 = stablehlo.dot_general %3498, %3510, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x2048x2048xf32>, tensor<32x2048x128xf32>) -> tensor<32x2048x128xf32>
    %3512 = stablehlo.reshape %3511 : (tensor<32x2048x128xf32>) -> tensor<1x32x2048x128xf32>
    %3513 = stablehlo.transpose %3512, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,2048,32,128]{3,1,2,0}"} : (tensor<1x32x2048x128xf32>) -> tensor<1x2048x32x128xf32>
    %3514 = stablehlo.reshape %3513 : (tensor<1x2048x32x128xf32>) -> tensor<2048x4096xf32>
    %3515 = stablehlo.transpose %arg6, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,4096]{0,1}"} : (tensor<4096x4096xf32>) -> tensor<4096x4096xf32>
    %3516 = stablehlo.dot %3514, %3515, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x4096xf32>) -> tensor<2048x4096xf32>
    %3517 = stablehlo.add %3429, %3516 : tensor<2048x4096xf32>
    %3518 = stablehlo.power %3517, %1 : tensor<2048x4096xf32>
    %3519 = stablehlo.reduce(%3518 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3520 = stablehlo.multiply %3519, %0 : tensor<2048xf32>
    %3521 = stablehlo.reshape %3520 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %3522 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %3523 = stablehlo.add %3521, %3522 : tensor<2048x1xf32>
    %3524 = stablehlo.rsqrt %3523 : tensor<2048x1xf32>
    %3525 = stablehlo.reshape %3524 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %3526 = stablehlo.broadcast_in_dim %3525, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %3527 = stablehlo.multiply %3517, %3526 : tensor<2048x4096xf32>
    %3528 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %3529 = stablehlo.multiply %3527, %3528 : tensor<2048x4096xf32>
    %3530 = stablehlo.transpose %arg362, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %3531 = stablehlo.dot %3529, %3530, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %3532 = stablehlo.logistic %3531 : tensor<2048x11008xf32>
    %3533 = stablehlo.multiply %3531, %3532 : tensor<2048x11008xf32>
    %3534 = stablehlo.transpose %arg4, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,11008]{0,1}"} : (tensor<11008x4096xf32>) -> tensor<4096x11008xf32>
    %3535 = stablehlo.dot %3529, %3534, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x11008xf32>) -> tensor<2048x11008xf32>
    %3536 = stablehlo.multiply %3533, %3535 : tensor<2048x11008xf32>
    %3537 = stablehlo.transpose %arg3, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[11008,4096]{0,1}"} : (tensor<4096x11008xf32>) -> tensor<11008x4096xf32>
    %3538 = stablehlo.dot %3536, %3537, precision = [DEFAULT, DEFAULT] : (tensor<2048x11008xf32>, tensor<11008x4096xf32>) -> tensor<2048x4096xf32>
    %3539 = stablehlo.add %3517, %3538 : tensor<2048x4096xf32>
    %3540 = stablehlo.power %3539, %1 : tensor<2048x4096xf32>
    %3541 = stablehlo.reduce(%3540 init: %7) across dimensions = [1] : (tensor<2048x4096xf32>, tensor<f32>) -> tensor<2048xf32>
     reducer(%arg363: tensor<f32>, %arg364: tensor<f32>)  {
      %3554 = stablehlo.add %arg363, %arg364 : tensor<f32>
      stablehlo.return %3554 : tensor<f32>
    }
    %3542 = stablehlo.multiply %3541, %0 : tensor<2048xf32>
    %3543 = stablehlo.reshape %3542 : (tensor<2048xf32>) -> tensor<2048x1xf32>
    %3544 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<2048x1xf32>
    %3545 = stablehlo.add %3543, %3544 : tensor<2048x1xf32>
    %3546 = stablehlo.rsqrt %3545 : tensor<2048x1xf32>
    %3547 = stablehlo.reshape %3546 : (tensor<2048x1xf32>) -> tensor<2048xf32>
    %3548 = stablehlo.broadcast_in_dim %3547, dims = [0] : (tensor<2048xf32>) -> tensor<2048x4096xf32>
    %3549 = stablehlo.multiply %3539, %3548 : tensor<2048x4096xf32>
    %3550 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<4096xf32>) -> tensor<2048x4096xf32>
    %3551 = stablehlo.multiply %3549, %3550 : tensor<2048x4096xf32>
    %3552 = stablehlo.transpose %arg0, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[4096,32000]{0,1}"} : (tensor<32000x4096xf32>) -> tensor<4096x32000xf32>
    %3553 = stablehlo.dot %3551, %3552, precision = [DEFAULT, DEFAULT] : (tensor<2048x4096xf32>, tensor<4096x32000xf32>) -> tensor<2048x32000xf32>
    return %3553, %62, %97, %180, %207, %290, %317, %400, %427, %510, %537, %620, %647, %730, %757, %840, %867, %950, %977, %1060, %1087, %1170, %1197, %1280, %1307, %1390, %1417, %1500, %1527, %1610, %1637, %1720, %1747, %1830, %1857, %1940, %1967, %2050, %2077, %2160, %2187, %2270, %2297, %2380, %2407, %2490, %2517, %2600, %2627, %2710, %2737, %2820, %2847, %2930, %2957, %3040, %3067, %3150, %3177, %3260, %3287, %3370, %3397, %3480, %3507 : tensor<2048x32000xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>, tensor<2304x32x128xf32>
  }
}
